<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interview Preparation Tool</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }

        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .header p {
            font-size: 1.1em;
            opacity: 0.9;
        }

        .header-nav {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin-top: 20px;
        }

        .header-nav-btn {
            background: rgba(255, 255, 255, 0.2);
            color: white;
            border: 2px solid white;
            padding: 10px 20px;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s;
            font-size: 1em;
        }

        .header-nav-btn:hover {
            background: white;
            color: #667eea;
        }

        .header-nav-btn.active {
            background: white;
            color: #667eea;
        }

        .main-content {
            display: flex;
            min-height: 600px;
        }

        .sidebar {
            width: 300px;
            background: #f8f9fa;
            border-right: 2px solid #e9ecef;
            padding: 20px;
            overflow-y: auto;
            max-height: 600px;
            transition: all 0.3s;
        }

        .sidebar.hidden {
            width: 0;
            padding: 0;
            overflow: hidden;
            border-right: none;
        }

        .category-filter {
            margin-bottom: 20px;
        }

        .category-filter-title {
            font-size: 0.9em;
            color: #666;
            margin-bottom: 10px;
            font-weight: bold;
        }

        .category-chips {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 20px;
        }

        .category-chip {
            background: white;
            color: #667eea;
            border: 2px solid #667eea;
            padding: 6px 12px;
            border-radius: 20px;
            cursor: pointer;
            font-size: 0.85em;
            transition: all 0.3s;
            font-weight: 500;
        }

        .category-chip:hover {
            background: #e7e9ff;
        }

        .category-chip.active {
            background: #667eea;
            color: white;
        }

        .question-counter {
            background: #f8f9fa;
            padding: 10px 15px;
            border-radius: 8px;
            margin-bottom: 15px;
            text-align: center;
            font-size: 0.9em;
            color: #666;
        }

        .question-counter strong {
            color: #667eea;
            font-size: 1.2em;
        }

        .question-list {
            padding: 0;
        }

        .question-item {
            padding: 12px 15px;
            margin: 8px 0;
            background: white;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.3s;
            border-left: 3px solid transparent;
            display: flex;
            align-items: flex-start;
            gap: 10px;
        }

        .question-number {
            background: #667eea;
            color: white;
            min-width: 30px;
            height: 30px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 0.85em;
            flex-shrink: 0;
        }

        .question-text {
            flex: 1;
            font-size: 0.9em;
            line-height: 1.4;
        }

        .question-category-tag {
            font-size: 0.75em;
            color: #667eea;
            opacity: 0.7;
            margin-top: 4px;
        }

        .question-item:hover {
            background: #e7e9ff;
            border-left-color: #667eea;
            transform: translateX(5px);
        }

        .question-item.active {
            background: #667eea;
            color: white;
            border-left-color: #764ba2;
        }

        .question-item.active .question-number {
            background: #764ba2;
        }

        .question-item.active .question-category-tag {
            color: white;
            opacity: 0.9;
        }

        .question-practice-info {
            display: flex;
            gap: 8px;
            margin-top: 6px;
            font-size: 0.7em;
        }

        .practice-mini-badge {
            background: #e7e9ff;
            color: #667eea;
            padding: 2px 8px;
            border-radius: 10px;
            font-weight: 600;
        }

        .question-item.active .practice-mini-badge {
            background: rgba(255, 255, 255, 0.3);
            color: white;
        }

        .practice-mini-badge.zero {
            background: #ffe7e7;
            color: #dc3545;
        }

        .question-item.active .practice-mini-badge.zero {
            background: rgba(220, 53, 69, 0.3);
            color: white;
        }

        .sidebar-toggle {
            position: fixed;
            left: 20px;
            top: 50%;
            transform: translateY(-50%);
            background: #667eea;
            color: white;
            border: none;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            cursor: pointer;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
            z-index: 100;
            transition: all 0.3s;
            display: none;
            align-items: center;
            justify-content: center;
            font-size: 1.2em;
        }

        .sidebar-toggle.visible {
            display: flex;
        }

        .sidebar-toggle:hover {
            background: #5568d3;
            transform: translateY(-50%) scale(1.1);
        }

        .content-area {
            flex: 1;
            padding: 40px;
            overflow-y: auto;
            max-height: 600px;
        }

        .question-header {
            margin-bottom: 30px;
        }

        .question-title {
            font-size: 1.8em;
            color: #333;
            margin-bottom: 10px;
        }

        .question-category-badge {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.9em;
        }

        .answer-section {
            margin-bottom: 30px;
            border: 2px solid #e9ecef;
            border-radius: 10px;
            overflow: hidden;
        }

        .answer-header {
            background: #f8f9fa;
            padding: 15px 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            cursor: pointer;
            transition: background 0.3s;
        }

        .answer-header:hover {
            background: #e9ecef;
        }

        .answer-header h3 {
            color: #333;
            font-size: 1.2em;
        }

        .answer-controls {
            display: flex;
            gap: 10px;
            align-items: center;
        }

        .toggle-btn {
            background: #667eea;
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 5px;
            cursor: pointer;
            transition: all 0.3s;
            font-size: 0.9em;
        }

        .toggle-btn:hover {
            background: #5568d3;
            transform: scale(1.05);
        }

        .counter {
            display: flex;
            align-items: center;
            gap: 5px;
            background: white;
            padding: 3px 8px;
            border-radius: 5px;
            border: 1px solid #dee2e6;
            font-size: 0.75em;
            opacity: 0.8;
        }

        .counter-value {
            font-weight: bold;
            color: #667eea;
            min-width: 20px;
            text-align: center;
            font-size: 0.9em;
        }

        .counter-btn {
            background: #28a745;
            color: white;
            border: none;
            width: 18px;
            height: 18px;
            border-radius: 50%;
            cursor: pointer;
            font-size: 0.75em;
            transition: all 0.3s;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .counter-btn:hover {
            background: #218838;
            transform: scale(1.1);
            opacity: 1;
        }

        .answer-content {
            padding: 20px;
            background: white;
            display: none;
            line-height: 1.8;
            color: #555;
        }

        .answer-content.visible {
            display: block;
        }

        .answer-content p {
            white-space: pre-line;
            margin-bottom: 15px;
            line-height: 2;
            text-align: justify;
        }

        .answer-content ul {
            margin-left: 20px;
            margin-top: 10px;
        }

        .answer-content li {
            margin: 8px 0;
        }

        .key-concepts {
            margin-top: 30px;
        }

        .key-concepts h3 {
            margin-bottom: 15px;
            color: #333;
        }

        .concept-buttons {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }

        .concept-btn {
            background: #764ba2;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s;
            font-size: 1em;
        }

        .concept-btn:hover {
            background: #5d3a82;
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(118, 75, 162, 0.4);
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 2px solid #e9ecef;
        }

        .nav-btn {
            background: #667eea;
            color: white;
            border: none;
            padding: 12px 30px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 1em;
            transition: all 0.3s;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .nav-btn:hover:not(:disabled) {
            background: #5568d3;
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }

        .nav-btn:disabled {
            background: #ccc;
            cursor: not-allowed;
        }

        /* Modal Styles */
        .modal {
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.7);
            animation: fadeIn 0.3s;
        }

        .modal.active {
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .modal-content {
            background: white;
            padding: 30px;
            border-radius: 15px;
            max-width: 600px;
            max-height: 80vh;
            overflow-y: auto;
            animation: slideIn 0.3s;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
        }

        .modal-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
            padding-bottom: 15px;
            border-bottom: 2px solid #e9ecef;
        }

        .modal-header h2 {
            color: #764ba2;
        }

        .close-btn {
            background: #dc3545;
            color: white;
            border: none;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            cursor: pointer;
            font-size: 1.2em;
            transition: all 0.3s;
        }

        .close-btn:hover {
            background: #c82333;
            transform: rotate(90deg);
        }

        .modal-body {
            line-height: 2;
            color: #555;
            white-space: pre-line;
            text-align: justify;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        @keyframes slideIn {
            from {
                transform: translateY(-50px);
                opacity: 0;
            }
            to {
                transform: translateY(0);
                opacity: 1;
            }
        }

        /* Practice Overview Styles */
        .practice-overview {
            display: none;
        }

        .practice-overview.active {
            display: block;
        }

        .overview-stats {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }

        .stat-card {
            background: white;
            padding: 15px 20px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
            flex: 1;
            min-width: 150px;
        }

        .stat-card h4 {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 5px;
        }

        .stat-card .stat-value {
            font-size: 2em;
            font-weight: bold;
            color: #667eea;
        }

        .overview-table {
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        .overview-category {
            margin-bottom: 20px;
        }

        .overview-category-header {
            background: #667eea;
            color: white;
            padding: 15px 20px;
            font-weight: bold;
            font-size: 1.1em;
        }

        .overview-question {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 15px 20px;
            border-bottom: 1px solid #e9ecef;
            cursor: pointer;
            transition: all 0.3s;
        }

        .overview-question:hover {
            background: #f8f9fa;
            padding-left: 25px;
        }

        .overview-question:last-child {
            border-bottom: none;
        }

        .overview-question-text {
            flex: 1;
            color: #333;
        }

        .overview-practice-count {
            display: flex;
            gap: 15px;
            align-items: center;
            font-size: 0.9em;
        }

        .practice-badge {
            background: #e7e9ff;
            color: #667eea;
            padding: 5px 12px;
            border-radius: 15px;
            font-weight: bold;
            min-width: 60px;
            text-align: center;
        }

        .practice-badge.low {
            background: #ffe7e7;
            color: #dc3545;
        }

        .practice-badge.medium {
            background: #fff3cd;
            color: #ffc107;
        }

        .practice-badge.high {
            background: #d4edda;
            color: #28a745;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            body {
                padding: 0;
            }

            .container {
                border-radius: 0;
            }

            .header h1 {
                font-size: 1.5em;
            }

            .header p {
                font-size: 0.9em;
            }

            .header-nav {
                flex-wrap: wrap;
            }

            .header-nav-btn {
                padding: 8px 15px;
                font-size: 0.9em;
            }

            .main-content {
                flex-direction: column;
            }

            .sidebar {
                width: 100%;
                max-height: 400px;
                border-right: none;
                border-bottom: 2px solid #e9ecef;
            }

            .sidebar.hidden {
                max-height: 0;
                padding: 0;
            }

            .sidebar-toggle {
                left: 10px;
                top: auto;
                bottom: 20px;
                transform: none;
            }

            .sidebar-toggle:hover {
                transform: scale(1.1);
            }

            .content-area {
                padding: 15px;
                max-height: none;
            }

            .question-title {
                font-size: 1.3em;
            }

            .answer-header {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }

            .answer-controls {
                width: 100%;
                justify-content: space-between;
            }

            .concept-buttons {
                flex-direction: column;
            }

            .concept-btn {
                width: 100%;
            }

            .navigation {
                flex-direction: column;
                gap: 10px;
            }

            .nav-btn {
                width: 100%;
                justify-content: center;
            }

            .modal-content {
                margin: 20px;
                max-width: calc(100% - 40px);
                padding: 20px;
            }

            .overview-stats {
                flex-direction: column;
            }

            .stat-card {
                min-width: 100%;
            }

            .overview-question {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }

            .overview-practice-count {
                width: 100%;
                justify-content: space-between;
            }
        }

        @media (max-width: 480px) {
            .header h1 {
                font-size: 1.2em;
            }

            .header p {
                font-size: 0.85em;
            }

            .sidebar {
                padding: 10px;
            }

            .content-area {
                padding: 10px;
            }

            .answer-header h3 {
                font-size: 1em;
            }

            .toggle-btn {
                padding: 6px 12px;
                font-size: 0.85em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ðŸŽ¯ Interview Preparation Tool</h1>
            <p>Master your interview with structured practice and key concepts</p>
            <div class="header-nav">
                <button class="header-nav-btn active" onclick="switchView('questions')">ðŸ“š Questions</button>
                <button class="header-nav-btn" onclick="switchView('overview')">ðŸ“Š Practice Overview</button>
            </div>
        </div>

        <div class="main-content" id="questionsView">
            <button class="sidebar-toggle" id="sidebarToggle" onclick="toggleSidebar()">â˜°</button>
            
            <div class="sidebar" id="sidebar">
                <div style="padding: 20px; color: #666;">Loading questions...</div>
            </div>

            <div class="content-area" id="contentArea">
                <div class="question-header">
                    <h2 class="question-title" id="questionTitle">Select a question to begin</h2>
                    <span class="question-category-badge" id="categoryBadge"></span>
                </div>

                <div id="questionContent">
                    <!-- Question content will be dynamically loaded here -->
                </div>
            </div>
        </div>

        <div class="practice-overview" id="overviewView">
            <div class="content-area">
                <h2 style="margin-bottom: 20px; color: #333;">ðŸ“Š Practice Overview</h2>
                
                <div class="overview-stats" id="overviewStats">
                    <!-- Stats will be dynamically loaded here -->
                </div>

                <div id="overviewContent">
                    <!-- Overview content will be dynamically loaded here -->
                </div>
            </div>
        </div>
    </div>

    <!-- Modal for Key Concepts -->
    <div class="modal" id="conceptModal">
        <div class="modal-content">
            <div class="modal-header">
                <h2 id="conceptTitle">Key Concept</h2>
                <button class="close-btn" onclick="closeModal()">&times;</button>
            </div>
            <div class="modal-body" id="conceptBody">
                <!-- Concept content will be loaded here -->
            </div>
        </div>
    </div>

    <script>
        // JSON Data
        const interviewData = {
            categories: [
                {
                    id: 1,
                    name: ".NET Core 8+ Backend",
                    questions: [
                        {
                            id: 1,
                            question: "What are the key new features in .NET 8 that you would leverage in a production application?",
                            suggestedAnswer: ".NET 8 brings several production-ready features that I would leverage. First, there's native AOT compilation which significantly reduces startup time and memory footprint, perfect for serverless scenarios.\n\nThe improved performance includes faster JSON serialization with System.Text.Json and enhanced LINQ operations. For web APIs, there are built-in rate limiting middleware and improved minimal APIs with better parameter binding and validation.\n\nThe new TimeProvider abstraction makes testing time-dependent code much easier. From my experience with .NET 8 microservices at BIPO, the enhanced observability with OpenTelemetry integration has been invaluable for distributed tracing across our multi-country HR services. The improved container support and reduced image sizes also align well with our Kubernetes deployments.",
                            bulletPoints: [
                                "Native AOT compilation for faster startup and reduced memory usage",
                                "Built-in rate limiting middleware for API protection",
                                "Enhanced minimal APIs with better parameter binding",
                                "TimeProvider abstraction for testable time-dependent code",
                                "Improved JSON serialization performance",
                                "Better OpenTelemetry integration for observability",
                                "Optimized container images with smaller sizes"
                            ],
                            keyConcepts: [
                                {
                                    name: "Native AOT (Ahead-of-Time)",
                                    explanation: "Native AOT compiles your .NET application directly to native machine code before deployment, rather than using Just-In-Time compilation at runtime.\n\nThis means your app starts much faster and uses less memory because it doesn't need the .NET runtime to compile code on the fly.\n\nThink of it like the difference between having a ready-to-eat meal versus having to cook it first - AOT gives you the ready-to-eat version."
                                },
                                {
                                    name: "Rate Limiting Middleware",
                                    explanation: "Rate limiting controls how many requests a client can make to your API within a specific time window.\n\nIt's like a bouncer at a club who only lets in a certain number of people per hour. This protects your API from being overwhelmed by too many requests, whether from legitimate users or malicious attacks.\n\n.NET 8 includes this built-in, so you don't need external libraries."
                                },
                                {
                                    name: "OpenTelemetry Integration",
                                    explanation: "OpenTelemetry is a standard way to collect telemetry data (logs, metrics, traces) from your applications.\n\nIt's like having a flight recorder in an airplane - it tracks everything that happens so you can understand what went wrong or how to optimize performance.\n\nThe improved integration in .NET 8 makes it easier to monitor distributed systems where one request might touch multiple services."
                                }
                            ]
                        },
                        {
                            id: 2,
                            question: "Explain minimal APIs in .NET Core 8 and when you would use them over traditional controllers.",
                            suggestedAnswer: "Minimal APIs are a simplified way to create HTTP APIs in .NET with minimal code and dependencies. Instead of using controllers with attributes and conventions, you define endpoints directly in Program.cs using methods like MapGet, MapPost, etc.\n\nThey're perfect for microservices, simple APIs, or serverless functions where you want reduced ceremony and faster startup times. I would use minimal APIs for lightweight services like health check endpoints, webhook receivers, or simple CRUD operations.\n\nHowever, for complex applications with many endpoints, shared logic, and extensive validation, traditional controllers offer better organization through action filters, model binding, and conventional routing. At BIPO, we used minimal APIs for our internal service-to-service communication endpoints where simplicity and performance were priorities, while customer-facing APIs used controllers for better structure and OpenAPI documentation generation.",
                            bulletPoints: [
                                "Simplified syntax with endpoints defined in Program.cs",
                                "Reduced boilerplate code compared to controllers",
                                "Faster startup time and lower memory footprint",
                                "Ideal for microservices and serverless scenarios",
                                "Use controllers for complex APIs with many endpoints",
                                "Controllers better for shared logic via filters",
                                "Minimal APIs great for simple CRUD or webhooks"
                            ],
                            keyConcepts: [
                                {
                                    name: "Endpoint Definition",
                                    explanation: "In minimal APIs, you define endpoints directly using methods like app.MapGet('/api/users', () => {...}).\n\nIt's like writing a simple function that responds to a web request, without needing a whole class structure.\n\nThis makes the code more straightforward for simple scenarios, but can become messy if you have many endpoints."
                                },
                                {
                                    name: "Reduced Ceremony",
                                    explanation: "Ceremony refers to the boilerplate code you need to write - things like class declarations, attributes, inheritance, etc.\n\nMinimal APIs reduce this ceremony by letting you write just the essential code.\n\nThink of it like texting versus writing a formal letter - sometimes you just need to send a quick message without all the formalities."
                                },
                                {
                                    name: "Action Filters",
                                    explanation: "Action filters in traditional controllers are reusable pieces of code that run before or after your endpoint logic. They're like middleware but specifically for controllers.\n\nFor example, you might have a filter that validates authentication on every endpoint automatically.\n\nMinimal APIs don't have this built-in structure, so you need to handle such cross-cutting concerns differently."
                                }
                            ]
                        },
                        {
                            id: 3,
                            question: "How do you implement and configure middleware in .NET Core 8?",
                            suggestedAnswer: "Middleware in .NET Core is configured in the request pipeline using the WebApplicationBuilder in Program.cs. You add middleware using methods like UseAuthentication, UseAuthorization, or custom middleware with Use, Map, or Run methods.\n\nThe order matters - middleware executes in the order added for requests and reverse order for responses. To create custom middleware, you implement a class with an InvokeAsync method that takes HttpContext and a RequestDelegate for the next middleware. You can also use inline middleware with app.Use.\n\nFor example, I've implemented custom middleware for request logging, correlation ID injection, and tenant identification in multi-tenant applications. At Liven Group during our .NET 6 migration, we created middleware to handle legacy authentication tokens during the transition period, allowing both old and new auth mechanisms to coexist.",
                            bulletPoints: [
                                "Configure in Program.cs using WebApplicationBuilder",
                                "Order matters: request flows top-to-bottom, response bottom-to-top",
                                "Built-in middleware: UseAuthentication, UseAuthorization, UseCors",
                                "Custom middleware: class with InvokeAsync(HttpContext, RequestDelegate)",
                                "Inline middleware using app.Use() for simple logic",
                                "Can short-circuit pipeline by not calling next()",
                                "Common uses: logging, authentication, error handling, request modification"
                            ],
                            keyConcepts: [
                                {
                                    name: "Request Pipeline",
                                    explanation: "The request pipeline is like an assembly line for HTTP requests. Each middleware component is a station on that line that can inspect, modify, or respond to the request.\n\nThe request passes through each middleware in order, and then the response travels back through them in reverse.\n\nIf any middleware decides to respond early (short-circuit), the request doesn't continue down the line."
                                },
                                {
                                    name: "RequestDelegate",
                                    explanation: "RequestDelegate is a function pointer to the next middleware in the pipeline. When you create custom middleware, you receive this delegate and call it to pass control to the next component.\n\nThink of it like a relay race baton - you do your work, then pass the baton (call next()) to continue the race.\n\nIf you don't call next(), you're ending the race early."
                                },
                                {
                                    name: "Middleware Order",
                                    explanation: "The order you add middleware is critical because it determines what runs first. For example, exception handling should be early so it catches errors from all other middleware.\n\nAuthentication must come before authorization (you need to know who someone is before checking what they can do).\n\nIt's like getting dressed - you put on underwear before pants, not after!"
                                }
                            ]
                        },
                        {
                            id: 4,
                            question: "What are the different service lifetimes in .NET Core DI (Singleton, Scoped, Transient) and when would you use each?",
                            suggestedAnswer: "There are three service lifetimes in .NET Core dependency injection. Singleton creates one instance for the entire application lifetime - use this for stateless services like loggers or configuration. Scoped creates one instance per HTTP request or scope - perfect for database contexts, unit of work patterns, or request-specific data. Transient creates a new instance every time it's requested - use for lightweight stateless services.\n\nThe key rule is never inject a shorter-lived service into a longer-lived one, like injecting a scoped DbContext into a singleton service, as this causes captive dependency issues.\n\nAt BIPO, we use singleton for our configuration services and caching layers, scoped for Entity Framework DbContext and our unit of work implementations across microservices, and transient for simple data transformation services and validators that have no state.",
                            bulletPoints: [
                                "Singleton: one instance for application lifetime",
                                "Scoped: one instance per HTTP request or scope",
                                "Transient: new instance every time requested",
                                "Singleton use cases: loggers, configuration, caches",
                                "Scoped use cases: DbContext, unit of work, request data",
                                "Transient use cases: lightweight stateless services",
                                "Never inject shorter-lived services into longer-lived ones (captive dependency)"
                            ],
                            keyConcepts: [
                                {
                                    name: "Service Lifetime",
                                    explanation: "Service lifetime determines how long an instance of a service lives and when it gets disposed.\n\nThink of it like renting an apartment: Singleton is buying a house (you keep it forever), Scoped is renting monthly (new tenant each month/request), and Transient is a hotel room (new room every night/injection).\n\nChoosing the right lifetime affects both performance and correctness."
                                },
                                {
                                    name: "Captive Dependency",
                                    explanation: "A captive dependency occurs when a long-lived service (like Singleton) holds a reference to a short-lived service (like Scoped).\n\nIt's like keeping last month's milk in your fridge - the milk should have been thrown out, but because it's captured by the fridge, it stays around too long and causes problems.\n\nThis commonly happens with DbContext being injected into singletons."
                                },
                                {
                                    name: "Scope",
                                    explanation: "A scope is a boundary for service lifetime. In web applications, each HTTP request creates a new scope automatically.\n\nAll scoped services within that request share the same instance, but a new request gets fresh instances.\n\nYou can also create manual scopes using IServiceScopeFactory for background tasks or batch operations."
                                }
                            ]
                        },
                        {
                            id: 5,
                            question: "How do you handle configuration in .NET Core 8 (appsettings.json, environment variables, Azure Key Vault)?",
                            suggestedAnswer: "Configuration in .NET Core uses a layered approach where later sources override earlier ones. The default order is appsettings.json, then appsettings.{Environment}.json, then environment variables, then command-line arguments.\n\nFor sensitive data, I integrate Azure Key Vault using AddAzureKeyVault extension. Environment variables are great for container deployments as they override file-based config without changing code. I use the Options pattern with IOptions or IOptionsSnapshot to inject strongly-typed configuration.\n\nFor multi-environment setups, I keep common settings in appsettings.json, environment-specific overrides in appsettings.Development.json or appsettings.Production.json, and secrets in Key Vault. At Liven Group, we used AWS Parameter Store similarly, with Terraform managing the infrastructure, and environment variables in Docker containers overriding base configuration for different deployment targets across Australia, Indonesia, and Singapore.",
                            bulletPoints: [
                                "Layered configuration: appsettings.json â†’ environment-specific â†’ env vars â†’ command-line",
                                "Later sources override earlier ones",
                                "Azure Key Vault for sensitive data (connection strings, API keys)",
                                "Environment variables ideal for containerized deployments",
                                "Use Options pattern for strongly-typed configuration",
                                "appsettings.{Environment}.json for environment-specific settings",
                                "Never commit secrets to source control"
                            ],
                            keyConcepts: [
                                {
                                    name: "Configuration Providers",
                                    explanation: "Configuration providers are sources where your app reads settings from. .NET Core checks multiple providers in order and merges them together, with later providers winning conflicts.\n\nThink of it like layering transparent sheets - you see through to earlier layers, but if a later layer has something written, it covers what's below.\n\nThis lets you have base settings in files and override them with environment variables."
                                },
                                {
                                    name: "Environment Variables",
                                    explanation: "Environment variables are key-value pairs set at the operating system or container level. They're perfect for configuration because they don't require changing files or redeploying code.\n\nIn .NET Core, you use double underscores to represent nested configuration (e.g., ConnectionStrings__DefaultConnection becomes ConnectionStrings:DefaultConnection in your config)."
                                },
                                {
                                    name: "Azure Key Vault",
                                    explanation: "Azure Key Vault is a secure cloud service for storing secrets, keys, and certificates. Instead of putting database passwords in config files, you store them in Key Vault and your application fetches them at startup.\n\nIt's like having a safe deposit box at a bank instead of hiding money under your mattress - much more secure and you get audit logs of who accessed what."
                                }
                            ]
                        },
                        {
                            id: 6,
                            question: "Explain the Options pattern in .NET Core and how you use it for strongly-typed configuration.",
                            suggestedAnswer: "The Options pattern provides a way to access configuration using strongly-typed classes instead of raw strings. You create a POCO class matching your configuration structure, then register it with services.Configure in Program.cs. You inject it using IOptions, IOptionsSnapshot, or IOptionsMonitor depending on your needs.\n\nIOptions is singleton and reads config once at startup. IOptionsSnapshot is scoped and re-reads per request, useful for multi-tenant scenarios. IOptionsMonitor is singleton but reloads when configuration changes. You can also add validation using data annotations or IValidateOptions.\n\nThis pattern prevents typos, provides IntelliSense, and makes configuration testable. At BIPO, we use this extensively for feature flags, API endpoint configurations, and database connection settings across our microservices, with IOptionsSnapshot for tenant-specific settings that vary per request in our multi-country HR system.",
                            bulletPoints: [
                                "Strongly-typed configuration using POCO classes",
                                "Register with services.Configure<TOptions>()",
                                "IOptions: singleton, reads once at startup",
                                "IOptionsSnapshot: scoped, re-reads per request",
                                "IOptionsMonitor: singleton, reloads on config changes",
                                "Supports validation via data annotations",
                                "Provides IntelliSense and compile-time safety"
                            ],
                            keyConcepts: [
                                {
                                    name: "POCO Configuration Class",
                                    explanation: "POCO stands for Plain Old CLR Object - just a simple class with properties that match your configuration structure.\n\nFor example, if your appsettings.json has a 'Database' section with 'ConnectionString' and 'Timeout' properties, you create a DatabaseOptions class with those same properties.\n\nThis gives you type safety and IntelliSense when accessing configuration."
                                },
                                {
                                    name: "IOptions vs IOptionsSnapshot vs IOptionsMonitor",
                                    explanation: "These three interfaces serve different needs. IOptions reads configuration once at startup and never changes - fast but static.\n\nIOptionsSnapshot reads fresh configuration for each HTTP request - good for per-request settings like tenant data.\n\nIOptionsMonitor watches for configuration file changes and reloads automatically - useful for feature flags that you want to change without restarting the app."
                                },
                                {
                                    name: "Options Validation",
                                    explanation: "You can validate your configuration at startup using data annotations like [Required] or [Range] on your options class.\n\nThis catches configuration errors early when the app starts, rather than failing at runtime when the config is first used.\n\nIt's like checking your car has gas before starting a road trip, rather than running out halfway there."
                                }
                            ]
                        },
                        {
                            id: 7,
                            question: "What is the difference between IHostedService and BackgroundService for scheduled tasks?",
                            suggestedAnswer: "IHostedService is the base interface for long-running background tasks in .NET Core, with StartAsync and StopAsync methods. BackgroundService is an abstract base class that implements IHostedService and provides a simpler ExecuteAsync method where you put your background logic.\n\nBackgroundService handles the boilerplate of managing cancellation tokens and graceful shutdown. For simple background tasks, use BackgroundService. For more control over startup and shutdown, implement IHostedService directly.\n\nI typically use BackgroundService for scheduled jobs like data synchronization or cleanup tasks. At Liven Group, we used BackgroundService for scheduled menu synchronization across restaurant locations and for processing queued orders from SNS. For more complex scenarios requiring precise control over lifecycle, like coordinating multiple services during shutdown, I implement IHostedService directly.",
                            bulletPoints: [
                                "IHostedService: interface with StartAsync and StopAsync",
                                "BackgroundService: abstract class implementing IHostedService",
                                "BackgroundService provides ExecuteAsync method for background logic",
                                "BackgroundService handles cancellation token management",
                                "Use BackgroundService for simple scheduled tasks",
                                "Use IHostedService for complex lifecycle control",
                                "Both registered with AddHostedService()"
                            ],
                            keyConcepts: [
                                {
                                    name: "Hosted Service Lifecycle",
                                    explanation: "Hosted services start when the application starts and stop when it shuts down. StartAsync runs during application startup (before the app accepts requests), and StopAsync runs during shutdown (after the app stops accepting new requests).\n\nThis makes them perfect for background tasks that should run for the entire application lifetime, like message queue processors or scheduled jobs."
                                },
                                {
                                    name: "ExecuteAsync Method",
                                    explanation: "ExecuteAsync is the main method in BackgroundService where you put your background work. It runs on a background thread and receives a CancellationToken that signals when the application is shutting down.\n\nYou typically have a loop that runs until cancellation is requested.\n\nThink of it like a worker that keeps doing their job until they're told to go home."
                                },
                                {
                                    name: "Graceful Shutdown",
                                    explanation: "Graceful shutdown means giving your background tasks time to finish their current work before the application terminates. When StopAsync is called, you should signal your tasks to stop (via CancellationToken) and wait for them to complete.\n\nIt's like giving employees a 5-minute warning before closing the office, so they can save their work rather than just cutting the power."
                                }
                            ]
                        },
                        {
                            id: 8,
                            question: "How do you implement health checks in .NET Core for monitoring application status?",
                            suggestedAnswer: "Health checks in .NET Core allow you to expose endpoints that report application health status. You add health checks with builder.Services.AddHealthChecks() and map the endpoint with app.MapHealthChecks.\n\nYou can create custom health checks by implementing IHealthCheck with a CheckHealthAsync method that returns Healthy, Degraded, or Unhealthy. Common checks include database connectivity, external API availability, and disk space. You can have multiple endpoints like /health/live for liveness (is the app running) and /health/ready for readiness (is the app ready to accept traffic).\n\nThis is crucial for Kubernetes probes and load balancer health monitoring. At BIPO, we implement health checks for our SQL Server, MongoDB, Redis connections, and external API dependencies. Each microservice exposes both liveness and readiness probes, allowing Kubernetes to automatically restart unhealthy pods and route traffic only to ready instances.",
                            bulletPoints: [
                                "Add with AddHealthChecks() and map with MapHealthChecks()",
                                "Implement IHealthCheck for custom checks",
                                "Returns Healthy, Degraded, or Unhealthy status",
                                "Common checks: database, external APIs, disk space, memory",
                                "Separate endpoints for liveness and readiness",
                                "Integrates with Kubernetes probes and load balancers",
                                "Can include detailed diagnostic information"
                            ],
                            keyConcepts: [
                                {
                                    name: "Liveness vs Readiness",
                                    explanation: "Liveness checks answer 'Is the application alive?' - if it fails, the container should be restarted. Readiness checks answer 'Is the application ready to serve traffic?' - if it fails, stop sending requests but don't restart.\n\nFor example, your app might be alive but not ready if it's still warming up caches.\n\nThink of liveness as checking if someone is breathing, and readiness as checking if they're awake and ready to work."
                                },
                                {
                                    name: "Health Check Status",
                                    explanation: "Health checks return three possible statuses: Healthy (everything is fine), Degraded (working but with issues, like slow database), and Unhealthy (critical failure).\n\nDegraded is useful for alerting without taking the service offline.\n\nIt's like a traffic light: green (healthy), yellow (degraded - proceed with caution), red (unhealthy - stop)."
                                },
                                {
                                    name: "Kubernetes Probes",
                                    explanation: "Kubernetes uses health check endpoints to manage containers. The liveness probe checks if a container is alive - if it fails repeatedly, Kubernetes restarts it.\n\nThe readiness probe checks if a container can handle requests - if it fails, Kubernetes removes it from the load balancer temporarily.\n\nThis enables self-healing infrastructure where unhealthy services are automatically recovered."
                                }
                            ]
                        },
                        {
                            id: 9,
                            question: "Explain async/await in C# and best practices for asynchronous programming in .NET Core.",
                            suggestedAnswer: "Async/await enables non-blocking asynchronous programming in C#. When you await an async operation like a database call or HTTP request, the thread is freed to handle other work instead of blocking. The async keyword marks a method as asynchronous, and await pauses execution until the awaited task completes, then resumes.\n\nBest practices include: always await async methods (don't use .Result or .Wait() as they cause deadlocks), use ConfigureAwait(false) in library code to avoid capturing context, prefer Task over async void except for event handlers, and propagate async all the way up (async all the way).\n\nAvoid mixing sync and async code. Use ValueTask for hot paths where the operation often completes synchronously. At BIPO with our microservices architecture, we use async/await extensively for database operations, external API calls, and message queue processing, which significantly improves throughput by allowing each service instance to handle more concurrent requests.",
                            bulletPoints: [
                                "Async/await enables non-blocking asynchronous operations",
                                "Frees threads to handle other work while waiting",
                                "Always await async methods, never use .Result or .Wait()",
                                "Use ConfigureAwait(false) in library code",
                                "Avoid async void except for event handlers",
                                "Propagate async all the way up the call stack",
                                "Use ValueTask for frequently synchronous operations"
                            ],
                            keyConcepts: [
                                {
                                    name: "Thread Pool Efficiency",
                                    explanation: "When you await an I/O operation, the thread doesn't sit idle waiting - it returns to the thread pool to handle other requests.\n\nWhen the I/O completes, a thread (possibly a different one) picks up where you left off.\n\nIt's like a waiter taking multiple orders instead of standing at one table waiting for the kitchen - they can serve more customers with the same number of waiters."
                                },
                                {
                                    name: "Deadlock with .Result",
                                    explanation: "Calling .Result or .Wait() on a Task in certain contexts (like ASP.NET) can cause deadlocks. This happens because .Result blocks the current thread waiting for the task, but the task needs that same thread to complete.\n\nIt's like waiting for yourself to finish - you're stuck forever.\n\nAlways use await instead."
                                },
                                {
                                    name: "ConfigureAwait(false)",
                                    explanation: "By default, await captures the current synchronization context and resumes on it. ConfigureAwait(false) tells await not to capture the context, which improves performance in library code.\n\nIn ASP.NET Core, this is less critical than in older frameworks, but it's still good practice for libraries.\n\nThink of it like not caring which checkout lane you return to at a store - any lane will do."
                                }
                            ]
                        },
                        {
                            id: 10,
                            question: "What are records in C# and how do they differ from classes?",
                            suggestedAnswer: "Records are reference types introduced in C# 9 designed for immutable data models. The key differences from classes are: records provide value-based equality by default (comparing property values, not references), built-in ToString() with all properties, with-expressions for non-destructive mutation, and positional syntax for concise declarations.\n\nRecords are perfect for DTOs, API models, and domain value objects where you want immutability and value semantics. Classes use reference equality by default and are better for entities with identity and mutable state. Records can be declared as record class (reference type) or record struct (value type).\n\nI use records extensively for API request/response models, configuration objects, and domain events. At BIPO, we use records for all our API contracts and domain events in our event-driven architecture, which makes the code more concise and prevents accidental mutations.",
                            bulletPoints: [
                                "Records are reference types with value-based equality",
                                "Immutable by default with init-only properties",
                                "Built-in ToString() showing all properties",
                                "With-expressions for non-destructive mutation",
                                "Positional syntax: record Person(string Name, int Age)",
                                "Perfect for DTOs, API models, value objects",
                                "Classes better for entities with identity and mutable state"
                            ],
                            keyConcepts: [
                                {
                                    name: "Value-Based Equality",
                                    explanation: "Records compare equality based on property values, not object identity. Two record instances with the same property values are considered equal, even if they're different objects in memory.\n\nWith classes, two instances are only equal if they're the exact same object (reference equality).\n\nIt's like comparing two $20 bills - they're equal because of their value, not because they're the same physical bill."
                                },
                                {
                                    name: "With-Expressions",
                                    explanation: "With-expressions create a copy of a record with some properties changed, leaving the original unchanged. For example: var newPerson = person with { Age = 30 }.\n\nThis is perfect for immutable data structures where you need to 'modify' values without actually mutating them.\n\nIt's like making a photocopy of a document and changing one field on the copy."
                                },
                                {
                                    name: "Positional Records",
                                    explanation: "Positional records use a concise syntax: record Person(string Name, int Age). This automatically creates properties, a constructor, and deconstruction.\n\nIt's shorthand that reduces boilerplate code dramatically.\n\nInstead of writing 10 lines of code for a simple data class, you write one line."
                                }
                            ]
                        },
                        {
                            id: 11,
                            question: "How do you implement global exception handling in .NET Core Web API?",
                            suggestedAnswer: "There are several approaches to global exception handling in .NET Core. The most common is using exception handling middleware with app.UseExceptionHandler() or custom middleware. You can also use an exception filter by implementing IExceptionFilter or IAsyncExceptionFilter.\n\nFor middleware approach, create a custom middleware that wraps the next() call in try-catch, logs the exception, and returns a standardized error response. For filters, create a class implementing IExceptionFilter and register it globally. I prefer middleware for truly global handling and filters for controller-specific logic.\n\nThe response should include appropriate status codes, error messages, and correlation IDs for tracing. Never expose stack traces in production. At Liven Group during our .NET 6 migration, we implemented global exception middleware that logged to CloudWatch, returned standardized error responses with correlation IDs, and handled different exception types appropriately - validation errors as 400, not found as 404, and unexpected errors as 500.",
                            bulletPoints: [
                                "Use app.UseExceptionHandler() for built-in exception handling",
                                "Create custom middleware with try-catch around next()",
                                "Implement IExceptionFilter for filter-based approach",
                                "Log exceptions with correlation IDs for tracing",
                                "Return standardized error responses with appropriate status codes",
                                "Never expose stack traces or sensitive data in production",
                                "Handle different exception types differently (validation, not found, server error)"
                            ],
                            keyConcepts: [
                                {
                                    name: "Exception Middleware",
                                    explanation: "Exception middleware sits early in the pipeline and wraps all subsequent middleware in a try-catch block. When any exception bubbles up, it catches it, logs it, and returns a user-friendly error response.\n\nIt's like a safety net at a circus - no matter where someone falls from, the net catches them.\n\nThis ensures no unhandled exceptions crash your application or leak sensitive information."
                                },
                                {
                                    name: "Correlation ID",
                                    explanation: "A correlation ID is a unique identifier for each request that's included in logs and error responses. When an error occurs, the user gets this ID, and support can use it to find the exact logs for that request.\n\nIt's like a tracking number for a package - it lets you trace the entire journey of that specific request through your system, even across multiple services."
                                },
                                {
                                    name: "Exception Filters",
                                    explanation: "Exception filters are attributes or global filters that handle exceptions specifically for MVC/API controllers. They run after middleware but before the response is sent.\n\nThey're useful for controller-specific exception handling, like converting domain exceptions to appropriate HTTP responses.\n\nThink of them as specialized handlers that understand the context of web requests better than generic middleware."
                                }
                            ]
                        },
                        {
                            id: 12,
                            question: "Explain the purpose and implementation of filters in ASP.NET Core (Action, Authorization, Exception, Resource, Result).",
                            suggestedAnswer: "Filters in ASP.NET Core are attributes that run code before or after specific stages in the request pipeline. Authorization filters run first and check if the user can access the resource. Resource filters run before model binding and after authorization - useful for caching.\n\nAction filters run before and after action method execution - good for logging or modifying arguments. Exception filters handle exceptions from actions. Result filters run before and after the action result executes - useful for modifying responses.\n\nYou implement filters by inheriting from appropriate base classes or interfaces and registering them globally, per controller, or per action. Filters provide cross-cutting concerns without cluttering action methods. At BIPO, we use action filters for audit logging, authorization filters for multi-tenant access control, and result filters for response formatting and HATEOAS link injection in our REST APIs.",
                            bulletPoints: [
                                "Authorization filters: check user permissions (run first)",
                                "Resource filters: run before model binding, useful for caching",
                                "Action filters: run before/after action execution, for logging",
                                "Exception filters: handle exceptions from actions",
                                "Result filters: run before/after result execution, modify responses",
                                "Apply globally, per controller, or per action",
                                "Provide cross-cutting concerns without cluttering code"
                            ],
                            keyConcepts: [
                                {
                                    name: "Filter Pipeline Order",
                                    explanation: "Filters execute in a specific order: Authorization â†’ Resource â†’ Model Binding â†’ Action â†’ Exception (if error) â†’ Result. Understanding this order is crucial for proper implementation.\n\nFor example, authorization must run before action to prevent unauthorized access.\n\nIt's like security checkpoints at an airport - you go through them in a specific order, and each one serves a different purpose."
                                },
                                {
                                    name: "Cross-Cutting Concerns",
                                    explanation: "Cross-cutting concerns are functionalities that affect multiple parts of your application, like logging, security, or caching.\n\nFilters let you implement these concerns once and apply them across many actions without duplicating code.\n\nInstead of adding logging code to every action method, you write one logging filter and apply it everywhere."
                                },
                                {
                                    name: "Filter Scope",
                                    explanation: "Filters can be applied at three scopes: globally (affects all actions), controller-level (affects all actions in that controller), or action-level (affects only that specific action).\n\nYou can also control order with the Order property.\n\nThis flexibility lets you apply broad policies while still allowing specific overrides where needed."
                                }
                            ]
                        },
                        {
                            id: 13,
                            question: "What is the difference between ValueTask and Task in .NET?",
                            suggestedAnswer: "Task is a reference type that always allocates on the heap, while ValueTask is a value type that can avoid heap allocation when the operation completes synchronously. ValueTask is useful for hot paths where operations often complete synchronously, like reading from a cache that's usually in memory.\n\nHowever, ValueTask has restrictions: it should only be awaited once, and you shouldn't call .Result or .Wait() on it. For most scenarios, use Task. Use ValueTask only when profiling shows allocation pressure from frequently synchronous operations.\n\nValueTask can wrap either a result or a Task, making it efficient for both sync and async paths. At BIPO, we use ValueTask in our caching layer where cache hits (synchronous) are common, and only cache misses require actual async database calls. This reduced our GC pressure significantly in high-throughput scenarios.",
                            bulletPoints: [
                                "Task: reference type, always heap allocated",
                                "ValueTask: value type, avoids allocation for synchronous completion",
                                "Use Task by default for most scenarios",
                                "Use ValueTask for hot paths with frequent synchronous completion",
                                "ValueTask restrictions: await only once, no .Result/.Wait()",
                                "ValueTask reduces GC pressure in high-throughput scenarios",
                                "Profile before optimizing - premature optimization is costly"
                            ],
                            keyConcepts: [
                                {
                                    name: "Heap Allocation",
                                    explanation: "Every time you create a Task, it allocates memory on the heap, which later needs garbage collection. In high-throughput scenarios with millions of operations, these allocations add up and cause GC pressure.\n\nValueTask, being a value type, can live on the stack when the operation completes synchronously, avoiding this allocation.\n\nIt's like using a reusable water bottle instead of disposable ones - less waste."
                                },
                                {
                                    name: "Synchronous Completion",
                                    explanation: "An operation completes synchronously when the result is immediately available without waiting. For example, reading from an in-memory cache is synchronous, while reading from a database is asynchronous.\n\nIf your async method often completes synchronously (like 90% cache hits), ValueTask can optimize away the Task allocation for those cases while still supporting the async path when needed."
                                },
                                {
                                    name: "ValueTask Restrictions",
                                    explanation: "ValueTask is designed for a single consumption - you await it once and you're done. You can't await it multiple times or store it for later like you can with Task.\n\nThis is because ValueTask might be backed by a pooled object that gets reused.\n\nBreaking these rules leads to subtle bugs. Think of it like a ticket that gets torn when used - you can't reuse it."
                                }
                            ]
                        },
                        {
                            id: 14,
                            question: "How do you implement rate limiting in .NET Core 8?",
                            suggestedAnswer: ".NET 8 includes built-in rate limiting middleware. You configure it with builder.Services.AddRateLimiter() and specify policies like Fixed Window, Sliding Window, Token Bucket, or Concurrency. Fixed Window allows X requests per time window. Sliding Window is similar but smoother. Token Bucket refills tokens at a rate, allowing bursts. Concurrency limits simultaneous requests.\n\nYou apply policies globally or per endpoint with [EnableRateLimiting] attribute. You can also create custom policies. The middleware returns 429 Too Many Requests when limits are exceeded.\n\nFor distributed systems, consider external solutions like Redis-based rate limiting. At BIPO, we use fixed window rate limiting on our public APIs to prevent abuse, with different limits for authenticated vs anonymous users. For internal microservice communication, we use concurrency limiters to prevent cascading failures when downstream services slow down.",
                            bulletPoints: [
                                "Built-in rate limiting in .NET 8 with AddRateLimiter()",
                                "Fixed Window: X requests per time period",
                                "Sliding Window: smoother than fixed window",
                                "Token Bucket: allows bursts with token refill",
                                "Concurrency: limits simultaneous requests",
                                "Apply globally or per endpoint with [EnableRateLimiting]",
                                "Returns 429 Too Many Requests when exceeded"
                            ],
                            keyConcepts: [
                                {
                                    name: "Fixed vs Sliding Window",
                                    explanation: "Fixed window counts requests in fixed time periods (e.g., 100 requests per minute starting at :00). This can allow 200 requests in a short burst (100 at :59 and 100 at 1:00).\n\nSliding window tracks requests over a rolling time period, preventing this burst issue.\n\nIt's like the difference between a monthly subscription that resets on the 1st versus one that resets exactly 30 days from when you signed up."
                                },
                                {
                                    name: "Token Bucket Algorithm",
                                    explanation: "Token bucket starts with a bucket of tokens. Each request consumes a token. Tokens refill at a constant rate. If the bucket is empty, requests are rejected.\n\nThis allows bursts (using accumulated tokens) while maintaining an average rate.\n\nIt's like a coffee shop punch card that refills one punch per day - you can save up punches for a busy day."
                                },
                                {
                                    name: "Distributed Rate Limiting",
                                    explanation: "When you have multiple servers, each server's in-memory rate limiter works independently, so the total limit is multiplied by the number of servers.\n\nFor true distributed rate limiting, you need a shared state store like Redis where all servers check and update the same counters.\n\nThis ensures the limit applies across your entire cluster, not per server."
                                }
                            ]
                        },
                        {
                            id: 15,
                            question: "Explain LINQ and provide examples of complex query operations you've implemented.",
                            suggestedAnswer: "LINQ (Language Integrated Query) provides a consistent query syntax for different data sources like collections, databases, and XML. It uses method syntax with lambda expressions or query syntax similar to SQL. Common operations include Where (filter), Select (project), GroupBy, Join, OrderBy, and aggregate functions.\n\nLINQ to Entities translates to SQL for databases. Complex examples I've implemented include: multi-level grouping with aggregations for reporting, left joins with multiple conditions, nested queries with SelectMany for flattening hierarchies, and combining multiple data sources.\n\nAt Liven Group, I used LINQ extensively for complex restaurant analytics - grouping orders by location and time period with revenue calculations, joining menu items with sales data across multiple databases (SQL Server and PostgreSQL), and using projection to optimize queries by selecting only needed fields. At BIPO, I implemented complex payroll calculations using LINQ to aggregate employee data across multiple countries with different tax rules.",
                            bulletPoints: [
                                "Language Integrated Query for consistent query syntax",
                                "Works with collections, databases (EF), XML, and more",
                                "Method syntax: collection.Where(x => x.Age > 18).Select(x => x.Name)",
                                "Query syntax: from x in collection where x.Age > 18 select x.Name",
                                "Common operations: Where, Select, GroupBy, Join, OrderBy, Sum, Count",
                                "LINQ to Entities translates to SQL for database queries",
                                "Deferred execution: query runs when enumerated, not when defined"
                            ],
                            keyConcepts: [
                                {
                                    name: "Deferred Execution",
                                    explanation: "LINQ queries don't execute when you write them - they execute when you enumerate the results (foreach, ToList(), Count(), etc.).\n\nThis means you can build up complex queries step by step, and the actual data retrieval happens only once at the end.\n\nIt's like writing a shopping list throughout the week but only going to the store on Saturday - you make one trip with the complete list."
                                },
                                {
                                    name: "Expression Trees",
                                    explanation: "When you use LINQ with Entity Framework, your lambda expressions are converted to expression trees - data structures representing the code. EF then translates these to SQL.\n\nThis is why you can't use arbitrary C# methods in LINQ to Entities - they need to be translatable to SQL.\n\nIt's like having a translator who can only translate certain phrases, not everything you might say."
                                },
                                {
                                    name: "IEnumerable vs IQueryable",
                                    explanation: "IEnumerable executes queries in memory using LINQ to Objects. IQueryable builds expression trees and executes on the data source (like SQL Server).\n\nIf you call ToList() too early, you pull all data into memory and filter there. Keep queries as IQueryable until the last moment to let the database do the heavy lifting.\n\nIt's like filtering water at the source versus filling a pool and filtering it after."
                                }
                            ]
                        }
                    ]
                },
                {
                    id: 2,
                    name: "API Design & Swagger",
                    questions: [
                        {
                            id: 16,
                            question: "What are RESTful API design principles and how do you apply them?",
                            suggestedAnswer: "RESTful APIs follow six key principles: client-server architecture, statelessness, cacheability, layered system, uniform interface, and code on demand (optional). The uniform interface includes resource identification via URIs, manipulation through representations, self-descriptive messages, and HATEOAS.\n\nUse HTTP verbs correctly: GET for retrieval, POST for creation, PUT for full updates, PATCH for partial updates, DELETE for removal. Resources should be nouns, not verbs. Use proper status codes: 2xx for success, 4xx for client errors, 5xx for server errors.\n\nKeep APIs stateless - each request contains all needed information. Design for idempotency where possible. At BIPO, our invoice management API follows REST principles with resource-based URIs like /api/invoices/{id}, proper HTTP verb usage, and consistent response formats. We use HATEOAS for navigation links and implement caching headers for frequently accessed resources.",
                            bulletPoints: [
                                "Use HTTP verbs correctly: GET, POST, PUT, PATCH, DELETE",
                                "Resources as nouns in URIs: /api/users, not /api/getUsers",
                                "Stateless: each request self-contained",
                                "Proper status codes: 2xx success, 4xx client error, 5xx server error",
                                "Idempotent operations where possible (PUT, DELETE)",
                                "Use HTTP headers for metadata (Content-Type, Authorization)",
                                "HATEOAS for discoverability (optional but recommended)"
                            ],
                            keyConcepts: [
                                {
                                    name: "Statelessness",
                                    explanation: "Stateless means the server doesn't store any client context between requests. Each request must contain all information needed to process it (like authentication tokens).\n\nThis makes scaling easier because any server can handle any request - you don't need sticky sessions.\n\nIt's like going to a different bank teller each time and showing your ID every time, rather than one teller remembering you."
                                },
                                {
                                    name: "Idempotency",
                                    explanation: "An idempotent operation produces the same result no matter how many times you call it. PUT and DELETE should be idempotent - updating a user with the same data twice has the same effect as once, deleting something twice doesn't cause an error.\n\nThis is important for reliability when requests might be retried.\n\nIt's like pressing an elevator button multiple times - the elevator still only comes once."
                                },
                                {
                                    name: "Resource-Based Design",
                                    explanation: "REST APIs are organized around resources (things) rather than actions. You use HTTP verbs to specify the action on the resource.\n\nFor example, GET /users/123 gets a user, DELETE /users/123 deletes them. This is cleaner than having /getUser?id=123 and /deleteUser?id=123.\n\nResources should be nouns representing business entities."
                                }
                            ]
                        },
                        {
                            id: 17,
                            question: "How do you version APIs in .NET Core and what are the different versioning strategies?",
                            suggestedAnswer: "There are four main API versioning strategies. URL versioning puts version in the path: /api/v1/users. Query string versioning uses a parameter: /api/users?api-version=1.0. Header versioning uses custom headers: X-API-Version: 1.0. Media type versioning uses Accept header: application/vnd.company.v1+json.\n\nIn .NET Core, use Microsoft.AspNetCore.Mvc.Versioning package. Configure with AddApiVersioning() and specify how to read version (URL, query, header). You can support multiple versions simultaneously and deprecate old versions gracefully.\n\nURL versioning is most visible and cache-friendly. Header versioning keeps URLs clean. I prefer URL versioning for public APIs due to clarity and ease of testing. At BIPO, we use URL versioning for our public APIs with a deprecation policy that gives clients 6 months notice before removing old versions.",
                            bulletPoints: [
                                "URL versioning: /api/v1/users (most visible, cache-friendly)",
                                "Query string: /api/users?api-version=1.0",
                                "Header versioning: X-API-Version: 1.0 (clean URLs)",
                                "Media type: Accept: application/vnd.company.v1+json",
                                "Use Microsoft.AspNetCore.Mvc.Versioning package",
                                "Support multiple versions simultaneously",
                                "Implement deprecation policy for old versions"
                            ],
                            keyConcepts: [
                                {
                                    name: "Breaking vs Non-Breaking Changes",
                                    explanation: "Breaking changes require a new API version because they break existing clients (removing fields, changing data types, changing behavior). Non-breaking changes don't require new versions (adding optional fields, new endpoints).\n\nUnderstanding this distinction helps you decide when to bump versions.\n\nIt's like renovating a house - adding a room is non-breaking, but changing the front door location breaks everyone's muscle memory."
                                },
                                {
                                    name: "API Deprecation",
                                    explanation: "Deprecation is the process of phasing out old API versions. You announce a version is deprecated, give clients time to migrate, then eventually remove it.\n\nGood practice includes setting deprecation headers, documenting migration guides, and providing reasonable timelines.\n\nIt's like a store announcing it's closing - you give customers advance notice and time to find alternatives."
                                },
                                {
                                    name: "Version Negotiation",
                                    explanation: "Version negotiation is how the API determines which version to use when a client makes a request. You can have default versions, require explicit versions, or support version ranges.\n\nThe versioning middleware handles this automatically based on your configuration.\n\nThink of it like a restaurant menu with seasonal items - the server knows which version of the menu to bring based on what you ask for."
                                }
                            ]
                        },
                        {
                            id: 18,
                            question: "Explain how to configure and customize Swagger/OpenAPI documentation in .NET Core.",
                            suggestedAnswer: "Swagger/OpenAPI documentation is configured using Swashbuckle.AspNetCore package. Add services with AddSwaggerGen() in Program.cs, configure metadata like title, version, description, and contact info. Use XML comments by enabling XML documentation file in project settings and calling IncludeXmlComments().\n\nCustomize with operation filters, document filters, and schema filters for advanced scenarios. Add examples using [SwaggerOperation] and [SwaggerResponse] attributes. Configure authentication schemes with AddSecurityDefinition() and AddSecurityRequirement(). Group endpoints by tags.\n\nCustomize UI with SwaggerUI options. At BIPO, we generate comprehensive API documentation with XML comments on all public endpoints, include request/response examples, document all possible status codes, and configure JWT bearer authentication in Swagger UI so developers can test authenticated endpoints directly from the documentation.",
                            bulletPoints: [
                                "Install Swashbuckle.AspNetCore package",
                                "Configure with AddSwaggerGen() and UseSwagger()/UseSwaggerUI()",
                                "Enable XML comments for detailed documentation",
                                "Add metadata: title, version, description, contact",
                                "Configure authentication with AddSecurityDefinition()",
                                "Use attributes: [SwaggerOperation], [SwaggerResponse]",
                                "Customize with filters: IOperationFilter, IDocumentFilter"
                            ],
                            keyConcepts: [
                                {
                                    name: "OpenAPI Specification",
                                    explanation: "OpenAPI (formerly Swagger) is a standard format for describing REST APIs. It's a JSON/YAML document that describes all your endpoints, parameters, responses, and authentication.\n\nTools can read this spec to generate documentation, client SDKs, and test cases.\n\nIt's like a blueprint for your API that both humans and machines can understand."
                                },
                                {
                                    name: "XML Documentation Comments",
                                    explanation: "XML comments (/// <summary>) in your C# code can be included in Swagger documentation. This lets you write API docs right next to the code, keeping them in sync.\n\nWhen you enable XML documentation file generation, Swashbuckle reads these comments and adds them to the OpenAPI spec.\n\nIt's like adding sticky notes to your code that also appear in the user manual."
                                },
                                {
                                    name: "Swagger Filters",
                                    explanation: "Filters let you customize how Swagger generates documentation. Operation filters modify individual endpoints, document filters modify the entire spec, and schema filters modify model schemas.\n\nYou might use filters to add custom headers, hide internal endpoints, or enrich examples.\n\nThey're like post-processing steps that refine the generated documentation."
                                }
                            ]
                        },
                        {
                            id: 19,
                            question: "How do you implement authentication and authorization in Swagger UI?",
                            suggestedAnswer: "To add authentication to Swagger UI, configure security definitions in AddSwaggerGen(). For JWT Bearer, use AddSecurityDefinition with type Bearer and scheme bearer. Then add AddSecurityRequirement to apply it globally or use [Authorize] attributes on controllers.\n\nFor OAuth2, configure flows (authorization code, implicit, etc.) with token and authorization URLs. In Swagger UI, users click the Authorize button, enter credentials or tokens, and subsequent requests include authentication headers. You can also configure API keys or Basic authentication.\n\nAt BIPO, we implement JWT Bearer authentication in Swagger with AddSecurityDefinition specifying the bearer scheme. Developers can click Authorize, paste their JWT token, and test all authenticated endpoints. This significantly improves developer experience as they can test the entire API including secured endpoints directly from the documentation.",
                            bulletPoints: [
                                "Configure security in AddSwaggerGen() with AddSecurityDefinition()",
                                "JWT Bearer: type Bearer, scheme bearer",
                                "OAuth2: configure flows with token/authorization URLs",
                                "API Key: specify in header, query, or cookie",
                                "Add AddSecurityRequirement() for global application",
                                "Users click Authorize button in Swagger UI",
                                "Supports multiple authentication schemes simultaneously"
                            ],
                            keyConcepts: [
                                {
                                    name: "Security Definitions",
                                    explanation: "Security definitions tell Swagger what authentication methods your API supports. You define the type (Bearer, OAuth2, API Key, Basic), where to send credentials (header, query, cookie), and any additional parameters.\n\nThis information is used to render the Authorize button and dialog in Swagger UI.\n\nIt's like telling the documentation what keys are needed to unlock different doors."
                                },
                                {
                                    name: "Security Requirements",
                                    explanation: "Security requirements specify which endpoints need which authentication. You can apply them globally (all endpoints), per controller, or per action.\n\nThis determines which endpoints show the lock icon in Swagger UI.\n\nWhen you authorize in Swagger, it automatically adds the auth headers to requests for endpoints with security requirements."
                                },
                                {
                                    name: "Bearer Token",
                                    explanation: "Bearer token authentication means the client sends a token (usually JWT) in the Authorization header as 'Bearer {token}'. The server validates this token to authenticate the request.\n\nIn Swagger UI, users paste their token once, and it's automatically included in all subsequent requests.\n\nIt's like showing a VIP pass at a club - once you show it, you're recognized for the rest of the night."
                                }
                            ]
                        },
                        {
                            id: 20,
                            question: "What are the best practices for API error handling and status code usage?",
                            suggestedAnswer: "Use appropriate HTTP status codes: 200 OK for success, 201 Created for resource creation, 204 No Content for successful deletion, 400 Bad Request for validation errors, 401 Unauthorized for missing/invalid authentication, 403 Forbidden for insufficient permissions, 404 Not Found for missing resources, 409 Conflict for business rule violations, 422 Unprocessable Entity for semantic errors, 429 Too Many Requests for rate limiting, 500 Internal Server Error for unexpected errors, 503 Service Unavailable for temporary outages.\n\nReturn consistent error response format with error code, message, and details. Include correlation IDs for tracing. Use ProblemDetails (RFC 7807) format. Never expose stack traces or sensitive information in production.\n\nLog all errors with context. At BIPO, we use a standardized error response format across all microservices with error codes, user-friendly messages, field-level validation errors, and correlation IDs that link to our distributed tracing system.",
                            bulletPoints: [
                                "2xx: Success (200 OK, 201 Created, 204 No Content)",
                                "4xx: Client errors (400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found)",
                                "5xx: Server errors (500 Internal Server Error, 503 Service Unavailable)",
                                "Use ProblemDetails (RFC 7807) for consistent error format",
                                "Include correlation IDs for tracing",
                                "Provide field-level validation errors",
                                "Never expose stack traces or sensitive data in production"
                            ],
                            keyConcepts: [
                                {
                                    name: "ProblemDetails Format",
                                    explanation: "ProblemDetails is a standard format (RFC 7807) for error responses in HTTP APIs. It includes type (error category), title (short description), status (HTTP code), detail (explanation), and instance (correlation ID).\n\nUsing this standard makes your API consistent with industry practices and easier for clients to handle.\n\nIt's like having a standard form for reporting issues - everyone knows where to find the information they need."
                                },
                                {
                                    name: "4xx vs 5xx Errors",
                                    explanation: "4xx errors mean the client made a mistake (bad request, unauthorized, not found) - the client should fix something and retry. 5xx errors mean the server had a problem - the client's request was fine, but the server couldn't process it.\n\nThis distinction helps clients know whether to retry (5xx) or fix their request (4xx).\n\nIt's like the difference between 'you wrote the wrong address' (4xx) versus 'the post office is closed' (5xx)."
                                },
                                {
                                    name: "Error Granularity",
                                    explanation: "Good error responses provide enough detail for clients to fix the problem without exposing security risks. For validation errors, specify which fields failed and why.\n\nFor business rule violations, explain what rule was broken. But never expose implementation details, database errors, or stack traces.\n\nIt's like telling someone their password is wrong without revealing what the correct password is."
                                }
                            ]
                        },
                        {
                            id: 21,
                            question: "How do you implement pagination, filtering, and sorting in Web APIs?",
                            suggestedAnswer: "Pagination prevents loading large datasets by returning data in pages. Use query parameters like pageNumber and pageSize (e.g., /api/users?pageNumber=1&pageSize=20). Return metadata in headers or response body: totalCount, totalPages, hasNext, hasPrevious.\n\nFor filtering, use query parameters matching property names (/api/users?status=active&role=admin). For sorting, use a sort parameter with field and direction (/api/users?sortBy=lastName&sortOrder=desc). Implement using LINQ Skip() and Take() for pagination, Where() for filtering, and OrderBy() for sorting.\n\nConsider cursor-based pagination for large datasets. Return links to next/previous pages (HATEOAS). At Liven Group, I implemented pagination for restaurant order lists with cursor-based pagination for real-time order streams, and at BIPO for employee lists across multiple countries with complex filtering by country, department, and employment status, plus sorting by multiple fields.",
                            bulletPoints: [
                                "Pagination: pageNumber and pageSize query parameters",
                                "Return metadata: totalCount, totalPages, hasNext, hasPrevious",
                                "Filtering: property-based query parameters",
                                "Sorting: sortBy and sortOrder parameters",
                                "Implement with LINQ: Skip(), Take(), Where(), OrderBy()",
                                "Cursor-based pagination for large/real-time datasets",
                                "Include navigation links (next, previous, first, last)"
                            ],
                            keyConcepts: [
                                {
                                    name: "Offset vs Cursor Pagination",
                                    explanation: "Offset pagination uses page numbers (skip X, take Y). It's simple but can miss or duplicate items if data changes between requests.\n\nCursor pagination uses a unique identifier (like last seen ID) to fetch the next batch. It's more reliable for real-time data but harder to implement.\n\nThink of offset like saying 'give me page 5' versus cursor like saying 'give me items after ID 12345'."
                                },
                                {
                                    name: "Pagination Metadata",
                                    explanation: "Pagination metadata tells clients about the dataset: how many total items, how many pages, whether there are more pages.\n\nYou can return this in response headers (X-Total-Count, Link header) or in the response body.\n\nThis lets clients build proper UI pagination controls. It's like a book telling you 'page 5 of 200' so you know how much more there is to read."
                                },
                                {
                                    name: "Query Parameter Design",
                                    explanation: "Good query parameter design makes APIs intuitive. Use consistent naming (camelCase or snake_case), support multiple filters, allow combining filters with AND logic, and document all options.\n\nFor complex scenarios, consider POST with a filter object in the body instead of dozens of query parameters.\n\nKeep it simple for common cases, powerful for complex ones."
                                }
                            ]
                        },
                        {
                            id: 22,
                            question: "Explain content negotiation in ASP.NET Core Web API.",
                            suggestedAnswer: "Content negotiation is how the client and server agree on the response format. The client specifies preferred formats using the Accept header (e.g., Accept: application/json or Accept: application/xml). ASP.NET Core examines this header and returns data in the requested format if supported.\n\nBy default, ASP.NET Core supports JSON. To add XML support, call AddXmlSerializerFormatters() or AddXmlDataContractSerializerFormatters(). You can create custom formatters for other formats like CSV or Protocol Buffers.\n\nThe framework automatically serializes your response objects to the negotiated format. If the requested format isn't supported, it returns 406 Not Acceptable or falls back to a default format. At BIPO, we support both JSON and XML for our invoice API to accommodate different client systems, with JSON as the default and XML available for legacy integrations.",
                            bulletPoints: [
                                "Client specifies format with Accept header",
                                "Server returns data in requested format if supported",
                                "Default support for JSON in ASP.NET Core",
                                "Add XML with AddXmlSerializerFormatters()",
                                "Create custom formatters for CSV, Protocol Buffers, etc.",
                                "Returns 406 Not Acceptable if format unsupported",
                                "Can configure default format and fallback behavior"
                            ],
                            keyConcepts: [
                                {
                                    name: "Accept Header",
                                    explanation: "The Accept header tells the server what content types the client can handle, with optional quality values for preferences (e.g., Accept: application/json, application/xml;q=0.9).\n\nThe server picks the best match. If you don't specify Accept, you typically get the default format (JSON).\n\nIt's like ordering at a restaurant and saying 'I prefer steak, but chicken is okay too'."
                                },
                                {
                                    name: "Output Formatters",
                                    explanation: "Output formatters are components that serialize your C# objects into specific formats (JSON, XML, etc.). ASP.NET Core includes JSON formatter by default.\n\nYou can add more formatters or create custom ones by implementing IOutputFormatter. The framework automatically picks the right formatter based on content negotiation.\n\nThink of formatters as translators that convert your data into different languages."
                                },
                                {
                                    name: "Media Types",
                                    explanation: "Media types (MIME types) identify content formats: application/json for JSON, application/xml for XML, text/csv for CSV, etc.\n\nThey're standardized identifiers so clients and servers speak the same language about formats.\n\nYou can also define custom media types for your API-specific formats, like application/vnd.company.invoice+json."
                                }
                            ]
                        },
                        {
                            id: 23,
                            question: "How do you implement CORS in .NET Core and what security considerations should you keep in mind?",
                            suggestedAnswer: "CORS (Cross-Origin Resource Sharing) allows web applications from different domains to access your API. Configure it in Program.cs with AddCors() to define policies and UseCors() to apply them. You can allow specific origins, all origins (not recommended for production), specific HTTP methods, and specific headers.\n\nFor credentials (cookies, auth headers), you must specify exact origins, not wildcards. Apply CORS globally or per controller/action. Security considerations: never use AllowAnyOrigin with AllowCredentials, whitelist specific origins in production, be careful with AllowAnyHeader, validate origin dynamically if needed, and consider CORS as one layer of security, not the only one.\n\nAt BIPO, we configure CORS to allow our React frontend domains (development, staging, production) with credentials enabled for JWT cookies, and we dynamically validate origins against a configuration list for multi-tenant scenarios.",
                            bulletPoints: [
                                "Configure with AddCors() and apply with UseCors()",
                                "Specify allowed origins, methods, and headers",
                                "Never use AllowAnyOrigin() with AllowCredentials()",
                                "Whitelist specific origins in production",
                                "Apply globally or per controller/action",
                                "CORS is browser security, not API security",
                                "Consider preflight request caching for performance"
                            ],
                            keyConcepts: [
                                {
                                    name: "Same-Origin Policy",
                                    explanation: "Browsers enforce same-origin policy: a web page can only make requests to the same domain it came from. This prevents malicious sites from stealing data.\n\nCORS is a way to selectively relax this restriction by having the server say 'I allow requests from these other domains'.\n\nIt's like a bouncer at a club who normally only lets in members, but the owner can say 'also let in people from this other club'."
                                },
                                {
                                    name: "Preflight Requests",
                                    explanation: "For certain requests (like PUT, DELETE, or custom headers), browsers send a preflight OPTIONS request first to check if CORS allows it. The server responds with allowed methods and headers.\n\nOnly if approved does the browser send the actual request. This adds overhead, so you can cache preflight responses.\n\nIt's like asking permission before doing something, rather than doing it and apologizing later."
                                },
                                {
                                    name: "CORS vs API Security",
                                    explanation: "CORS is a browser security feature - it doesn't protect your API from non-browser clients like Postman or cURL. You still need proper authentication and authorization.\n\nCORS just prevents unauthorized websites from making requests from users' browsers.\n\nThink of CORS as controlling which websites can use your API through a browser, not controlling who can access your API in general."
                                }
                            ]
                        },
                        {
                            id: 24,
                            question: "What is HATEOAS and when would you implement it in your API?",
                            suggestedAnswer: "HATEOAS (Hypermedia As The Engine Of Application State) is a REST constraint where responses include links to related resources and available actions. Instead of clients hardcoding URLs, the API tells them what they can do next. For example, a GET /orders/123 response includes links to update, cancel, or view related customer.\n\nThis makes APIs self-documenting and evolvable - you can change URLs without breaking clients. Implement HATEOAS when building public APIs, when you want loose coupling between client and server, or when you need API discoverability.\n\nIt adds complexity, so skip it for simple internal APIs or when clients are tightly coupled anyway. At BIPO, we implement HATEOAS in our public invoice API, where each invoice response includes links to related actions (pay, download PDF, view customer) and navigation (next/previous invoices), making it easier for third-party integrators to discover and use our API without extensive documentation.",
                            bulletPoints: [
                                "Include hypermedia links in responses",
                                "Links show available actions and related resources",
                                "Makes APIs self-documenting and discoverable",
                                "Allows URL changes without breaking clients",
                                "Use for public APIs and loose coupling",
                                "Skip for simple internal APIs (adds complexity)",
                                "Common formats: HAL, JSON:API, Siren"
                            ],
                            keyConcepts: [
                                {
                                    name: "Hypermedia Links",
                                    explanation: "Hypermedia links in API responses tell clients what actions are available and how to perform them. Each link has a relation (rel) describing what it does and an href with the URL.\n\nFor example, a 'self' link points to the current resource, a 'next' link to the next page.\n\nIt's like a website with navigation links - you don't need to know all URLs upfront, you just follow the links."
                                },
                                {
                                    name: "API Discoverability",
                                    explanation: "Discoverability means clients can explore your API by following links, without reading extensive documentation. Start at a root endpoint, see what links are available, follow them to discover more.\n\nThis is especially valuable for public APIs where you don't control the clients.\n\nIt's like exploring a city by following street signs versus needing a detailed map memorized in advance."
                                },
                                {
                                    name: "Loose Coupling",
                                    explanation: "HATEOAS enables loose coupling because clients don't hardcode URLs - they follow links provided by the server. If you change a URL structure, clients still work because they're following the links you provide.\n\nThis makes your API more maintainable and evolvable.\n\nIt's like giving someone directions turn-by-turn versus giving them an address - if the route changes, turn-by-turn directions still get them there."
                                }
                            ]
                        },
                        {
                            id: 25,
                            question: "How do you handle file uploads and downloads in Web API?",
                            suggestedAnswer: "For file uploads, use IFormFile parameter with [FromForm] attribute. For multiple files, use IFormFileCollection or List<IFormFile>. Validate file size, type, and content before processing. Stream large files instead of loading entirely into memory.\n\nSave to disk, database, or cloud storage like Azure Blob Storage. For downloads, return File() result with file stream, content type, and filename. Use FileStreamResult for large files to stream them. Set appropriate headers: Content-Disposition for download behavior, Content-Type for file type.\n\nImplement resumable uploads for large files using chunking. Consider virus scanning for uploaded files. At Liven Group, we handled menu image uploads to S3 with validation and resizing. At BIPO, we process bulk employee data CSV uploads with streaming to handle files with hundreds of thousands of rows, and generate PDF payslips for download with proper content disposition headers.",
                            bulletPoints: [
                                "Upload: use IFormFile with [FromForm] attribute",
                                "Validate file size, type, and content",
                                "Stream large files, don't load entirely into memory",
                                "Save to Azure Blob Storage, S3, or local disk",
                                "Download: return File() with stream, content type, filename",
                                "Set Content-Disposition header for download behavior",
                                "Consider chunked uploads for large files"
                            ],
                            keyConcepts: [
                                {
                                    name: "File Streaming",
                                    explanation: "Streaming means processing a file in chunks rather than loading it entirely into memory. This is crucial for large files - a 1GB file would consume 1GB of RAM if loaded fully.\n\nWith streaming, you read and process small chunks at a time, keeping memory usage constant.\n\nIt's like drinking from a water fountain versus filling a bucket first - much more efficient for large amounts."
                                },
                                {
                                    name: "Content-Disposition Header",
                                    explanation: "Content-Disposition tells the browser how to handle the file: 'inline' displays it in the browser (like PDFs), 'attachment' triggers a download. You can also specify the filename.\n\nFor example: Content-Disposition: attachment; filename=invoice.pdf.\n\nThis controls whether users see the file or download it. It's like the difference between showing someone a document versus handing them a copy to take home."
                                },
                                {
                                    name: "File Validation",
                                    explanation: "Never trust file uploads. Validate file size to prevent DoS attacks, check file extensions and MIME types to prevent malicious uploads, and ideally scan content for viruses.\n\nDon't rely solely on client-side validation - always validate server-side. Store uploaded files outside the web root to prevent execution.\n\nIt's like checking IDs at a door - you can't just trust everyone who walks in."
                                }
                            ]
                        }
                    ]
                },
                {
                    id: 3,
                    name: "Entity Framework & SQL",
                    questions: [
                        {
                            id: 26,
                            question: "What is Entity Framework Core and how does it differ from Entity Framework 6?",
                            suggestedAnswer: "Entity Framework Core is a lightweight, cross-platform ORM (Object-Relational Mapper) that's a complete rewrite of EF6. Key differences: EF Core is cross-platform (Windows, Linux, macOS) while EF6 is Windows-only. EF Core is lighter and faster with better performance. EF Core supports more database providers including NoSQL. EF Core has better LINQ translation and query performance. EF Core uses a simpler configuration API with fluent API and data annotations. However, EF6 has some features EF Core initially lacked like lazy loading by default (now available via proxies), automatic migrations, and some advanced mapping scenarios. EF Core is the future and actively developed. At Liven Group, we migrated from EF6 to EF Core during our .NET 6 upgrade, benefiting from improved query performance and cross-platform support for our Docker deployments. At BIPO, we use EF Core exclusively across all microservices with SQL Server, MongoDB provider, and DynamoDB.",
                            bulletPoints: [
                                "EF Core: cross-platform, lightweight, modern rewrite",
                                "EF6: Windows-only, mature, feature-complete",
                                "EF Core: better performance and LINQ translation",
                                "EF Core: supports more database providers",
                                "EF Core: simpler configuration with fluent API",
                                "EF Core: no lazy loading by default (opt-in with proxies)",
                                "EF Core is actively developed, EF6 is maintenance mode"
                            ],
                            keyConcepts: [
                                {
                                    name: "ORM (Object-Relational Mapper)",
                                    explanation: "An ORM maps database tables to C# classes and rows to objects, letting you work with databases using object-oriented code instead of writing SQL. You query and manipulate objects, and the ORM translates to SQL behind the scenes. It's like having a translator between your C# code and the database - you speak C#, the database speaks SQL, and the ORM translates."
                                },
                                {
                                    name: "Cross-Platform Support",
                                    explanation: "EF Core runs on Windows, Linux, and macOS, while EF6 only runs on Windows. This is crucial for Docker containers and cloud deployments where you might use Linux for cost savings. Cross-platform means you can develop on Mac, test on Linux, and deploy anywhere. It's like a car that runs on any type of road versus one that only works on highways."
                                },
                                {
                                    name: "Database Providers",
                                    explanation: "Database providers are plugins that let EF Core work with different databases. There are providers for SQL Server, PostgreSQL, MySQL, SQLite, Cosmos DB, and even in-memory databases for testing. You can switch providers with minimal code changes. It's like having universal adapters that let you plug your device into any outlet worldwide."
                                }
                            ]
                        },
                        {
                            id: 27,
                            question: "Explain the difference between Code First, Database First, and Model First approaches in EF Core.",
                            suggestedAnswer: "Code First means you write C# entity classes first, and EF Core generates the database schema from them using migrations. This is the most common approach in modern development, giving you full control over your domain model. Database First means you have an existing database and use scaffolding (dotnet ef dbcontext scaffold) to generate entity classes from it. This is useful for legacy databases or when the database is designed separately. Model First (using visual designers) is not supported in EF Core - it was an EF6 feature. In EF Core, you choose between Code First or Database First. Code First is preferred for new projects as it keeps your code as the source of truth and enables version control of schema changes through migrations. At BIPO, we use Code First exclusively for all new microservices, defining entities in code and using migrations to evolve the schema. At Liven Group during migration, we used Database First to scaffold existing databases, then switched to Code First for ongoing development.",
                            bulletPoints: [
                                "Code First: write C# classes, generate database from code",
                                "Database First: existing database, scaffold C# classes from it",
                                "Model First: not supported in EF Core (was EF6 feature)",
                                "Code First: best for new projects, code is source of truth",
                                "Database First: useful for legacy databases",
                                "Code First uses migrations for schema changes",
                                "Database First uses scaffolding command"
                            ],
                            keyConcepts: [
                                {
                                    name: "Migrations",
                                    explanation: "Migrations are version-controlled schema changes. When you modify entity classes in Code First, you create a migration that describes the changes (add column, create table, etc.). You can apply migrations to update the database or roll them back. It's like Git commits for your database schema - each migration is a checkpoint you can move forward or backward through."
                                },
                                {
                                    name: "Scaffolding",
                                    explanation: "Scaffolding is the process of generating C# entity classes and DbContext from an existing database. EF Core reads the database schema and creates corresponding classes. This is a one-time generation, though you can re-scaffold if the database changes. Think of it like reverse engineering - you're working backwards from the database to create code."
                                },
                                {
                                    name: "Source of Truth",
                                    explanation: "The source of truth is the authoritative definition of your data model. In Code First, it's your C# classes. In Database First, it's the database schema. Having code as source of truth is generally better because code can be version controlled, reviewed, and tested more easily than database schemas. It's like having the recipe (code) versus just the finished dish (database)."
                                }
                            ]
                        },
                        {
                            id: 28,
                            question: "How do you optimize Entity Framework queries for performance?",
                            suggestedAnswer: "Key EF Core optimization techniques: Use AsNoTracking() for read-only queries to avoid change tracking overhead. Use Select() to project only needed columns instead of loading entire entities. Implement eager loading with Include() to avoid N+1 problems. Use compiled queries for frequently executed queries. Avoid loading navigation properties you don't need. Use pagination with Skip() and Take() for large datasets. Use AsSplitQuery() for multiple Include() to avoid cartesian explosion. Profile queries with logging or tools like EF Core Plus. Add appropriate database indexes. Use raw SQL for complex queries that don't translate well. Batch operations instead of individual saves. At Liven Group, I optimized restaurant order queries by using AsNoTracking for read-only dashboards, adding indexes on frequently queried columns, and using Select projections to load only display fields. At BIPO, we use compiled queries for frequently-run payroll calculations and split queries for complex employee data with multiple includes.",
                            bulletPoints: [
                                "Use AsNoTracking() for read-only queries",
                                "Project with Select() to load only needed columns",
                                "Eager load with Include() to prevent N+1 queries",
                                "Use compiled queries for frequently executed queries",
                                "Implement pagination with Skip() and Take()",
                                "Use AsSplitQuery() to avoid cartesian explosion",
                                "Add database indexes on frequently queried columns",
                                "Use raw SQL for complex queries"
                            ],
                            keyConcepts: [
                                {
                                    name: "Change Tracking",
                                    explanation: "Change tracking is EF Core's mechanism for detecting changes to entities so it can generate UPDATE statements. It keeps a snapshot of original values and compares them when you call SaveChanges(). This has overhead - memory and CPU. For read-only queries where you won't modify data, use AsNoTracking() to skip this overhead. It's like the difference between borrowing a book you might write notes in versus just reading it at the library."
                                },
                                {
                                    name: "Cartesian Explosion",
                                    explanation: "Cartesian explosion happens when you Include() multiple collections in one query. EF Core joins them, creating duplicate parent data for each combination of children. A parent with 10 items in collection A and 10 in collection B results in 100 rows. AsSplitQuery() runs separate queries for each collection, avoiding this. It's like getting one big package versus multiple smaller packages - sometimes separate is more efficient."
                                },
                                {
                                    name: "Compiled Queries",
                                    explanation: "Compiled queries are pre-compiled LINQ expressions that EF Core doesn't need to translate every time. For queries executed frequently with different parameters, compiling them once improves performance. It's like having a template letter where you just fill in the name, versus writing a new letter from scratch each time."
                                }
                            ]
                        },
                        {
                            id: 29,
                            question: "What is the N+1 query problem and how do you solve it in EF Core?",
                            suggestedAnswer: "The N+1 problem occurs when you load a collection of entities (1 query), then access a navigation property for each entity (N queries), resulting in N+1 total queries. For example, loading 100 orders then accessing order.Customer for each causes 101 queries. This severely impacts performance. Solutions: Use eager loading with Include() to load related data in the initial query. Use Select() to project needed data in one query. Use explicit loading with Load() if you need related data conditionally. Enable lazy loading carefully (it can hide N+1 problems). Use AsSplitQuery() for multiple includes. Profile your queries to detect N+1 issues. At BIPO, I encountered N+1 when displaying employee lists with department names - initially 1 query for employees plus 1 per employee for department. Fixed by adding Include(e => e.Department), reducing to 1 query. Always check generated SQL with logging or profilers.",
                            bulletPoints: [
                                "N+1: one query for collection, N queries for related data",
                                "Causes severe performance degradation",
                                "Solution: eager loading with Include()",
                                "Solution: projection with Select()",
                                "Solution: explicit loading with Load() when conditional",
                                "Avoid lazy loading or use carefully",
                                "Profile queries to detect N+1 problems",
                                "Check generated SQL with logging"
                            ],
                            keyConcepts: [
                                {
                                    name: "Eager Loading",
                                    explanation: "Eager loading means loading related data upfront in the same query using Include(). Instead of separate queries for each relationship, everything loads together with JOINs. This solves N+1 by making it 1 query instead of N+1. It's like getting all your groceries in one trip versus making a separate trip for each item."
                                },
                                {
                                    name: "Lazy Loading",
                                    explanation: "Lazy loading automatically loads navigation properties when you access them. It's convenient but dangerous because it hides database calls and easily causes N+1 problems. In EF Core, lazy loading is opt-in using proxies. For most scenarios, explicit eager or explicit loading is better because you control when queries happen. It's like automatic bill pay - convenient but you might not notice unexpected charges."
                                },
                                {
                                    name: "Query Profiling",
                                    explanation: "Query profiling means monitoring what SQL queries EF Core generates. You can enable EF Core logging to see queries in console, use SQL Server Profiler, or tools like MiniProfiler. This helps you spot N+1 problems and other inefficiencies. It's like checking your car's fuel efficiency - you need to measure to know if there's a problem."
                                }
                            ]
                        },
                        {
                            id: 30,
                            question: "Explain eager loading, lazy loading, and explicit loading in EF Core.",
                            suggestedAnswer: "These are three strategies for loading related data. Eager loading uses Include() or ThenInclude() to load related entities in the initial query with JOINs. Lazy loading automatically loads navigation properties when accessed, requiring proxies and causing additional queries. Explicit loading uses Entry().Collection().Load() or Entry().Reference().Load() to manually load related data after the initial query. Eager loading is best for most scenarios as it's predictable and avoids N+1 problems. Lazy loading is convenient but can cause performance issues and is opt-in in EF Core. Explicit loading is useful when you conditionally need related data based on business logic. At BIPO, we primarily use eager loading with Include() for predictable performance, and explicit loading when we need to load related data based on user permissions or conditional logic that can't be determined upfront.",
                            bulletPoints: [
                                "Eager: Include() loads related data upfront with JOINs",
                                "Lazy: auto-loads when accessed, requires proxies, causes extra queries",
                                "Explicit: manually load with Entry().Collection().Load()",
                                "Eager loading: best for most scenarios, predictable",
                                "Lazy loading: convenient but risky, opt-in in EF Core",
                                "Explicit loading: useful for conditional loading",
                                "Choose based on access patterns and performance needs"
                            ],
                            keyConcepts: [
                                {
                                    name: "Navigation Properties",
                                    explanation: "Navigation properties are properties on your entity classes that reference related entities. For example, an Order entity might have a Customer navigation property. These properties let you navigate relationships in your object graph. How these properties get populated (eager, lazy, or explicit) determines when and how related data is loaded from the database."
                                },
                                {
                                    name: "Proxies",
                                    explanation: "Proxies are dynamically generated classes that inherit from your entities and override navigation properties to enable lazy loading. When you access a navigation property, the proxy intercepts it and loads data from the database. In EF Core, you need to install Microsoft.EntityFrameworkCore.Proxies and call UseLazyLoadingProxies() to enable this. It's like having a middleman who fetches data on demand."
                                },
                                {
                                    name: "Loading Strategy Trade-offs",
                                    explanation: "Each loading strategy has trade-offs. Eager loading makes one larger query upfront - good for predictable performance but might load unneeded data. Lazy loading makes many small queries as needed - convenient but unpredictable and prone to N+1. Explicit loading gives you control but requires more code. Choose based on your access patterns: if you always need the data, eager load it; if you rarely need it, explicitly load when necessary."
                                }
                            ]
                        },
                        {
                            id: 31,
                            question: "How do you implement database migrations in EF Core?",
                            suggestedAnswer: "Migrations track and apply database schema changes. Create a migration with 'dotnet ef migrations add MigrationName' after modifying entity classes. This generates a migration file with Up() and Down() methods. Apply migrations with 'dotnet ef database update' to update the database, or 'dotnet ef database update PreviousMigration' to rollback. You can generate SQL scripts with 'dotnet ef migrations script' for production deployments. Migrations are stored in code and version controlled. For production, use SQL scripts rather than automatic updates for safety. You can also apply migrations programmatically with context.Database.Migrate() at startup. At Liven Group during our .NET 6 migration, we used migrations to gradually evolve the schema across multiple databases. At BIPO, we generate SQL scripts from migrations and apply them through our CI/CD pipeline with approval gates for production.",
                            bulletPoints: [
                                "Create: dotnet ef migrations add MigrationName",
                                "Apply: dotnet ef database update",
                                "Rollback: dotnet ef database update PreviousMigration",
                                "Generate SQL: dotnet ef migrations script",
                                "Migrations stored in code, version controlled",
                                "Use SQL scripts for production (safer than auto-update)",
                                "Can apply programmatically with context.Database.Migrate()"
                            ],
                            keyConcepts: [
                                {
                                    name: "Up and Down Methods",
                                    explanation: "Each migration has Up() and Down() methods. Up() applies the migration (e.g., creates a table), Down() reverts it (e.g., drops the table). This lets you move forward or backward through your schema history. It's like having undo/redo for your database structure. EF Core generates these methods automatically based on your entity changes."
                                },
                                {
                                    name: "Migration History Table",
                                    explanation: "EF Core maintains a __EFMigrationsHistory table in your database that tracks which migrations have been applied. When you run 'database update', EF Core checks this table and applies any pending migrations. This ensures migrations are only applied once and in the correct order. It's like a logbook that records what changes have been made."
                                },
                                {
                                    name: "Production Migration Strategy",
                                    explanation: "For production, never use automatic migrations at startup - it's risky. Instead, generate SQL scripts from migrations, review them, test in staging, and apply through controlled deployment processes. This gives you visibility and control over schema changes. Some teams use database migration tools like DbUp or Flyway for additional control. It's like having a checklist and approval process for important changes."
                                }
                            ]
                        },
                        {
                            id: 32,
                            question: "What are indexes in SQL and how do you create them in EF Core?",
                            suggestedAnswer: "Indexes are database structures that speed up data retrieval by creating a sorted lookup table for specific columns. They're like a book's index - instead of scanning every page, you look up the topic and jump to the right page. In EF Core, create indexes using fluent API with HasIndex() in OnModelCreating, or use [Index] attribute on entity classes. You can create single-column or composite indexes, unique indexes, and filtered indexes. Indexes speed up queries but slow down inserts/updates and use storage. Index foreign keys, frequently queried columns, and columns used in WHERE, JOIN, and ORDER BY clauses. Avoid over-indexing. At Liven Group, I added indexes on order timestamps and restaurant IDs for dashboard queries, improving response times from seconds to milliseconds. At BIPO, we index employee numbers, email addresses, and frequently filtered fields like country and department.",
                            bulletPoints: [
                                "Indexes speed up queries by creating sorted lookups",
                                "Create with HasIndex() in OnModelCreating or [Index] attribute",
                                "Types: single-column, composite, unique, filtered",
                                "Index foreign keys and frequently queried columns",
                                "Indexes speed reads but slow writes and use storage",
                                "Index columns in WHERE, JOIN, ORDER BY clauses",
                                "Avoid over-indexing - balance read vs write performance"
                            ],
                            keyConcepts: [
                                {
                                    name: "Clustered vs Non-Clustered Index",
                                    explanation: "A clustered index determines the physical order of data in the table - there can only be one per table, usually the primary key. Non-clustered indexes create separate structures pointing to the data - you can have many of these. It's like the difference between organizing books by author on the shelf (clustered) versus having a card catalog that points to where books are (non-clustered)."
                                },
                                {
                                    name: "Composite Index",
                                    explanation: "A composite index covers multiple columns together. The order matters - an index on (LastName, FirstName) works for queries filtering by LastName or both, but not just FirstName. Use composite indexes when you frequently query multiple columns together. It's like a phone book sorted by last name then first name - you can find 'Smith, John' but can't efficiently find all Johns."
                                },
                                {
                                    name: "Index Overhead",
                                    explanation: "Every index speeds up reads but slows down writes because the index must be updated when data changes. Indexes also consume disk space. Too many indexes can hurt performance more than help. Analyze your query patterns and index strategically. It's like having too many bookmarks - they help find things but make the book heavier and harder to update."
                                }
                            ]
                        },
                        {
                            id: 33,
                            question: "Explain query execution plans and how to analyze them for optimization.",
                            suggestedAnswer: "A query execution plan shows how SQL Server executes a query - what indexes it uses, how it joins tables, and where it spends time. View plans in SQL Server Management Studio with 'Display Estimated Execution Plan' or 'Include Actual Execution Plan'. Look for table scans (bad - reads entire table), index seeks (good - uses index), high-cost operations, and missing index suggestions. Key metrics: relative cost percentages, number of rows, and actual vs estimated rows. Common issues: table scans instead of index seeks, key lookups, implicit conversions, and parameter sniffing. At Liven Group, I used execution plans to identify missing indexes on order queries and found a table scan on a million-row table that was fixed with an index, reducing query time from 5 seconds to 50ms. At BIPO, we regularly analyze execution plans for payroll calculation queries to ensure they scale across countries.",
                            bulletPoints: [
                                "Shows how SQL Server executes a query",
                                "View in SSMS with 'Display Execution Plan'",
                                "Look for: table scans (bad), index seeks (good)",
                                "Check relative cost percentages and row counts",
                                "Common issues: missing indexes, key lookups, implicit conversions",
                                "SQL Server suggests missing indexes",
                                "Compare estimated vs actual rows for statistics issues"
                            ],
                            keyConcepts: [
                                {
                                    name: "Table Scan vs Index Seek",
                                    explanation: "A table scan reads every row in a table to find matches - slow for large tables. An index seek uses an index to jump directly to matching rows - much faster. In execution plans, table scans appear as thick arrows and high costs. If you see table scans on large tables, you probably need an index. It's like reading an entire dictionary versus looking up a word in the index."
                                },
                                {
                                    name: "Key Lookup",
                                    explanation: "A key lookup happens when an index doesn't contain all needed columns, so SQL Server must look up additional data from the table. This is expensive. Fix it by creating a covering index (includes all needed columns) or using INCLUDE in the index. It's like finding a book in the card catalog but then having to walk to the shelf to get it - an extra step that slows things down."
                                },
                                {
                                    name: "Parameter Sniffing",
                                    explanation: "Parameter sniffing is when SQL Server creates an execution plan based on the first parameter values it sees, which might not be optimal for other values. This can cause performance issues if the first execution had atypical parameters. Solutions include query hints, recompiling, or using OPTIMIZE FOR. It's like tailoring a suit for the first person who walks in, then making everyone else wear that same size."
                                }
                            ]
                        },
                        {
                            id: 34,
                            question: "How do you implement transactions in EF Core?",
                            suggestedAnswer: "Transactions ensure multiple database operations succeed or fail together (ACID properties). EF Core automatically wraps SaveChanges() in a transaction. For explicit transactions, use context.Database.BeginTransaction(), perform operations, then call transaction.Commit() or transaction.Rollback(). Use 'using' statements for automatic disposal. For distributed transactions across multiple contexts or databases, use TransactionScope. You can also use database transactions with ExecuteSqlRaw. Set isolation levels to control concurrent access behavior. At BIPO, we use explicit transactions when processing payroll - we update employee records, create payment entries, and log audit trails all in one transaction, ensuring data consistency. If any step fails, everything rolls back. For multi-database scenarios like updating both SQL Server and MongoDB, we use the Saga pattern instead of distributed transactions.",
                            bulletPoints: [
                                "SaveChanges() automatically uses a transaction",
                                "Explicit: BeginTransaction(), Commit(), Rollback()",
                                "Use 'using' statements for automatic disposal",
                                "TransactionScope for distributed transactions",
                                "Set isolation levels for concurrency control",
                                "Transactions ensure ACID properties",
                                "Keep transactions short to avoid locking"
                            ],
                            keyConcepts: [
                                {
                                    name: "ACID Properties",
                                    explanation: "ACID stands for Atomicity (all or nothing), Consistency (data remains valid), Isolation (transactions don't interfere), and Durability (committed changes persist). Transactions guarantee these properties. For example, transferring money between accounts must be atomic - both debit and credit happen, or neither does. It's like a package deal - you get everything or nothing."
                                },
                                {
                                    name: "Isolation Levels",
                                    explanation: "Isolation levels control how transactions interact with concurrent operations. Read Uncommitted allows dirty reads, Read Committed prevents them, Repeatable Read prevents non-repeatable reads, Serializable prevents phantom reads. Higher isolation means more consistency but more locking and less concurrency. Choose based on your consistency requirements. It's like privacy settings - more privacy means less interaction with others."
                                },
                                {
                                    name: "Distributed Transactions",
                                    explanation: "Distributed transactions span multiple databases or services. They're complex and can cause performance issues. TransactionScope in .NET coordinates them, but many modern systems avoid distributed transactions in favor of eventual consistency patterns like Saga or event sourcing. It's like coordinating multiple people to do things simultaneously - possible but complicated, so sometimes it's better to do things in sequence with compensation if something fails."
                                }
                            ]
                        },
                        {
                            id: 35,
                            question: "What are the differences between AsNoTracking() and regular queries in EF Core?",
                            suggestedAnswer: "Regular queries enable change tracking - EF Core keeps a snapshot of entity values to detect changes for SaveChanges(). AsNoTracking() disables this, making queries faster and using less memory but preventing updates to those entities. Use AsNoTracking() for read-only scenarios like displaying lists, reports, or API responses where you won't modify data. Regular queries are needed when you'll update entities. AsNoTracking queries are 2-3x faster and use significantly less memory for large result sets. You can set AsNoTracking as default with ChangeTracker.QueryTrackingBehavior. At BIPO, we use AsNoTracking() for all read-only API endpoints and reports, significantly reducing memory usage in our invoice list endpoints that return hundreds of records. We only use tracking queries when we need to update entities.",
                            bulletPoints: [
                                "Regular queries: enable change tracking for updates",
                                "AsNoTracking(): no change tracking, read-only, faster",
                                "AsNoTracking() is 2-3x faster and uses less memory",
                                "Use AsNoTracking() for read-only scenarios",
                                "Use regular queries when updating entities",
                                "Can set as default with QueryTrackingBehavior",
                                "AsNoTracking() entities can't be updated via SaveChanges()"
                            ],
                            keyConcepts: [
                                {
                                    name: "Change Tracking Snapshot",
                                    explanation: "When EF Core tracks entities, it keeps a snapshot of their original values in memory. When you call SaveChanges(), it compares current values to the snapshot to generate UPDATE statements. This snapshot consumes memory and CPU. For 1000 entities, you're storing 2000 copies in memory (original + current). AsNoTracking() skips this entirely."
                                },
                                {
                                    name: "Identity Map",
                                    explanation: "EF Core's change tracker also maintains an identity map ensuring you get the same object instance for the same database row within a context. This prevents duplicate objects but adds overhead. AsNoTracking() bypasses this - you might get multiple instances for the same row, but queries are faster. It's like having a registry of who's who versus just meeting people without keeping track."
                                },
                                {
                                    name: "Performance Impact",
                                    explanation: "The performance difference between tracking and non-tracking queries grows with result set size. For 10 rows, the difference is negligible. For 10,000 rows, AsNoTracking() can be dramatically faster and use 50% less memory. Always use AsNoTracking() for large read-only queries. It's like the difference between taking notes in a meeting versus just listening - notes are useful if you'll reference them, but if you're just observing, skip the notes."
                                }
                            ]
                        },
                        {
                            id: 36,
                            question: "How do you write raw SQL queries in EF Core and when would you use them?",
                            suggestedAnswer: "EF Core supports raw SQL with FromSqlRaw() for queries and ExecuteSqlRaw() for commands. FromSqlRaw() returns entities that can be composed with LINQ. Use parameterized queries to prevent SQL injection: FromSqlRaw('SELECT * FROM Users WHERE Id = {0}', userId). For non-entity results, use SqlQuery<T>(). Use raw SQL when: LINQ doesn't translate well, you need database-specific features, performance optimization requires specific SQL, or calling stored procedures. However, prefer LINQ when possible for maintainability and database independence. At Liven Group, I used raw SQL for complex analytics queries with CTEs and window functions that LINQ couldn't express efficiently. At BIPO, we use stored procedures for complex payroll calculations that involve multiple steps and temporary tables, calling them with FromSqlRaw().",
                            bulletPoints: [
                                "FromSqlRaw() for queries returning entities",
                                "ExecuteSqlRaw() for INSERT, UPDATE, DELETE",
                                "SqlQuery<T>() for non-entity results",
                                "Always use parameterized queries (prevent SQL injection)",
                                "Can compose with LINQ after FromSqlRaw()",
                                "Use for: complex queries, stored procedures, performance optimization",
                                "Prefer LINQ when possible for maintainability"
                            ],
                            keyConcepts: [
                                {
                                    name: "SQL Injection",
                                    explanation: "SQL injection is a security vulnerability where attackers insert malicious SQL into your queries. Never concatenate user input into SQL strings. Always use parameterized queries where parameters are passed separately. EF Core's FromSqlRaw with {0} placeholders or FromSqlInterpolated with $'' syntax safely parameterizes values. It's like the difference between letting someone write on your form versus giving them a pre-printed form to fill in specific blanks."
                                },
                                {
                                    name: "Query Composition",
                                    explanation: "After FromSqlRaw(), you can add LINQ operations like Where(), OrderBy(), Take(). EF Core combines your SQL with the LINQ into a single query. This is powerful for building on raw SQL with additional filters. However, the raw SQL must return all columns of the entity. It's like starting with a base recipe and adding your own touches - you get the foundation from SQL and refine with LINQ."
                                },
                                {
                                    name: "Database Independence",
                                    explanation: "Raw SQL ties your code to a specific database dialect. If you switch from SQL Server to PostgreSQL, raw SQL might break. LINQ queries are database-independent - EF Core translates them to the appropriate SQL dialect. Use raw SQL only when necessary, and consider abstracting it behind repositories to isolate database-specific code. It's like writing in English versus using local slang - English works everywhere, slang only works locally."
                                }
                            ]
                        },
                        {
                            id: 37,
                            question: "Explain database normalization and denormalization strategies.",
                            suggestedAnswer: "Normalization organizes data to reduce redundancy and improve integrity through normal forms (1NF, 2NF, 3NF, BCNF). 1NF eliminates repeating groups, 2NF removes partial dependencies, 3NF removes transitive dependencies. Normalized databases avoid data anomalies and save storage but require more joins. Denormalization intentionally adds redundancy for performance by storing computed values or duplicating data to avoid joins. Use normalization for transactional systems (OLTP) where data integrity is critical. Use denormalization for read-heavy systems (OLAP), reporting, or when join performance is problematic. Common denormalization: storing aggregates, duplicating frequently joined data, or using materialized views. At Liven Group, we kept transactional tables normalized but created denormalized reporting tables with pre-computed sales totals. At BIPO, employee data is normalized but we denormalize frequently accessed fields like employee name and department name into payroll records to avoid joins in reports.",
                            bulletPoints: [
                                "Normalization: reduce redundancy, improve integrity",
                                "Normal forms: 1NF (no repeating groups), 2NF (no partial dependencies), 3NF (no transitive dependencies)",
                                "Denormalization: add redundancy for performance",
                                "Normalization: better for OLTP (transactional systems)",
                                "Denormalization: better for OLAP (reporting/analytics)",
                                "Common denormalization: computed values, duplicate data, materialized views",
                                "Balance data integrity vs query performance"
                            ],
                            keyConcepts: [
                                {
                                    name: "Data Anomalies",
                                    explanation: "Unnormalized data causes anomalies: insertion anomaly (can't add data without other data), update anomaly (must update multiple places), deletion anomaly (deleting one thing deletes unrelated data). For example, storing customer address with every order means updating address requires changing all orders. Normalization prevents these by storing each fact once. It's like having one master copy of a document versus many copies that can get out of sync."
                                },
                                {
                                    name: "OLTP vs OLAP",
                                    explanation: "OLTP (Online Transaction Processing) systems handle many small transactions - inserts, updates, deletes. They need normalized data for integrity. OLAP (Online Analytical Processing) systems handle complex queries and reports on large datasets. They benefit from denormalization for query speed. It's like the difference between a busy checkout counter (OLTP - many quick transactions) versus a research library (OLAP - complex analysis of lots of data)."
                                },
                                {
                                    name: "Materialized Views",
                                    explanation: "Materialized views are pre-computed query results stored as tables. They're a form of denormalization that speeds up complex queries by storing the results. You refresh them periodically or on-demand. They trade storage and staleness for query speed. It's like having a summary report ready to go versus generating it from scratch each time someone asks for it."
                                }
                            ]
                        },
                        {
                            id: 38,
                            question: "How do you implement soft deletes in EF Core?",
                            suggestedAnswer: "Soft deletes mark records as deleted without actually removing them, usually with an IsDeleted flag or DeletedAt timestamp. Implement by adding IsDeleted property to entities, creating a query filter in OnModelCreating with HasQueryFilter(e => !e.IsDeleted) to automatically exclude soft-deleted records, and overriding SaveChanges() to set IsDeleted instead of deleting. You can use interfaces like ISoftDelete for consistency. For audit trails, also track DeletedBy and DeletedAt. Query filters apply globally but can be ignored with IgnoreQueryFilters(). Soft deletes enable data recovery, audit trails, and maintaining referential integrity. At BIPO, we implement soft deletes on all major entities for compliance and audit requirements, allowing us to recover accidentally deleted data and maintain complete history for HR regulations across multiple countries.",
                            bulletPoints: [
                                "Add IsDeleted or DeletedAt property to entities",
                                "Use HasQueryFilter() to exclude soft-deleted records",
                                "Override SaveChanges() to set IsDeleted instead of deleting",
                                "Use ISoftDelete interface for consistency",
                                "Track DeletedBy and DeletedAt for audit",
                                "Use IgnoreQueryFilters() to include soft-deleted records",
                                "Benefits: data recovery, audit trails, referential integrity"
                            ],
                            keyConcepts: [
                                {
                                    name: "Query Filters",
                                    explanation: "Query filters are global filters applied to all queries for an entity type. When you set HasQueryFilter(e => !e.IsDeleted), EF Core automatically adds this WHERE clause to every query. This ensures soft-deleted records are hidden by default throughout your application. You don't need to remember to filter them manually. It's like having automatic spam filtering - emails go to spam automatically without you having to check each one."
                                },
                                {
                                    name: "SaveChanges Override",
                                    explanation: "Overriding SaveChanges() lets you intercept delete operations and convert them to updates that set IsDeleted = true. You check for entities in the Deleted state and change them to Modified with IsDeleted set. This makes soft deletes transparent to the rest of your application - code calls Remove() normally but records are soft-deleted. It's like having a trash can that doesn't actually throw things away, just moves them to a hidden folder."
                                },
                                {
                                    name: "Compliance and Audit",
                                    explanation: "Soft deletes are crucial for compliance in regulated industries. Many regulations require maintaining data history and audit trails. Soft deletes let you track who deleted what and when, and recover data if needed. Hard deletes make this impossible. In HR, financial, and healthcare systems, soft deletes are often mandatory. It's like keeping paper trails for legal purposes - you need to prove what happened and when."
                                }
                            ]
                        },
                        {
                            id: 39,
                            question: "What are stored procedures and how do you call them from EF Core?",
                            suggestedAnswer: "Stored procedures are pre-compiled SQL code stored in the database that can accept parameters and return results. They encapsulate complex logic, improve performance through compilation and caching, and provide security through controlled access. Call them in EF Core using FromSqlRaw() for queries or ExecuteSqlRaw() for commands. For output parameters, use SqlParameter with Direction = ParameterDirection.Output. Map stored procedures to entity types or use SqlQuery<T>() for custom result types. Stored procedures are useful for complex business logic, batch operations, or when DBAs manage database code separately. However, they reduce portability and make testing harder. At Liven Group, we used stored procedures for complex sales reporting with multiple CTEs. At BIPO, we have stored procedures for multi-step payroll calculations that involve temporary tables and complex tax logic that's easier to maintain in SQL.",
                            bulletPoints: [
                                "Pre-compiled SQL code stored in database",
                                "Call with FromSqlRaw() or ExecuteSqlRaw()",
                                "Use SqlParameter for input/output parameters",
                                "Benefits: performance, security, encapsulation",
                                "Drawbacks: reduced portability, harder testing",
                                "Useful for: complex logic, batch operations, DBA-managed code",
                                "Can return entities or custom result types"
                            ],
                            keyConcepts: [
                                {
                                    name: "Pre-compilation",
                                    explanation: "Stored procedures are compiled when created, not every time they run. The database creates an execution plan and caches it, making subsequent calls faster. Dynamic SQL is compiled each time. This performance benefit is less significant in modern databases with good query plan caching, but stored procedures still have an edge for complex operations. It's like having a recipe memorized versus reading it each time you cook."
                                },
                                {
                                    name: "Security Benefits",
                                    explanation: "Stored procedures can enhance security by granting users permission to execute procedures without direct table access. This prevents SQL injection and limits what operations users can perform. You can also audit procedure calls. However, this security model is less common in modern applications that use application-level security. It's like having a receptionist who can make appointments for you versus giving you direct access to the calendar."
                                },
                                {
                                    name: "Portability Trade-off",
                                    explanation: "Stored procedures tie your application to a specific database. Switching from SQL Server to PostgreSQL means rewriting procedures. They also make testing harder since you need a database to run them. Modern approaches favor keeping logic in application code for portability and testability. Use stored procedures when performance or database-side logic is critical, but understand the trade-offs. It's like building a custom solution versus using standard parts - custom might be better but harder to replace."
                                }
                            ]
                        },
                        {
                            id: 40,
                            question: "How do you handle concurrency conflicts in EF Core?",
                            suggestedAnswer: "Concurrency conflicts occur when multiple users modify the same data simultaneously. EF Core supports optimistic concurrency using concurrency tokens - typically a RowVersion/Timestamp column or properties marked with [ConcurrencyCheck]. When SaveChanges() detects a conflict (data changed since you loaded it), it throws DbUpdateConcurrencyException. Handle this by: refreshing data and retrying, showing conflict to user for resolution, or implementing last-write-wins. Configure concurrency tokens with IsRowVersion() or IsConcurrencyToken() in fluent API. For pessimistic concurrency, use database locks with transactions and isolation levels. At BIPO, we use RowVersion on critical entities like payroll records. When conflicts occur, we reload the entity with current values, show both versions to the user, and let them decide how to merge changes. This prevents one user's changes from silently overwriting another's.",
                            bulletPoints: [
                                "Optimistic concurrency: detect conflicts at save time",
                                "Use RowVersion/Timestamp or [ConcurrencyCheck] properties",
                                "DbUpdateConcurrencyException thrown on conflict",
                                "Handle by: refresh and retry, user resolution, last-write-wins",
                                "Configure with IsRowVersion() or IsConcurrencyToken()",
                                "Pessimistic concurrency: use database locks",
                                "Choose strategy based on conflict likelihood and business rules"
                            ],
                            keyConcepts: [
                                {
                                    name: "Optimistic vs Pessimistic Concurrency",
                                    explanation: "Optimistic concurrency assumes conflicts are rare - you don't lock data, just detect conflicts when saving. Pessimistic concurrency assumes conflicts are common - you lock data when reading so others can't modify it. Optimistic is better for web applications with many users and low conflict probability. Pessimistic is better for high-conflict scenarios but reduces concurrency. It's like assuming your parking spot will be available (optimistic) versus reserving it (pessimistic)."
                                },
                                {
                                    name: "RowVersion/Timestamp",
                                    explanation: "RowVersion (SQL Server) or Timestamp is a special column that automatically increments with each update. EF Core includes this value in UPDATE's WHERE clause. If the value changed since you loaded the row, the UPDATE affects 0 rows and EF Core throws an exception. This reliably detects conflicts without checking every column. It's like having a version number on a document - you can tell if someone edited it since you last saw it."
                                },
                                {
                                    name: "Conflict Resolution Strategies",
                                    explanation: "When conflicts occur, you have options: Client wins (overwrite database), Database wins (discard changes), Merge (combine both changes), or User decides (show conflict and let them choose). The right strategy depends on your business rules. Financial data might require user resolution, while non-critical data might use last-write-wins. It's like deciding who gets the last piece of cake - sometimes first come first served, sometimes you split it, sometimes you ask who wants it more."
                                }
                            ]
                        }
                    ]
                },
                {
                    id: 4,
                    name: "React & TypeScript Frontend",
                    questions: [
                        {
                            id: 41,
                            question: "What are React Hooks and explain the most commonly used ones (useState, useEffect, useContext)?",
                            suggestedAnswer: "React Hooks are functions that let you use state and other React features in functional components without writing classes. useState manages component state, returning current state and a setter function. useEffect handles side effects like data fetching, subscriptions, or DOM manipulation, running after render and optionally cleaning up. useContext accesses context values without prop drilling. Hooks must be called at the top level, not inside loops or conditions. They enable code reuse through custom hooks. Other common hooks include useReducer for complex state logic, useRef for mutable references, and useMemo/useCallback for performance. At BIPO, we use hooks extensively in our React Native mobile app and web dashboard. useState manages form inputs and UI state, useEffect fetches employee data from APIs, and useContext provides authentication and theme data throughout the app without passing props through every component.",
                            bulletPoints: [
                                "useState: manages component state",
                                "useEffect: handles side effects and lifecycle",
                                "useContext: accesses context without prop drilling",
                                "Must be called at top level (not in loops/conditions)",
                                "Enable code reuse through custom hooks",
                                "Other hooks: useReducer, useRef, useMemo, useCallback",
                                "Replaced class components for most use cases"
                            ],
                            keyConcepts: [
                                {
                                    name: "State Management",
                                    explanation: "State is data that changes over time in your component. useState gives you a state variable and a function to update it. When state changes, React re-renders the component. Unlike regular variables, state persists between renders. It's like having a notepad that remembers information even after you look away and look back."
                                },
                                {
                                    name: "Side Effects",
                                    explanation: "Side effects are operations that affect things outside the component's render function - API calls, timers, subscriptions, DOM manipulation. useEffect runs these after render. You can return a cleanup function to undo effects (like canceling subscriptions). The dependency array controls when effects run. It's like cleaning up after yourself - you set something up, and when you're done, you clean it up."
                                },
                                {
                                    name: "Prop Drilling",
                                    explanation: "Prop drilling is passing props through many component layers to reach a deeply nested component. It's tedious and makes code harder to maintain. useContext solves this by letting any component access context values directly, without passing through intermediaries. It's like having a company-wide announcement system versus passing messages person-to-person."
                                }
                            ]
                        },
                        {
                            id: 42,
                            question: "Explain the difference between useMemo and useCallback hooks.",
                            suggestedAnswer: "useMemo and useCallback are optimization hooks that prevent unnecessary recalculations and re-renders. useMemo memoizes a computed value, recalculating only when dependencies change. Use it for expensive calculations. useCallback memoizes a function, returning the same function instance unless dependencies change. Use it when passing callbacks to optimized child components that rely on reference equality. Both take a function and dependency array. The key difference: useMemo returns the result of calling the function, useCallback returns the function itself. Don't overuse them - they add overhead. Only use when profiling shows performance issues. At BIPO, we use useMemo for expensive payroll calculations that filter and aggregate large employee datasets, and useCallback for event handlers passed to memoized child components in our data tables to prevent unnecessary re-renders of table rows.",
                            bulletPoints: [
                                "useMemo: memoizes computed values",
                                "useCallback: memoizes functions",
                                "useMemo returns function result, useCallback returns function itself",
                                "Both take dependencies array",
                                "Use for: expensive calculations, preventing re-renders",
                                "Don't overuse - they add overhead",
                                "Only optimize when profiling shows issues"
                            ],
                            keyConcepts: [
                                {
                                    name: "Memoization",
                                    explanation: "Memoization is caching the result of expensive operations so you don't recalculate them every render. React remembers the previous result and dependencies. If dependencies haven't changed, it returns the cached result instead of recalculating. It's like remembering the answer to a math problem instead of solving it again every time someone asks."
                                },
                                {
                                    name: "Reference Equality",
                                    explanation: "In JavaScript, functions and objects are compared by reference, not value. A new function created each render is different from the previous one, even if the code is identical. This causes child components using React.memo to re-render unnecessarily. useCallback prevents this by returning the same function reference. It's like the difference between 'is this the same person?' versus 'do these people look alike?'"
                                },
                                {
                                    name: "Premature Optimization",
                                    explanation: "Adding useMemo/useCallback everywhere actually hurts performance because memoization has overhead. Only use them when you've profiled and found actual performance problems. Most components render fast enough without optimization. It's like adding insurance to everything you own - the insurance itself costs money, so only insure valuable things."
                                }
                            ]
                        },
                        {
                            id: 43,
                            question: "How do you manage global state in React applications (Context API, Redux, Zustand)?",
                            suggestedAnswer: "Global state management solutions share data across components without prop drilling. Context API is built into React - create a context, provide values at the top level, and consume with useContext. It's simple but can cause unnecessary re-renders. Redux is a predictable state container with actions, reducers, and a single store. It has more boilerplate but excellent dev tools and middleware support. Zustand is a lightweight alternative with minimal boilerplate, using hooks and not requiring providers. Choose based on complexity: Context for simple cases, Redux for large apps with complex state logic and time-travel debugging, Zustand for medium apps wanting simplicity. At BIPO, we use Context API for authentication and theme state, and Zustand for more complex application state like employee filters and form data. For the React Native mobile app, Zustand's simplicity and small bundle size were perfect for our needs.",
                            bulletPoints: [
                                "Context API: built-in, simple, can cause re-renders",
                                "Redux: predictable, more boilerplate, great dev tools",
                                "Zustand: lightweight, minimal boilerplate, hooks-based",
                                "Context: good for simple global state",
                                "Redux: good for complex state with middleware",
                                "Zustand: good middle ground with simplicity",
                                "Choose based on app complexity and team preference"
                            ],
                            keyConcepts: [
                                {
                                    name: "State Container",
                                    explanation: "A state container is a centralized place to store application state that multiple components need. Instead of lifting state up through many levels, you put it in one place and components access it directly. It's like having a shared filing cabinet that everyone can access, versus passing folders from person to person."
                                },
                                {
                                    name: "Actions and Reducers",
                                    explanation: "In Redux, actions are objects describing what happened (e.g., 'USER_LOGGED_IN'), and reducers are pure functions that take current state and an action, returning new state. This pattern makes state changes predictable and testable. It's like having a ledger where you record every transaction (action) and calculate the new balance (reducer)."
                                },
                                {
                                    name: "Re-render Optimization",
                                    explanation: "Context API re-renders all consuming components when any context value changes. For large apps, this can hurt performance. Solutions include splitting contexts, using selectors, or choosing Redux/Zustand which have built-in optimization. It's like a fire alarm that evacuates the entire building versus one that only evacuates the affected floor."
                                }
                            ]
                        },
                        {
                            id: 44,
                            question: "What is TypeScript and what are its benefits over JavaScript?",
                            suggestedAnswer: "TypeScript is a typed superset of JavaScript that compiles to plain JavaScript. It adds static typing, interfaces, enums, and advanced features while maintaining JavaScript compatibility. Benefits include: catching errors at compile time instead of runtime, better IDE support with IntelliSense and autocomplete, self-documenting code through types, safer refactoring, and improved code quality in large teams. TypeScript prevents common bugs like accessing undefined properties or passing wrong argument types. The type system is optional and gradual - you can adopt it incrementally. At BIPO, we use TypeScript for all React and React Native development. It catches bugs during development, makes our codebase more maintainable across multiple developers, and provides excellent IDE support that speeds up development. The initial learning curve pays off quickly in reduced bugs and better developer experience.",
                            bulletPoints: [
                                "Typed superset of JavaScript, compiles to JS",
                                "Static typing catches errors at compile time",
                                "Better IDE support: IntelliSense, autocomplete, refactoring",
                                "Self-documenting code through type annotations",
                                "Prevents common bugs (undefined, wrong types)",
                                "Optional and gradual adoption",
                                "Improves code quality in large teams"
                            ],
                            keyConcepts: [
                                {
                                    name: "Static vs Dynamic Typing",
                                    explanation: "Static typing (TypeScript) checks types at compile time before code runs. Dynamic typing (JavaScript) checks at runtime. Static typing catches errors earlier and provides better tooling, but requires more upfront work. It's like spell-check while you type versus finding spelling errors when you print the document."
                                },
                                {
                                    name: "Type Inference",
                                    explanation: "TypeScript can often infer types without explicit annotations. If you write const name = 'John', TypeScript knows name is a string. You don't need to write const name: string = 'John'. This reduces verbosity while maintaining type safety. It's like a smart assistant who understands context without you explaining everything."
                                },
                                {
                                    name: "Gradual Adoption",
                                    explanation: "You can add TypeScript to existing JavaScript projects incrementally. Start with .ts files alongside .js files, gradually converting. Use 'any' type for complex migrations. This makes adoption practical for large codebases. It's like renovating a house one room at a time while still living there, versus moving out and rebuilding everything."
                                }
                            ]
                        },
                        {
                            id: 45,
                            question: "Explain TypeScript generics and provide practical examples.",
                            suggestedAnswer: "Generics are type variables that let you write reusable code that works with multiple types while maintaining type safety. They're like function parameters but for types. Common example: Array<T> where T is the element type. Use generics for functions, interfaces, and classes. Syntax: function identity<T>(arg: T): T { return arg; }. Practical examples: API response types Response<T>, useState<User>(), generic utility functions like filterArray<T>(items: T[], predicate: (item: T) => boolean). Generics enable type-safe collections, reusable components, and flexible APIs. At BIPO, we use generics extensively for API client functions that return different data types, generic table components that work with any data shape, and utility functions for data transformation that maintain type safety across different entity types.",
                            bulletPoints: [
                                "Type variables for reusable, type-safe code",
                                "Syntax: function name<T>(arg: T): T",
                                "Used in functions, interfaces, classes",
                                "Common: Array<T>, Promise<T>, Response<T>",
                                "Enable type-safe collections and utilities",
                                "Constraints: <T extends SomeType>",
                                "Multiple type parameters: <T, U>"
                            ],
                            keyConcepts: [
                                {
                                    name: "Type Parameters",
                                    explanation: "Type parameters (like <T>) are placeholders for actual types that will be provided when the generic is used. When you call identity<string>('hello'), T becomes string. TypeScript infers the type if possible, so identity('hello') also works. It's like a template where you fill in the blanks - the structure is fixed but the content varies."
                                },
                                {
                                    name: "Generic Constraints",
                                    explanation: "Constraints limit what types can be used with a generic using 'extends'. For example, <T extends { id: number }> means T must have an id property. This lets you access properties safely while maintaining flexibility. It's like saying 'this function works with any vehicle, but it must have wheels' - you're flexible but with requirements."
                                },
                                {
                                    name: "Type Safety with Flexibility",
                                    explanation: "Generics provide both type safety and code reuse. Without generics, you'd either use 'any' (losing type safety) or duplicate code for each type. Generics let you write once and use with many types while keeping full type checking. It's like having a universal adapter that works with different plugs but still ensures proper voltage."
                                }
                            ]
                        },
                        {
                            id: 46,
                            question: "How do you type React components, props, and hooks in TypeScript?",
                            suggestedAnswer: "Type React components using React.FC<Props> or function Component(props: Props): JSX.Element. Define props with interfaces or types. For hooks, TypeScript often infers types, but you can be explicit: useState<User | null>(null), useRef<HTMLDivElement>(null). Type events as React.ChangeEvent<HTMLInputElement> or React.MouseEvent. For children, use React.ReactNode. For generic components, use generics: function List<T>(props: { items: T[] }). Use Partial<T> for optional props, Pick<T, 'field'> for subsets. At BIPO, we define strict prop interfaces for all components, use discriminated unions for variant props, and leverage TypeScript's utility types. This catches prop errors at compile time and provides excellent autocomplete in our IDE, making development faster and preventing runtime errors from incorrect prop usage.",
                            bulletPoints: [
                                "Components: React.FC<Props> or (props: Props) => JSX.Element",
                                "Props: define with interface or type",
                                "Hooks: useState<Type>, useRef<HTMLElement>",
                                "Events: React.ChangeEvent, React.MouseEvent",
                                "Children: React.ReactNode",
                                "Generic components: function List<T>(props: { items: T[] })",
                                "Utility types: Partial, Pick, Omit"
                            ],
                            keyConcepts: [
                                {
                                    name: "Props Interface",
                                    explanation: "Defining props with interfaces provides type safety and documentation. interface ButtonProps { text: string; onClick: () => void; disabled?: boolean; } makes it clear what props are required and optional. IDEs show this information when you use the component. It's like having a contract that specifies exactly what the component needs."
                                },
                                {
                                    name: "Event Typing",
                                    explanation: "React events have specific types like React.ChangeEvent<HTMLInputElement> for input changes or React.MouseEvent<HTMLButtonElement> for clicks. These types provide access to event properties with full type safety. Using the correct event type prevents errors and enables autocomplete for event properties. It's like having specialized tools for different jobs instead of one generic tool."
                                },
                                {
                                    name: "Generic Components",
                                    explanation: "Generic components work with different data types while maintaining type safety. A generic Table<T> component can display any data type, and TypeScript ensures you're using it correctly. This enables highly reusable components without sacrificing type safety. It's like a universal container that adapts to what you put in it while still keeping track of what's inside."
                                }
                            ]
                        },
                        {
                            id: 47,
                            question: "What is the virtual DOM and how does React's reconciliation algorithm work?",
                            suggestedAnswer: "The virtual DOM is a lightweight JavaScript representation of the actual DOM. When state changes, React creates a new virtual DOM tree, compares it with the previous one (diffing), and calculates the minimum changes needed. Then it updates only those parts of the real DOM (reconciliation). This is faster than manipulating the DOM directly because DOM operations are expensive. React uses a heuristic O(n) algorithm assuming: elements of different types produce different trees, and you can hint at stable elements with keys. Keys help React identify which items changed, were added, or removed in lists. Without keys, React might unnecessarily recreate elements. At BIPO, understanding reconciliation helps us optimize list rendering in our employee tables by using stable keys (employee IDs) and avoiding inline object creation that breaks reference equality.",
                            bulletPoints: [
                                "Virtual DOM: JavaScript representation of real DOM",
                                "Diffing: compare old and new virtual DOM trees",
                                "Reconciliation: update only changed parts of real DOM",
                                "O(n) algorithm using heuristics",
                                "Keys help identify list items across renders",
                                "Avoids expensive direct DOM manipulation",
                                "Different element types cause full subtree recreation"
                            ],
                            keyConcepts: [
                                {
                                    name: "DOM Manipulation Cost",
                                    explanation: "Manipulating the real DOM is slow because it triggers reflows and repaints. The browser must recalculate layouts and redraw pixels. React minimizes this by batching updates and only touching the DOM when necessary. The virtual DOM acts as a buffer that lets React optimize these operations. It's like planning all your errands before leaving the house versus making a separate trip for each errand."
                                },
                                {
                                    name: "Diffing Algorithm",
                                    explanation: "React's diffing algorithm compares trees level by level. If elements have different types, React destroys the old tree and builds a new one. If they're the same type, React updates only changed attributes. This heuristic approach is much faster than a perfect algorithm. It's like comparing two versions of a document by looking at each paragraph, not each character."
                                },
                                {
                                    name: "Keys in Lists",
                                    explanation: "Keys tell React which list items are the same across renders. Without keys, React uses index position, which breaks when items reorder. Good keys are stable, unique IDs. Bad keys are array indices or random values. Proper keys prevent unnecessary re-renders and maintain component state. It's like having name tags at a party - you can recognize people even if they move around."
                                }
                            ]
                        },
                        {
                            id: 48,
                            question: "Explain React component lifecycle and how it relates to hooks.",
                            suggestedAnswer: "Class components have lifecycle methods: componentDidMount (after first render), componentDidUpdate (after updates), componentWillUnmount (before removal). Hooks replace these: useEffect with empty deps array runs once like componentDidMount. useEffect with deps runs on mount and when deps change like componentDidUpdate. useEffect cleanup function runs like componentWillUnmount. useLayoutEffect runs synchronously after DOM mutations, like componentDidMount but before paint. Hooks provide more flexibility - you can have multiple useEffects for different concerns, unlike lifecycle methods that handle everything. At BIPO, we use useEffect for data fetching on mount, subscriptions with cleanup, and responding to prop changes. The hooks model is more intuitive than lifecycle methods because effects are organized by concern rather than lifecycle phase.",
                            bulletPoints: [
                                "Class lifecycle: componentDidMount, componentDidUpdate, componentWillUnmount",
                                "useEffect(fn, []) = componentDidMount",
                                "useEffect(fn, [deps]) = componentDidUpdate",
                                "useEffect cleanup = componentWillUnmount",
                                "useLayoutEffect = synchronous, before paint",
                                "Multiple useEffects for different concerns",
                                "Hooks organize by concern, not lifecycle phase"
                            ],
                            keyConcepts: [
                                {
                                    name: "Effect Timing",
                                    explanation: "useEffect runs after render and paint - the browser has already updated the screen. useLayoutEffect runs after render but before paint - synchronously. Use useEffect for most cases (data fetching, subscriptions). Use useLayoutEffect when you need to measure or mutate DOM before the user sees it. It's like the difference between sending a letter after the meeting (useEffect) versus handing it to someone before they leave (useLayoutEffect)."
                                },
                                {
                                    name: "Dependency Array",
                                    explanation: "The dependency array controls when effects run. Empty array [] runs once on mount. Array with values [count, user] runs when those values change. No array runs after every render (usually not what you want). Missing dependencies can cause bugs - use ESLint rules to catch them. It's like setting reminders - you specify what triggers the reminder."
                                },
                                {
                                    name: "Cleanup Functions",
                                    explanation: "Returning a function from useEffect creates a cleanup that runs before the effect runs again and when the component unmounts. Use it to cancel subscriptions, clear timers, or abort fetch requests. This prevents memory leaks and race conditions. It's like turning off the lights when you leave a room - you clean up after yourself."
                                }
                            ]
                        },
                        {
                            id: 49,
                            question: "How do you optimize React application performance (memoization, code splitting, lazy loading)?",
                            suggestedAnswer: "Performance optimization techniques: Use React.memo to prevent re-renders of components when props haven't changed. Use useMemo for expensive calculations and useCallback for stable function references. Implement code splitting with React.lazy() and Suspense to load components on demand, reducing initial bundle size. Use dynamic imports for routes and heavy features. Virtualize long lists with react-window or react-virtualized to render only visible items. Optimize images with lazy loading and proper formats. Avoid inline object/array creation in render. Use production builds. Profile with React DevTools Profiler to find bottlenecks. At BIPO, we code-split by route, virtualize employee lists with thousands of rows, memoize expensive payroll calculations, and lazy load heavy chart libraries. These optimizations reduced our initial load time by 60% and improved list scrolling performance significantly.",
                            bulletPoints: [
                                "React.memo: prevent re-renders when props unchanged",
                                "useMemo/useCallback: cache values and functions",
                                "Code splitting: React.lazy() and Suspense",
                                "Virtualization: render only visible list items",
                                "Lazy load images and heavy components",
                                "Avoid inline object/array creation",
                                "Profile with React DevTools to find bottlenecks"
                            ],
                            keyConcepts: [
                                {
                                    name: "Code Splitting",
                                    explanation: "Code splitting breaks your app into smaller chunks that load on demand. Instead of one huge bundle, users download only what they need initially. Use React.lazy() for component-level splitting and dynamic import() for any code. This dramatically reduces initial load time. It's like downloading a book chapter by chapter as you read, versus downloading the entire book upfront."
                                },
                                {
                                    name: "Virtualization",
                                    explanation: "Virtualization renders only visible items in long lists, not all thousands of items. As you scroll, items are recycled. Libraries like react-window handle this. Without virtualization, rendering 10,000 rows creates 10,000 DOM nodes - slow. With virtualization, you render maybe 20 visible rows. It's like a theater with moving scenery - you only see what's on stage, not everything backstage."
                                },
                                {
                                    name: "Performance Profiling",
                                    explanation: "Don't optimize blindly - profile first to find actual bottlenecks. React DevTools Profiler shows which components render, how long they take, and why they rendered. Optimize the slow parts, not everything. Premature optimization wastes time and adds complexity. It's like fixing the slowest part of your commute first, not optimizing every step."
                                }
                            ]
                        },
                        {
                            id: 50,
                            question: "What are controlled vs uncontrolled components in React?",
                            suggestedAnswer: "Controlled components have their value controlled by React state. The input's value prop is tied to state, and onChange updates state. React is the single source of truth. Uncontrolled components store their own state internally in the DOM. You access values with refs. Controlled components give you full control - validation, formatting, conditional logic. Uncontrolled components are simpler for basic forms but harder to validate or manipulate. Most React forms use controlled components. For file inputs (can't be controlled) or integrating with non-React code, use uncontrolled. At BIPO, we use controlled components for all forms, enabling real-time validation, conditional fields, and complex business logic. For example, our employee form validates as you type, shows/hides fields based on employment type, and formats inputs automatically - all possible because we control the values.",
                            bulletPoints: [
                                "Controlled: value from React state, onChange updates state",
                                "Uncontrolled: value stored in DOM, accessed via refs",
                                "Controlled: React is single source of truth",
                                "Controlled: enables validation, formatting, conditional logic",
                                "Uncontrolled: simpler for basic forms",
                                "Most React forms use controlled components",
                                "File inputs must be uncontrolled"
                            ],
                            keyConcepts: [
                                {
                                    name: "Single Source of Truth",
                                    explanation: "In controlled components, React state is the single source of truth for input values. The DOM reflects state, not the other way around. This makes the data flow predictable and enables features like validation and formatting. It's like having one master copy of a document that everyone references, versus everyone having their own copy that might differ."
                                },
                                {
                                    name: "Refs for Uncontrolled",
                                    explanation: "Uncontrolled components use refs to access DOM values when needed (like on submit). useRef creates a reference to the DOM element. This is simpler than controlled components but gives less control during user interaction. It's like checking your mailbox only when you need something versus monitoring every letter as it arrives."
                                },
                                {
                                    name: "When to Use Each",
                                    explanation: "Use controlled for most forms - you get validation, dynamic behavior, and predictable state. Use uncontrolled for simple forms where you just need the value on submit, file inputs (which must be uncontrolled), or integrating with non-React libraries. The extra code for controlled components is worth it for the control you gain."
                                }
                            ]
                        },
                        {
                            id: 51,
                            question: "How do you handle forms in React (React Hook Form, Formik)?",
                            suggestedAnswer: "Form libraries simplify form management. React Hook Form is lightweight, uses uncontrolled components with refs for performance, and has minimal re-renders. Formik uses controlled components, has more features but more re-renders. Both handle validation (with Yup or Zod schemas), error messages, submission, and field arrays. React Hook Form: useForm() hook, register() for fields, handleSubmit() for submission. Formik: Formik component or useFormik() hook. For simple forms, native React with controlled components works fine. For complex forms with validation, dynamic fields, or field arrays, use a library. At BIPO, we use React Hook Form for its performance and TypeScript support. Our employee onboarding form has 50+ fields with complex validation rules, conditional fields, and file uploads. React Hook Form handles this efficiently with minimal code and excellent performance.",
                            bulletPoints: [
                                "React Hook Form: lightweight, uncontrolled, minimal re-renders",
                                "Formik: controlled components, more features",
                                "Both support validation schemas (Yup, Zod)",
                                "Handle: validation, errors, submission, field arrays",
                                "React Hook Form: register(), handleSubmit()",
                                "Formik: Formik component or useFormik() hook",
                                "Use libraries for complex forms, native React for simple ones"
                            ],
                            keyConcepts: [
                                {
                                    name: "Form Validation",
                                    explanation: "Validation ensures user input meets requirements. Client-side validation provides immediate feedback. Schema validation libraries like Yup or Zod define rules declaratively. Form libraries integrate these schemas, showing errors automatically. Always validate server-side too - client validation is for UX, not security. It's like spell-check while typing (client) plus an editor reviewing your final draft (server)."
                                },
                                {
                                    name: "Re-render Performance",
                                    explanation: "Controlled components re-render on every keystroke because state updates. For large forms, this can be slow. React Hook Form minimizes re-renders by using refs and only re-rendering when necessary. Formik re-renders more but is still fast enough for most forms. Profile your forms if performance is an issue. It's like the difference between recalculating everything versus only recalculating what changed."
                                },
                                {
                                    name: "Field Arrays",
                                    explanation: "Field arrays handle dynamic lists of fields (like adding multiple phone numbers). Form libraries provide utilities for adding, removing, and reordering items while maintaining validation and state. Implementing this manually is complex. It's like having a template for repeating sections of a form that you can duplicate and remove as needed."
                                }
                            ]
                        },
                        {
                            id: 52,
                            question: "Explain error boundaries in React and how to implement them.",
                            suggestedAnswer: "Error boundaries are React components that catch JavaScript errors in their child component tree, log errors, and display fallback UI instead of crashing the whole app. They catch errors during rendering, in lifecycle methods, and in constructors, but not in event handlers, async code, or the error boundary itself. Implement by creating a class component with componentDidCatch() or static getDerivedStateFromError(). There's no hook equivalent yet. Wrap parts of your app with error boundaries to isolate failures. At BIPO, we wrap each major feature section in error boundaries. If the employee list crashes, the navigation and other sections still work. We log errors to our monitoring service and show user-friendly error messages with retry options. This prevents one component's bug from breaking the entire application.",
                            bulletPoints: [
                                "Catch errors in child component tree",
                                "Display fallback UI instead of crashing",
                                "Catch: rendering errors, lifecycle, constructors",
                                "Don't catch: event handlers, async code, error boundary itself",
                                "Implement with componentDidCatch() or getDerivedStateFromError()",
                                "No hook equivalent yet (must use class component)",
                                "Wrap app sections to isolate failures"
                            ],
                            keyConcepts: [
                                {
                                    name: "Graceful Degradation",
                                    explanation: "Error boundaries enable graceful degradation - when something breaks, the app doesn't completely fail. Users see an error message for the broken part but can still use the rest of the app. This is much better than a blank screen or crashed app. It's like a building with fire doors - a fire in one room doesn't burn down the whole building."
                                },
                                {
                                    name: "Error Boundary Limitations",
                                    explanation: "Error boundaries don't catch errors in event handlers (use try-catch), async code (use .catch()), or during server-side rendering. They only catch errors during React's render phase. For event handlers and async code, handle errors explicitly. It's like having a safety net that catches you when you fall off the tightrope, but not when you trip on the platform."
                                },
                                {
                                    name: "Error Logging",
                                    explanation: "Error boundaries are perfect for logging errors to monitoring services like Sentry or LogRocket. componentDidCatch receives the error and component stack, which you can send to your logging service. This helps you discover and fix bugs in production. It's like having security cameras that record incidents so you can review and prevent them."
                                }
                            ]
                        },
                        {
                            id: 53,
                            question: "How do you fetch data in React (useEffect, React Query, SWR)?",
                            suggestedAnswer: "Basic data fetching uses useEffect with fetch or axios, storing data in state. Handle loading and error states manually. React Query and SWR are data fetching libraries that handle caching, refetching, loading states, and errors automatically. React Query: useQuery() hook with query key and fetch function. SWR: useSWR() with key and fetcher. Both provide: automatic caching, background refetching, optimistic updates, pagination, and infinite scroll. Use native useEffect for simple one-time fetches. Use React Query/SWR for complex data fetching with caching needs. At BIPO, we use React Query for all API calls. It caches employee data, automatically refetches when data becomes stale, handles loading and error states, and provides excellent DevTools. This eliminated hundreds of lines of manual caching and state management code.",
                            bulletPoints: [
                                "Basic: useEffect with fetch/axios, manual state management",
                                "React Query: useQuery() with caching and refetching",
                                "SWR: useSWR() with stale-while-revalidate strategy",
                                "Libraries handle: caching, loading, errors, refetching",
                                "Features: pagination, infinite scroll, optimistic updates",
                                "Use useEffect for simple cases, libraries for complex data fetching",
                                "React Query/SWR eliminate manual cache management"
                            ],
                            keyConcepts: [
                                {
                                    name: "Stale-While-Revalidate",
                                    explanation: "SWR's strategy: show cached (stale) data immediately while fetching fresh data in the background. When fresh data arrives, update the UI. This makes apps feel instant while ensuring data freshness. React Query uses similar strategies. It's like reading yesterday's newspaper while today's is being delivered - you get information immediately and updates when available."
                                },
                                {
                                    name: "Query Keys",
                                    explanation: "Query keys identify queries for caching. useQuery(['users', userId], fetchUser) caches by user ID. Different IDs create different cache entries. Keys can be arrays with multiple values for complex queries. This enables automatic cache invalidation and refetching. It's like having a filing system where each folder has a unique label so you can find and update specific items."
                                },
                                {
                                    name: "Automatic Refetching",
                                    explanation: "React Query and SWR automatically refetch data in certain situations: when the window regains focus, when the network reconnects, or at configured intervals. This keeps data fresh without manual intervention. You can configure this behavior per query. It's like having a news app that automatically refreshes when you open it, ensuring you see the latest news."
                                }
                            ]
                        },
                        {
                            id: 54,
                            question: "What is React.memo and when should you use it?",
                            suggestedAnswer: "React.memo is a higher-order component that memoizes a component, preventing re-renders when props haven't changed. It does a shallow comparison of props. Use it for components that render often with the same props, expensive components, or components receiving new object/array references that are actually equal. Don't use it everywhere - it adds overhead. Only use when profiling shows unnecessary re-renders. For custom comparison, pass a comparison function as second argument. React.memo works with functional components; for class components, use PureComponent. At BIPO, we use React.memo on table row components that render hundreds of times. Without memo, editing one row re-rendered all rows. With memo, only the changed row re-renders, dramatically improving performance in our employee lists.",
                            bulletPoints: [
                                "HOC that prevents re-renders when props unchanged",
                                "Shallow comparison of props by default",
                                "Use for: frequently rendered components, expensive renders",
                                "Don't overuse - adds overhead",
                                "Custom comparison: React.memo(Component, arePropsEqual)",
                                "Functional component equivalent of PureComponent",
                                "Profile first to confirm unnecessary re-renders"
                            ],
                            keyConcepts: [
                                {
                                    name: "Shallow Comparison",
                                    explanation: "React.memo compares props using shallow equality - it checks if primitive values are equal and if objects/arrays have the same reference. It doesn't deep-compare object contents. If you pass new object instances with the same data, React.memo won't prevent re-renders. Use useMemo/useCallback to maintain stable references. It's like checking if two boxes are the same box versus checking if they contain the same items."
                                },
                                {
                                    name: "Memoization Overhead",
                                    explanation: "React.memo adds overhead - it must compare props on every render. For fast components, this overhead might be more expensive than just re-rendering. Only use memo when re-rendering is actually expensive or happens frequently. Profile to confirm. It's like using a calculator for simple math - sometimes doing it in your head is faster than getting the calculator out."
                                },
                                {
                                    name: "When Memo Helps",
                                    explanation: "React.memo helps most with: components that render many times (like list items), components with expensive render logic, or components that receive new object references but equivalent data. It's less helpful for components that render rarely or have simple render logic. The key is measuring actual performance impact."
                                }
                            ]
                        },
                        {
                            id: 55,
                            question: "How do you implement routing in React applications (React Router)?",
                            suggestedAnswer: "React Router is the standard routing library for React. Install react-router-dom, wrap your app in BrowserRouter, and define routes with Routes and Route components. Use Link or NavLink for navigation (don't use <a> tags - they cause full page reloads). Access route params with useParams(), navigate programmatically with useNavigate(), and access location with useLocation(). Implement nested routes, protected routes with authentication checks, and lazy loading with React.lazy(). Use Outlet for nested route rendering. For layouts, wrap routes in layout components. At BIPO, we use React Router for our web dashboard with nested routes for different sections, protected routes that redirect to login if unauthenticated, and lazy-loaded routes to reduce initial bundle size. We also use route-based code splitting to load each feature on demand.",
                            bulletPoints: [
                                "Install react-router-dom, wrap in BrowserRouter",
                                "Define routes with Routes and Route components",
                                "Navigate with Link/NavLink, not <a> tags",
                                "Hooks: useParams(), useNavigate(), useLocation()",
                                "Nested routes with Outlet component",
                                "Protected routes with authentication checks",
                                "Lazy load routes with React.lazy()"
                            ],
                            keyConcepts: [
                                {
                                    name: "Client-Side Routing",
                                    explanation: "React Router provides client-side routing - navigation happens without full page reloads. The URL changes, but JavaScript handles rendering the new view. This makes apps feel faster and enables smooth transitions. The browser's back/forward buttons still work. It's like flipping pages in a book versus closing one book and opening another."
                                },
                                {
                                    name: "Route Parameters",
                                    explanation: "Route parameters are dynamic segments in URLs like /users/:userId. The :userId part is a parameter you can access with useParams(). This enables dynamic routing where one route component handles many similar pages. It's like having a template for user profiles where the user ID determines which profile to show."
                                },
                                {
                                    name: "Protected Routes",
                                    explanation: "Protected routes require authentication or authorization. Implement by wrapping routes in a component that checks auth status and redirects to login if unauthorized. This prevents users from accessing pages they shouldn't see. It's like having a bouncer at a VIP section who checks if you're on the list before letting you in."
                                }
                            ]
                        }
                    ]
                },
                {
                    id: 5,
                    name: "Styling & UI Libraries",
                    questions: [
                        {
                            id: 56,
                            question: "What is TailwindCSS and what are its advantages over traditional CSS?",
                            suggestedAnswer: "TailwindCSS is a utility-first CSS framework that provides low-level utility classes to build custom designs directly in your HTML. Instead of writing custom CSS, you compose designs using pre-built classes like flex, pt-4, text-center. Advantages include: rapid development without context switching between HTML and CSS, consistent design system through predefined spacing and colors, smaller production CSS with PurgeCSS removing unused styles, no naming conflicts or specificity issues, responsive design with built-in breakpoint prefixes, and easy customization through configuration. It's more maintainable than traditional CSS because changes are localized to components. At BIPO, we use TailwindCSS for our React dashboard and it dramatically sped up UI development. We can prototype designs quickly, maintain consistency across the app, and our production CSS is only 15KB after purging unused styles.",
                            bulletPoints: [
                                "Utility-first CSS framework with pre-built classes",
                                "Rapid development without context switching",
                                "Consistent design system (spacing, colors, typography)",
                                "Smaller production CSS with PurgeCSS",
                                "No naming conflicts or specificity issues",
                                "Responsive design with breakpoint prefixes (sm:, md:, lg:)",
                                "Easy customization via tailwind.config.js"
                            ],
                            keyConcepts: [
                                {
                                    name: "Utility-First Approach",
                                    explanation: "Instead of writing semantic CSS classes like .card or .button, you use utility classes that do one thing like .flex or .bg-blue-500. You compose these utilities to build components. This seems verbose at first but becomes faster once you learn the utilities. It's like using LEGO blocks - each piece is simple, but you can build anything by combining them."
                                },
                                {
                                    name: "PurgeCSS",
                                    explanation: "TailwindCSS generates thousands of utility classes, but PurgeCSS removes unused ones in production by scanning your code. Your final CSS only includes classes you actually use. This keeps bundle sizes tiny despite Tailwind's comprehensive utility set. It's like having a huge toolbox but only packing the tools you need for a specific job."
                                },
                                {
                                    name: "Design Tokens",
                                    explanation: "Tailwind uses design tokens - predefined values for spacing (4, 8, 16px), colors (blue-500, gray-200), and typography. This enforces consistency - everyone uses the same spacing scale instead of arbitrary values. You can customize these tokens in the config file. It's like having a style guide built into your CSS framework."
                                }
                            ]
                        },
                        {
                            id: 57,
                            question: "How do you configure and customize TailwindCSS in a React project?",
                            suggestedAnswer: "Install TailwindCSS with npm install tailwindcss postcss autoprefixer, then run npx tailwindcss init to create tailwind.config.js. Configure content paths to include your React files for PurgeCSS. Customize theme in the config: extend colors, spacing, fonts, breakpoints. Add custom utilities with plugins. Import Tailwind directives in your CSS: @tailwind base, @tailwind components, @tailwind utilities. For React, you can use @apply in CSS to extract repeated utility patterns into custom classes. Use JIT mode for faster builds and arbitrary values. Install Tailwind IntelliSense VSCode extension for autocomplete. At BIPO, we customized Tailwind with our brand colors, added custom spacing for our 8-point grid system, and created custom utilities for common patterns like card shadows and button variants. We also use the forms plugin for better form styling.",
                            bulletPoints: [
                                "Install: npm install tailwindcss postcss autoprefixer",
                                "Initialize: npx tailwindcss init",
                                "Configure content paths for PurgeCSS",
                                "Customize theme: colors, spacing, fonts, breakpoints",
                                "Import directives: @tailwind base/components/utilities",
                                "Use @apply to extract repeated patterns",
                                "Enable JIT mode for faster builds"
                            ],
                            keyConcepts: [
                                {
                                    name: "Theme Extension",
                                    explanation: "In tailwind.config.js, you can extend the default theme with your custom values. For example, adding brand colors or custom spacing. Use 'extend' to add to defaults, or replace the entire theme section to override. This lets you maintain Tailwind's utilities while adding your design system. It's like adding custom tools to a standard toolbox without removing the originals."
                                },
                                {
                                    name: "JIT Mode",
                                    explanation: "Just-In-Time mode generates styles on-demand as you write them, rather than generating all possible combinations upfront. This makes development faster, enables arbitrary values like w-[137px], and reduces the need for PurgeCSS in development. It's like having a chef who cooks dishes to order versus a buffet with everything pre-made."
                                },
                                {
                                    name: "Apply Directive",
                                    explanation: "@apply lets you extract repeated utility combinations into custom CSS classes. For example, .btn { @apply px-4 py-2 bg-blue-500 rounded; }. Use this sparingly for truly reusable patterns. Overusing @apply defeats Tailwind's purpose. It's like creating a macro for commonly typed phrases - useful but don't overdo it."
                                }
                            ]
                        },
                        {
                            id: 58,
                            question: "Explain the concept of utility-first CSS and its benefits.",
                            suggestedAnswer: "Utility-first CSS means using small, single-purpose classes that do one thing well, rather than semantic classes that style entire components. Each utility class applies one CSS property, like text-center for text-align: center or mt-4 for margin-top. Benefits include: faster development by composing styles in markup, no need to name things or worry about specificity, easier to maintain because styles are co-located with markup, consistent design through predefined utilities, and smaller CSS bundles. The approach shifts styling from separate CSS files to the component itself, making it easier to understand what styles apply. Critics argue it's verbose and mixes concerns, but proponents find it more maintainable for component-based architectures. At BIPO, utility-first CSS eliminated the need for most custom CSS files, reduced our stylesheet size by 70%, and made it easier for developers to style components without CSS expertise.",
                            bulletPoints: [
                                "Single-purpose classes that do one thing",
                                "Compose styles in markup, not separate CSS files",
                                "Faster development without naming or specificity issues",
                                "Styles co-located with components for easier maintenance",
                                "Consistent design through predefined utilities",
                                "Smaller CSS bundles with proper purging",
                                "Better for component-based architectures"
                            ],
                            keyConcepts: [
                                {
                                    name: "Separation of Concerns",
                                    explanation: "Traditional CSS separates structure (HTML) from presentation (CSS). Utility-first CSS challenges this by putting styles in markup. The argument is that in component-based development, the component is the concern - HTML, CSS, and JS together. Separating them across files is actually less maintainable. It's like keeping a recipe's ingredients list in one book and instructions in another versus having them together."
                                },
                                {
                                    name: "Semantic vs Functional Classes",
                                    explanation: "Semantic classes describe what something is (.user-card, .primary-button). Functional/utility classes describe what they do (.flex, .bg-blue-500). Semantic classes seem cleaner but require constant naming decisions and can become outdated. Utility classes are more flexible and composable. It's like the difference between pre-made furniture versus modular pieces you can rearrange."
                                },
                                {
                                    name: "CSS Bloat",
                                    explanation: "Traditional CSS grows over time as developers add new styles, rarely removing old ones. Utility-first CSS with proper purging only includes used styles, keeping bundles small. You're not afraid to change styles because you're not creating new CSS, just using different combinations of existing utilities. It's like renting furniture versus buying - you only pay for what you're currently using."
                                }
                            ]
                        },
                        {
                            id: 59,
                            question: "What is NextUI and how does it compare to other component libraries like Material-UI or Ant Design?",
                            suggestedAnswer: "NextUI is a modern React UI library built on top of TailwindCSS with beautiful, accessible components out of the box. It provides pre-built components like buttons, modals, tables with excellent default styling and animations. Compared to Material-UI, NextUI is lighter weight, uses Tailwind utilities, and has a more modern aesthetic. Material-UI follows Google's Material Design with more components and maturity. Ant Design is enterprise-focused with comprehensive components but heavier bundle size. NextUI advantages: smaller bundle, better performance, Tailwind integration, modern design. Material-UI advantages: more components, better documentation, larger community. Ant Design advantages: enterprise features, comprehensive form handling. Choose based on project needs: NextUI for modern, lightweight apps; Material-UI for Material Design adherence; Ant Design for enterprise applications. At BIPO, we considered NextUI for its Tailwind integration but chose a custom component library built on Tailwind for maximum flexibility and brand consistency.",
                            bulletPoints: [
                                "NextUI: modern, lightweight, built on TailwindCSS",
                                "Material-UI: comprehensive, Material Design, mature ecosystem",
                                "Ant Design: enterprise-focused, heavy but feature-rich",
                                "NextUI: smaller bundle, better performance, modern aesthetic",
                                "Material-UI: more components, better docs, larger community",
                                "Ant Design: comprehensive forms, enterprise features",
                                "Choose based on project requirements and design goals"
                            ],
                            keyConcepts: [
                                {
                                    name: "Component Library Trade-offs",
                                    explanation: "Component libraries save development time but add bundle size and lock you into their design system. Lightweight libraries like NextUI are faster but have fewer components. Comprehensive libraries like Ant Design have everything but are heavier. Consider your needs: rapid prototyping favors comprehensive libraries, performance-critical apps favor lightweight ones. It's like choosing between a Swiss Army knife and specialized tools."
                                },
                                {
                                    name: "Design System Integration",
                                    explanation: "NextUI's Tailwind integration means you can use Tailwind utilities alongside NextUI components, maintaining consistency. Material-UI and Ant Design have their own styling systems, making it harder to mix with custom styles. If you're already using Tailwind, NextUI is more natural. If you want an opinionated design system, Material-UI or Ant Design provide that."
                                },
                                {
                                    name: "Accessibility Built-in",
                                    explanation: "Good component libraries handle accessibility (ARIA attributes, keyboard navigation, focus management) automatically. NextUI, Material-UI, and Ant Design all prioritize accessibility. This is crucial because implementing accessible components from scratch is complex. The library handles things like modal focus trapping and keyboard navigation so you don't have to."
                                }
                            ]
                        },
                        {
                            id: 60,
                            question: "How do you create reusable and composable React components?",
                            suggestedAnswer: "Create reusable components by following these principles: single responsibility (one component, one purpose), accept props for customization, use children prop for composition, provide sensible defaults, use TypeScript for prop types, and extract common patterns. Composition over inheritance - build complex components by combining simpler ones. Use render props or custom hooks for behavior reuse. Make components flexible with variant props but not overly generic. Document props and provide examples. Use compound components pattern for related components that work together. At BIPO, we have a component library with base components like Button, Input, Card that accept variant props. We compose these into domain components like EmployeeCard or PayrollForm. We use Storybook to document components with examples, making it easy for team members to discover and reuse components across our applications.",
                            bulletPoints: [
                                "Single responsibility: one component, one purpose",
                                "Accept props for customization, provide defaults",
                                "Use children prop for composition",
                                "TypeScript for prop validation and documentation",
                                "Composition over inheritance",
                                "Extract common patterns into shared components",
                                "Use Storybook or similar for component documentation"
                            ],
                            keyConcepts: [
                                {
                                    name: "Composition Pattern",
                                    explanation: "Composition means building complex components from simpler ones, like LEGO blocks. Instead of one monolithic component with many props, you have small components that work together. For example, Card, CardHeader, CardBody, CardFooter that compose into a complete card. This is more flexible than a single Card component with header/body/footer props. It's like modular furniture you can arrange versus a fixed piece."
                                },
                                {
                                    name: "Children Prop",
                                    explanation: "The children prop lets components wrap other content, enabling composition. A Button component that accepts children can wrap text, icons, or other components. This makes components flexible without adding specific props for every use case. It's like a container that can hold anything versus a box designed for one specific item."
                                },
                                {
                                    name: "Compound Components",
                                    explanation: "Compound components are a group of components that work together to form a complete UI pattern. Like Select, SelectTrigger, SelectContent, SelectItem that share state implicitly. This provides flexibility while maintaining a clean API. Users compose the components how they need while the library handles the complex state management. It's like a modular kitchen where appliances are designed to work together."
                                }
                            ]
                        },
                        {
                            id: 61,
                            question: "What are CSS modules and how do they prevent style conflicts?",
                            suggestedAnswer: "CSS Modules are CSS files where class names are locally scoped by default. When you import a CSS module, class names are transformed to unique identifiers, preventing global namespace pollution. Import styles as an object and reference classes like styles.button. This prevents style conflicts between components - each component's styles are isolated. CSS Modules work with standard CSS, support composition with composes keyword, and integrate with build tools like Webpack. They're simpler than CSS-in-JS but more maintainable than global CSS. Use CSS Modules when you want traditional CSS workflow with scoping benefits. At Liven Group, we used CSS Modules before migrating to TailwindCSS. They prevented style conflicts in our large codebase where multiple developers worked on different features. Each component had its own .module.css file, ensuring styles didn't leak or conflict.",
                            bulletPoints: [
                                "CSS files with locally scoped class names",
                                "Class names transformed to unique identifiers",
                                "Import as object: import styles from './Button.module.css'",
                                "Prevents global namespace pollution and conflicts",
                                "Works with standard CSS syntax",
                                "Supports composition with 'composes' keyword",
                                "Simpler than CSS-in-JS, safer than global CSS"
                            ],
                            keyConcepts: [
                                {
                                    name: "Local Scope",
                                    explanation: "CSS Modules make class names local to the component by default. A .button class in one component won't conflict with .button in another. The build tool transforms them to unique names like .Button_button__2x3h. This solves the global namespace problem in CSS where everything can potentially conflict. It's like having private variables in programming versus global variables."
                                },
                                {
                                    name: "Composition",
                                    explanation: "CSS Modules support composition where one class can inherit styles from another using the 'composes' keyword. For example, .primaryButton { composes: button; background: blue; }. This enables style reuse without preprocessors. It's like object inheritance in programming - you extend base styles with specific variations."
                                },
                                {
                                    name: "Build Tool Integration",
                                    explanation: "CSS Modules require build tool support (Webpack, Vite, etc.) to transform class names. The build tool generates unique identifiers and creates a mapping between original and transformed names. This happens automatically in Create React App and Next.js. It's like having a compiler that translates your code to a safe format."
                                }
                            ]
                        },
                        {
                            id: 62,
                            question: "How do you implement responsive design in React with TailwindCSS?",
                            suggestedAnswer: "TailwindCSS makes responsive design easy with mobile-first breakpoint prefixes. Apply utilities at different breakpoints using sm:, md:, lg:, xl:, 2xl: prefixes. Base styles apply to mobile, then override at larger screens. For example, text-sm md:text-base lg:text-lg makes text larger on bigger screens. Use flex-col md:flex-row to stack on mobile, row on desktop. Hide/show elements with hidden md:block. Customize breakpoints in tailwind.config.js. Use container class for max-width containers. Combine with React state for dynamic behavior based on screen size using window.matchMedia or libraries like react-responsive. At BIPO, our dashboard is fully responsive using Tailwind breakpoints. Tables become cards on mobile, sidebars collapse, and forms stack vertically. We use the mobile-first approach, designing for phones first then enhancing for larger screens.",
                            bulletPoints: [
                                "Mobile-first breakpoint prefixes: sm:, md:, lg:, xl:, 2xl:",
                                "Base styles for mobile, override at larger screens",
                                "Example: text-sm md:text-base lg:text-lg",
                                "Layout changes: flex-col md:flex-row",
                                "Hide/show: hidden md:block",
                                "Customize breakpoints in config",
                                "Combine with React state for dynamic behavior"
                            ],
                            keyConcepts: [
                                {
                                    name: "Mobile-First Approach",
                                    explanation: "Mobile-first means designing for small screens first, then adding styles for larger screens. In Tailwind, unprefixed utilities apply to all sizes, and prefixed ones override at that breakpoint and above. This is better than desktop-first because it's easier to enhance than strip down. It's like building a foundation and adding floors versus building a mansion and trying to fit it in a small lot."
                                },
                                {
                                    name: "Breakpoint System",
                                    explanation: "Tailwind's breakpoints are min-width media queries: sm (640px), md (768px), lg (1024px), xl (1280px), 2xl (1536px). When you use md:text-lg, it applies at 768px and above. You can customize these in the config. The system is simple but powerful - you can create complex responsive layouts with just class names. It's like having predefined size categories that cover most use cases."
                                },
                                {
                                    name: "Responsive Utilities",
                                    explanation: "Almost every Tailwind utility can be made responsive with breakpoint prefixes. This includes layout (flex, grid), spacing (padding, margin), sizing (width, height), typography, and more. This consistency makes responsive design intuitive - if you know the utility, you know how to make it responsive. It's like having a universal remote that works the same way for all devices."
                                }
                            ]
                        },
                        {
                            id: 63,
                            question: "Explain CSS-in-JS solutions (styled-components, emotion) and their pros/cons.",
                            suggestedAnswer: "CSS-in-JS writes CSS in JavaScript, generating styles at runtime or build time. Styled-components uses tagged template literals to create styled React components. Emotion is similar but more flexible with both object and string styles. Pros: scoped styles preventing conflicts, dynamic styling based on props, automatic vendor prefixing, dead code elimination, and co-location with components. Cons: runtime overhead (though Emotion has zero-runtime option), larger bundle sizes, learning curve, and potential performance issues with many styled components. Server-side rendering requires additional setup. Use CSS-in-JS when you need dynamic styling based on props or theme. At Liven Group, we used styled-components for our component library before migrating to Tailwind. It worked well for theming and dynamic styles but added bundle size and had SSR complexity. We switched to Tailwind for better performance and simpler mental model.",
                            bulletPoints: [
                                "Write CSS in JavaScript, generate styles at runtime/build time",
                                "Styled-components: tagged templates, styled.div``",
                                "Emotion: flexible, object or string styles",
                                "Pros: scoped styles, dynamic styling, auto-prefixing, co-location",
                                "Cons: runtime overhead, larger bundles, SSR complexity",
                                "Use for: dynamic styling, theming, prop-based styles",
                                "Alternatives: Tailwind (utility-first), CSS Modules (scoped CSS)"
                            ],
                            keyConcepts: [
                                {
                                    name: "Runtime vs Zero-Runtime",
                                    explanation: "Traditional CSS-in-JS (styled-components) generates styles at runtime in the browser, adding overhead. Zero-runtime solutions (Linaria, vanilla-extract) extract styles at build time to static CSS files, eliminating runtime cost. Runtime solutions are more flexible but slower. Zero-runtime is faster but less dynamic. It's like the difference between cooking to order versus pre-made meals."
                                },
                                {
                                    name: "Dynamic Styling",
                                    explanation: "CSS-in-JS excels at dynamic styling based on props or state. You can write styles that change based on component props without class name logic. For example, background: ${props => props.primary ? 'blue' : 'gray'}. This is harder with traditional CSS. However, this flexibility comes with performance cost if overused. It's like having a customizable product versus standard options."
                                },
                                {
                                    name: "Theming",
                                    explanation: "CSS-in-JS libraries provide built-in theming through context. Define a theme object with colors, spacing, etc., and access it in any styled component. This makes dark mode and brand customization easy. However, CSS custom properties (variables) can achieve similar results with better performance. It's like having a centralized settings panel versus hardcoded values throughout your code."
                                }
                            ]
                        },
                        {
                            id: 64,
                            question: "How do you implement dark mode in a React application?",
                            suggestedAnswer: "Implement dark mode using CSS custom properties (variables) or a styling solution's theming. Define color variables for light and dark themes, toggle a class or data attribute on the root element, and update variables accordingly. Store preference in localStorage to persist across sessions. Use matchMedia to detect system preference. With Tailwind, use dark: prefix for dark mode styles and configure darkMode: 'class' in config. With styled-components, use ThemeProvider with light/dark theme objects. Provide a toggle button to switch modes. Consider using libraries like next-themes for Next.js. At BIPO, we implement dark mode with Tailwind's dark mode feature. We detect system preference on first visit, allow manual toggle, and persist the choice in localStorage. Our color palette uses semantic tokens like bg-primary that map to different colors in light vs dark mode, ensuring consistency.",
                            bulletPoints: [
                                "Use CSS custom properties for theme colors",
                                "Toggle class/data attribute on root element",
                                "Store preference in localStorage",
                                "Detect system preference with matchMedia",
                                "Tailwind: dark: prefix with darkMode: 'class' config",
                                "Styled-components: ThemeProvider with theme objects",
                                "Provide toggle UI and persist user choice"
                            ],
                            keyConcepts: [
                                {
                                    name: "CSS Custom Properties",
                                    explanation: "CSS variables let you define colors once and reference them throughout your CSS. Define --bg-primary: white in light mode and --bg-primary: black in dark mode, then use var(--bg-primary) everywhere. When you toggle themes, just update the variables and all colors change automatically. It's like having a central color palette versus hardcoding colors everywhere."
                                },
                                {
                                    name: "System Preference Detection",
                                    explanation: "Modern browsers support prefers-color-scheme media query to detect if users prefer dark mode at the OS level. Use window.matchMedia('(prefers-color-scheme: dark)') to check this in JavaScript. Respect this preference by default but allow manual override. It's like having a default setting that matches user expectations but letting them customize if they want."
                                },
                                {
                                    name: "Flash of Wrong Theme",
                                    explanation: "When loading a page, there can be a flash where the wrong theme shows before JavaScript loads the saved preference. Prevent this by loading theme from localStorage in a blocking script in the HTML head, before React hydrates. This ensures the correct theme is applied immediately. It's like pre-heating an oven so it's ready when you need it."
                                }
                            ]
                        },
                        {
                            id: 65,
                            question: "What are accessibility (a11y) best practices for React components?",
                            suggestedAnswer: "Accessibility best practices include: use semantic HTML (button, nav, main) instead of divs, provide alt text for images, ensure keyboard navigation works (tab order, focus management), use ARIA attributes when semantic HTML isn't enough, maintain sufficient color contrast, provide labels for form inputs, make interactive elements focusable, announce dynamic content changes with ARIA live regions, and test with screen readers. In React, manage focus with useRef, handle keyboard events, and use libraries like react-aria for accessible components. Avoid div/span for clickable elements - use button. Ensure modals trap focus. At BIPO, we prioritize accessibility because we serve HR systems used by diverse employees. We use semantic HTML, test with screen readers, ensure keyboard navigation works throughout the app, and use automated tools like axe-core to catch accessibility issues during development.",
                            bulletPoints: [
                                "Use semantic HTML: button, nav, main, article",
                                "Provide alt text for images, labels for inputs",
                                "Ensure keyboard navigation: tab order, focus management",
                                "Use ARIA attributes when semantic HTML insufficient",
                                "Maintain color contrast ratios (WCAG guidelines)",
                                "Make interactive elements focusable and keyboard accessible",
                                "Test with screen readers and automated tools",
                                "Announce dynamic changes with ARIA live regions"
                            ],
                            keyConcepts: [
                                {
                                    name: "Semantic HTML",
                                    explanation: "Semantic HTML uses elements that describe their meaning: button for buttons, nav for navigation, main for main content. Screen readers understand these elements and announce them appropriately. Using div with onClick instead of button loses this semantic meaning and requires extra ARIA attributes. It's like using the right tool for the job - a hammer for nails, not a shoe."
                                },
                                {
                                    name: "Focus Management",
                                    explanation: "Focus management means controlling where keyboard focus goes, especially important for modals, dropdowns, and dynamic content. When opening a modal, focus should move into it and be trapped there until closed. When closing, focus returns to the trigger. Use useRef and element.focus() in React. It's like guiding someone through a building - you don't leave them lost, you show them where to go next."
                                },
                                {
                                    name: "ARIA Attributes",
                                    explanation: "ARIA (Accessible Rich Internet Applications) attributes provide additional semantics when HTML isn't enough. aria-label provides accessible names, aria-expanded indicates expandable state, role defines element purpose. However, use semantic HTML first - ARIA is a supplement, not replacement. It's like adding subtitles to a movie - helpful but the movie should make sense without them too."
                                }
                            ]
                        }
                    ]
                },
                {
                    id: 6,
                    name: "Next.js (Bonus)",
                    questions: [
                        {
                            id: 66,
                            question: "What is Next.js and what advantages does it provide over Create React App?",
                            suggestedAnswer: "Next.js is a React framework that provides server-side rendering, static site generation, and built-in routing, while Create React App is a client-side only React starter. Next.js advantages include: better SEO with server-side rendering, faster initial page loads, automatic code splitting, file-based routing without react-router, API routes for backend functionality, image and font optimization, and multiple rendering strategies (SSR, SSG, ISR). It handles configuration complexity and provides production optimizations out of the box. CRA is simpler for SPAs but requires additional setup for routing, SSR, and optimization. Next.js is better for content-heavy sites, e-commerce, and applications needing SEO. CRA is fine for dashboards and internal tools where SEO doesn't matter. Next.js has steeper learning curve but better performance and DX. While I haven't used Next.js in production at Liven or BIPO, I understand its architecture and how it compares to traditional React SPAs that we've built.",
                            bulletPoints: [
                                "Next.js: SSR, SSG, ISR; CRA: client-side only",
                                "Better SEO with server-side rendering",
                                "Faster initial page loads",
                                "File-based routing built-in",
                                "API routes for backend functionality",
                                "Image and font optimization",
                                "Production optimizations out of the box"
                            ],
                            keyConcepts: [
                                {
                                    name: "Server-Side Rendering",
                                    explanation: "SSR generates HTML on the server before sending to the browser. This means search engines see full content immediately, improving SEO. It also improves perceived performance - users see content faster even though JavaScript is still loading. CRA renders everything client-side, so initial HTML is empty. It's like receiving a completed form versus a blank form you have to fill out yourself."
                                },
                                {
                                    name: "Framework vs Library",
                                    explanation: "Next.js is an opinionated framework with conventions for routing, data fetching, and deployment. CRA is a build tool for React (a library), giving you more freedom but requiring more decisions. Frameworks provide structure and best practices but less flexibility. Libraries give flexibility but more setup work. It's like following a recipe versus improvising a dish."
                                },
                                {
                                    name: "Hybrid Rendering",
                                    explanation: "Next.js lets you choose rendering strategy per page - some pages server-rendered, others static, others client-only. This flexibility is powerful for complex apps with different content types. CRA is purely client-side - you'd need to eject and add SSR manually. It's like having multiple cooking methods available versus only one."
                                }
                            ]
                        },
                        {
                            id: 67,
                            question: "Explain the different rendering methods in Next.js (SSR, SSG, ISR, CSR).",
                            suggestedAnswer: "Next.js supports four rendering methods. SSR (Server-Side Rendering) generates HTML on each request using getServerSideProps - best for dynamic, user-specific content. SSG (Static Site Generation) generates HTML at build time using getStaticProps - fastest performance, best for content that rarely changes. ISR (Incremental Static Regeneration) combines both - pages are static but regenerate in background after revalidation period - balances performance and freshness. CSR (Client-Side Rendering) is traditional React rendering in the browser - use for highly interactive dashboards. Choose SSR for personalized content, SSG for marketing pages, ISR for product catalogs, CSR for admin dashboards. Next.js can mix these strategies in one app. The key trade-off is freshness versus performance - SSR is freshest but slowest, SSG is fastest but can be stale, ISR balances both, CSR is flexible but SEO-poor.",
                            bulletPoints: [
                                "SSR: HTML on each request (getServerSideProps)",
                                "SSG: HTML at build time (getStaticProps)",
                                "ISR: static with background regeneration (revalidate)",
                                "CSR: client-side rendering (traditional React)",
                                "SSR: dynamic, fresh, slower",
                                "SSG: static, fast, can be stale",
                                "ISR: static performance with periodic updates"
                            ],
                            keyConcepts: [
                                {
                                    name: "Server-Side Rendering (SSR)",
                                    explanation: "SSR generates HTML on the server for each request. When a user visits, Next.js runs getServerSideProps, fetches data, renders the page, and sends HTML. This ensures fresh content but adds latency since each request waits for data fetching and rendering. Use for user dashboards, personalized content, or data that changes constantly. It's like cooking a fresh meal for each customer."
                                },
                                {
                                    name: "Static Site Generation (SSG)",
                                    explanation: "SSG generates HTML at build time. Next.js runs getStaticProps during build, creates HTML files, and serves them as static assets from CDN. This is incredibly fast - just serving pre-built HTML. Use for marketing pages, blogs, documentation. The downside is content is frozen until next build. It's like meal prepping - fast to serve but prepared in advance."
                                },
                                {
                                    name: "Incremental Static Regeneration (ISR)",
                                    explanation: "ISR is SSG with automatic updates. Pages are static but regenerate in background after a revalidation period. First request after revalidation serves stale content while triggering regeneration, subsequent requests get updated content. This gives SSG speed with SSR freshness. Use for product catalogs, news sites. It's like having pre-made meals that get refreshed periodically."
                                }
                            ]
                        },
                        {
                            id: 68,
                            question: "How does file-based routing work in Next.js?",
                            suggestedAnswer: "Next.js uses file-based routing where the file structure in pages or app directory determines routes. A file at pages/about.js becomes the /about route. Index files map to the directory root - pages/index.js is the homepage. Dynamic routes use brackets: pages/posts/[id].js matches /posts/1, /posts/2, etc. Access the parameter with useRouter or getServerSideProps params. Catch-all routes use [...slug].js to match multiple segments. Nested routes follow folder structure - pages/blog/posts/first.js becomes /blog/posts/first. API routes work similarly in pages/api directory. In App Router, use page.js for routes and folders for nesting. This convention-over-configuration approach eliminates routing configuration and makes the site structure intuitive. It's one of Next.js's most loved features because it's so simple yet powerful.",
                            bulletPoints: [
                                "File structure determines routes",
                                "pages/about.js â†’ /about route",
                                "Index files: pages/index.js â†’ /",
                                "Dynamic routes: [id].js for /posts/:id",
                                "Catch-all routes: [...slug].js for multiple segments",
                                "Nested routes follow folder structure",
                                "API routes in pages/api directory"
                            ],
                            keyConcepts: [
                                {
                                    name: "Convention Over Configuration",
                                    explanation: "File-based routing eliminates the need to configure routes in code. The file structure IS the routing configuration. This reduces boilerplate and makes it obvious where to find code for a specific route. No more hunting through routing files to find which component renders for a URL. It's like organizing files in folders automatically creating a website structure."
                                },
                                {
                                    name: "Dynamic Routes",
                                    explanation: "Dynamic routes use square brackets in filenames to capture URL parameters. [id].js matches any value in that position, accessible via router.query.id. This is perfect for detail pages like blog posts or products. You can have multiple dynamic segments: [category]/[id].js. It's like having a template that works for many similar pages."
                                },
                                {
                                    name: "Catch-All Routes",
                                    explanation: "Catch-all routes using [...slug].js match any number of URL segments. For example, docs/[...slug].js matches /docs/a, /docs/a/b, /docs/a/b/c. The slug parameter is an array of segments. This is useful for documentation sites or flexible content structures. Optional catch-all uses [[...slug]].js to also match the base route. It's like a wildcard that captures everything."
                                }
                            ]
                        },
                        {
                            id: 69,
                            question: "What are API routes in Next.js and when would you use them?",
                            suggestedAnswer: "API routes let you build API endpoints as part of your Next.js application. Files in pages/api directory become serverless functions. A file at pages/api/users.js becomes the /api/users endpoint. Export a handler function that receives req and res objects, similar to Express. API routes run on the server, can access databases and secrets, and support all HTTP methods. Use them for form submissions, database queries, authentication, webhooks, or proxying external APIs. They're serverless by default on Vercel but can run on Node.js servers. Benefits include: no CORS issues since same origin, shared code with frontend, simplified deployment. However, for complex APIs, a separate backend might be better. API routes are perfect for simple backend needs without setting up a separate server. They blur the line between frontend and backend, enabling full-stack development in one codebase.",
                            bulletPoints: [
                                "Serverless API endpoints in pages/api directory",
                                "pages/api/users.js â†’ /api/users endpoint",
                                "Export handler(req, res) function",
                                "Run on server, access databases and secrets",
                                "Support all HTTP methods",
                                "Use for: forms, auth, webhooks, API proxying",
                                "No CORS issues, shared code with frontend"
                            ],
                            keyConcepts: [
                                {
                                    name: "Serverless Functions",
                                    explanation: "API routes are serverless functions that run on-demand when called. On Vercel, each API route is a separate Lambda function. They scale automatically and you only pay for execution time. This is different from a traditional server that's always running. For low-traffic APIs, this is cost-effective. For high-traffic, consider a dedicated server. It's like calling an Uber versus owning a car."
                                },
                                {
                                    name: "Full-Stack in One Codebase",
                                    explanation: "API routes enable full-stack development without separate backend projects. Your frontend and API live together, share TypeScript types, and deploy together. This simplifies development and deployment. However, this coupling can be a downside if you need the API to serve multiple clients or scale independently. It's like having a combo meal versus ordering items separately."
                                },
                                {
                                    name: "Middleware Pattern",
                                    explanation: "API routes don't have built-in middleware like Express, but you can create wrapper functions for common logic like authentication, error handling, or validation. Libraries like next-connect provide Express-like middleware support. You can also use Next.js middleware for request interception before routes. It's like building your own toolbox for handling requests."
                                }
                            ]
                        },
                        {
                            id: 70,
                            question: "Explain the App Router vs Pages Router in Next.js 13+.",
                            suggestedAnswer: "App Router is the new routing system in Next.js 13+ using the app directory, while Pages Router uses the pages directory. App Router introduces React Server Components by default, nested layouts, parallel routes, and streaming. It uses file conventions: page.js for routes, layout.js for layouts, loading.js for loading states, error.js for error boundaries. Data fetching happens in async components, not getServerSideProps. App Router has better performance with automatic code splitting and streaming. Pages Router is more mature with broader ecosystem support and simpler mental model. Both are fully supported and can coexist in the same app for incremental migration. App Router is the future but has a steeper learning curve. Choose App Router for new projects to leverage latest features, Pages Router for simpler projects or when ecosystem compatibility matters. The mental model shifts from pages to components with the App Router.",
                            bulletPoints: [
                                "App Router: app/ directory, Server Components",
                                "Pages Router: pages/ directory, traditional React",
                                "App Router: nested layouts, parallel routes, streaming",
                                "App Router: file conventions (page.js, layout.js, loading.js)",
                                "App Router: async components for data fetching",
                                "Pages Router: getServerSideProps, getStaticProps",
                                "Both supported, can coexist for migration"
                            ],
                            keyConcepts: [
                                {
                                    name: "React Server Components",
                                    explanation: "App Router uses React Server Components by default - components that run only on the server, never sent to the client. They can directly access databases and don't increase bundle size. Client Components (marked with 'use client') run on both server and client. This architecture enables better performance and simpler data fetching. It's like having backend code in your component file, but safely on the server."
                                },
                                {
                                    name: "Nested Layouts",
                                    explanation: "App Router supports nested layouts that persist across route changes. Define a layout.js that wraps child routes, and it won't re-render when navigating between children. This is great for shared navigation, sidebars, or authentication wrappers. Pages Router required custom _app.js modifications for this. It's like having a picture frame that stays while you swap the pictures inside."
                                },
                                {
                                    name: "Streaming and Suspense",
                                    explanation: "App Router supports streaming HTML to the browser as it's generated, showing content progressively. Use React Suspense to show loading states for slow components while rendering fast ones immediately. This improves perceived performance. Pages Router sends complete HTML only after everything is ready. It's like streaming a video versus waiting for the entire download."
                                }
                            ]
                        }
                    ]
                },
                {
                    id: 7,
                    name: "Authentication & Security",
                    questions: [
                        {
                            id: 71,
                            question: "How do you implement JWT authentication in .NET Core Web API?",
                            suggestedAnswer: "Implement JWT authentication by installing Microsoft.AspNetCore.Authentication.JwtBearer package. Configure authentication in Program.cs with AddAuthentication and AddJwtBearer, specifying token validation parameters like issuer, audience, signing key, and lifetime. Create a login endpoint that validates credentials and generates JWT tokens using JwtSecurityTokenHandler with claims for user identity and roles. Add [Authorize] attribute to controllers or actions requiring authentication. The middleware validates tokens on each request, extracting claims into User principal. Store tokens in HTTP-only cookies or local storage on client side. Use refresh tokens for long-lived sessions. At BIPO, we use JWT authentication across our microservices. The API Gateway validates tokens and forwards claims to downstream services. We use RS256 asymmetric signing for better security, with public keys distributed to services for validation. Tokens include user ID, roles, and tenant ID claims for multi-tenant authorization.",
                            bulletPoints: [
                                "Install Microsoft.AspNetCore.Authentication.JwtBearer",
                                "Configure AddAuthentication and AddJwtBearer in Program.cs",
                                "Specify validation parameters: issuer, audience, signing key",
                                "Create login endpoint that generates JWT tokens",
                                "Use JwtSecurityTokenHandler with user claims",
                                "Add [Authorize] attribute to protected endpoints",
                                "Implement refresh tokens for long-lived sessions"
                            ],
                            keyConcepts: [
                                {
                                    name: "JWT Structure",
                                    explanation: "JWT (JSON Web Token) has three parts: header (algorithm and type), payload (claims like user ID and roles), and signature (verifies integrity). They're base64 encoded and joined with dots: header.payload.signature. The server signs tokens with a secret key. Clients send tokens in Authorization: Bearer header. The server validates the signature to ensure tokens weren't tampered with. It's like a sealed envelope - you can see the contents but can't change them without breaking the seal."
                                },
                                {
                                    name: "Stateless Authentication",
                                    explanation: "JWT enables stateless authentication - the server doesn't store session data. All information needed for authentication is in the token itself. This scales well because servers don't need to share session state. However, you can't revoke tokens before expiration without additional infrastructure. It's like a ticket to an event - it's valid until the event ends, and you don't need to check a guest list."
                                },
                                {
                                    name: "Claims-Based Identity",
                                    explanation: "JWTs contain claims - statements about the user like ID, email, roles. The server adds claims when creating tokens, and they're available throughout the request pipeline. Use claims for authorization decisions. Common claims: sub (subject/user ID), name, email, role. Custom claims can include tenant ID or permissions. It's like an ID badge that lists your name, department, and access levels."
                                }
                            ]
                        },
                        {
                            id: 72,
                            question: "Explain the difference between authentication and authorization.",
                            suggestedAnswer: "Authentication is verifying who you are - proving your identity with credentials like username and password. Authorization is determining what you're allowed to do - checking permissions after identity is established. Authentication answers 'Who are you?' while authorization answers 'What can you do?'. Authentication happens first, then authorization. For example, logging in is authentication, accessing admin features is authorization. In .NET Core, authentication is handled by authentication middleware and schemes like JWT or cookies. Authorization uses policies and roles to control access to resources. You can be authenticated but not authorized - logged in but lacking permission. At BIPO, authentication verifies employee identity through SSO integration. Authorization checks if they can access specific HR modules based on their role and country. A payroll manager in Singapore can view Singapore payroll but not Australia payroll.",
                            bulletPoints: [
                                "Authentication: verifying identity (who you are)",
                                "Authorization: determining permissions (what you can do)",
                                "Authentication happens first, then authorization",
                                "Authentication: login, credentials, identity verification",
                                "Authorization: roles, policies, access control",
                                "Can be authenticated but not authorized",
                                "Different middleware and mechanisms in .NET Core"
                            ],
                            keyConcepts: [
                                {
                                    name: "Identity Verification",
                                    explanation: "Authentication is like showing your ID at airport security - you prove who you are with credentials. Common methods include passwords, biometrics, tokens, or certificates. Once authenticated, the system knows your identity and can make authorization decisions. Without authentication, the system treats you as anonymous. It's the foundation of security - you can't control access without knowing who's accessing."
                                },
                                {
                                    name: "Access Control",
                                    explanation: "Authorization is like having different keys for different doors - your identity determines which resources you can access. It's based on roles (admin, user), permissions (read, write), or policies (custom rules). Authorization happens after authentication using the authenticated identity. You might be authenticated as a regular user but not authorized to access admin features. It's about enforcing the principle of least privilege."
                                },
                                {
                                    name: "Separation of Concerns",
                                    explanation: "Authentication and authorization are separate concerns that should be handled independently. You might change authentication methods (password to SSO) without changing authorization logic. Or update authorization rules without touching authentication. This separation makes systems more flexible and maintainable. It's like having separate systems for verifying ID and checking access levels."
                                }
                            ]
                        },
                        {
                            id: 73,
                            question: "How do you implement role-based and policy-based authorization in .NET Core?",
                            suggestedAnswer: "Role-based authorization uses [Authorize(Roles = \"Admin,Manager\")] attribute to restrict access based on user roles. Add role claims when creating tokens or during authentication. Policy-based authorization is more flexible - define policies in Program.cs with AddAuthorization, specifying requirements like minimum age, specific claims, or custom logic. Use [Authorize(Policy = \"PolicyName\")] on controllers or actions. Implement custom requirements by creating IAuthorizationRequirement and AuthorizationHandler classes. Policies can combine multiple requirements with AND logic. Role-based is simpler for basic scenarios, policy-based handles complex rules. At BIPO, we use policy-based authorization extensively. Policies check user roles, country assignment, and feature flags. For example, ViewPayroll policy requires PayrollManager role AND country claim matching the requested payroll country. This handles our multi-jurisdiction requirements elegantly.",
                            bulletPoints: [
                                "Role-based: [Authorize(Roles = \"Admin,Manager\")]",
                                "Add role claims during authentication",
                                "Policy-based: define policies with AddAuthorization",
                                "Use [Authorize(Policy = \"PolicyName\")]",
                                "Custom requirements: IAuthorizationRequirement + Handler",
                                "Policies combine multiple requirements",
                                "Policy-based more flexible for complex rules"
                            ],
                            keyConcepts: [
                                {
                                    name: "Role-Based Access Control (RBAC)",
                                    explanation: "RBAC assigns permissions to roles, then assigns roles to users. A user with Admin role gets all admin permissions. It's simple and works well for basic scenarios. However, it can become unwieldy with many roles or complex permission combinations. Roles are typically stored as claims in the user's identity. It's like job titles determining what you can do - managers can approve expenses, employees can submit them."
                                },
                                {
                                    name: "Policy-Based Authorization",
                                    explanation: "Policies define authorization rules as code, not just role checks. A policy can require specific claims, check custom logic, or combine multiple conditions. For example, a policy might require Admin role OR Manager role with specific department. Policies are more maintainable than scattering authorization logic throughout code. They centralize authorization rules in one place. It's like having a rulebook that defines who can do what under various conditions."
                                },
                                {
                                    name: "Custom Requirements",
                                    explanation: "Custom requirements let you implement any authorization logic. Create a requirement class and a handler that evaluates it. The handler can access the user's claims, check databases, call external services, etc. For example, a MinimumAgeRequirement that checks birthdate claim. This extensibility makes policy-based authorization incredibly powerful for complex scenarios. It's like writing custom rules for your specific business logic."
                                }
                            ]
                        },
                        {
                            id: 74,
                            question: "What is OAuth 2.0 and OpenID Connect, and how do you implement them?",
                            suggestedAnswer: "OAuth 2.0 is an authorization framework for delegated access - allowing apps to access resources on behalf of users without sharing passwords. It uses access tokens issued by an authorization server. OpenID Connect (OIDC) is an identity layer on top of OAuth 2.0, adding authentication with ID tokens containing user information. OAuth 2.0 is for authorization (accessing APIs), OIDC is for authentication (verifying identity). Common flows: Authorization Code for web apps, Client Credentials for service-to-service. Implement by using libraries like IdentityServer, Auth0, or Azure AD. Configure authentication with AddOpenIdConnect in .NET Core. At BIPO, we use OIDC with Azure AD for employee authentication. Users log in through Azure AD SSO, receive ID tokens with profile information and access tokens for API calls. This provides single sign-on across our applications and integrates with enterprise identity providers.",
                            bulletPoints: [
                                "OAuth 2.0: authorization framework for delegated access",
                                "OpenID Connect: authentication layer on OAuth 2.0",
                                "OAuth: access tokens for API access",
                                "OIDC: ID tokens with user information",
                                "Flows: Authorization Code, Client Credentials, Implicit",
                                "Use libraries: IdentityServer, Auth0, Azure AD",
                                "Configure with AddOpenIdConnect in .NET Core"
                            ],
                            keyConcepts: [
                                {
                                    name: "Delegated Access",
                                    explanation: "OAuth 2.0 enables delegated access - granting limited access to your resources without sharing credentials. For example, allowing a photo printing app to access your Google Photos without giving it your Google password. The app gets an access token with specific scopes (permissions). This is safer than sharing passwords and allows fine-grained control. It's like giving a valet key that only starts the car, not opens the trunk."
                                },
                                {
                                    name: "Authorization vs Authentication",
                                    explanation: "OAuth 2.0 is for authorization (what you can access), not authentication (who you are). OpenID Connect adds authentication on top. OAuth gives access tokens for API calls. OIDC adds ID tokens with user information for authentication. Many people confuse them because they're often used together. Use OAuth for API access, OIDC for login. It's like the difference between a key card (access) and an ID badge (identity)."
                                },
                                {
                                    name: "Authorization Code Flow",
                                    explanation: "The most secure OAuth flow for web apps. User is redirected to authorization server, logs in, grants permission, and is redirected back with an authorization code. The app exchanges this code for access and ID tokens. The code exchange happens server-side with client secret, preventing token exposure to the browser. This flow is recommended for applications that can keep secrets secure. It's like getting a ticket stub that you exchange for the actual ticket at the counter."
                                }
                            ]
                        },
                        {
                            id: 75,
                            question: "How do you securely store and manage sensitive data (passwords, API keys)?",
                            suggestedAnswer: "Never store passwords in plain text - hash them with bcrypt, Argon2, or PBKDF2 with salt. Use .NET's PasswordHasher or Identity framework. For API keys and secrets, use environment variables or secret management services like Azure Key Vault, AWS Secrets Manager, or HashiCorp Vault. Never commit secrets to source control. Use User Secrets for local development. In production, inject secrets through environment variables or secret managers. Encrypt sensitive data at rest in databases using Transparent Data Encryption or column-level encryption. Use HTTPS for data in transit. Implement secret rotation policies. At BIPO, we use Azure Key Vault for all secrets - database connection strings, API keys, encryption keys. Applications authenticate to Key Vault using managed identities, eliminating hardcoded credentials. We rotate secrets quarterly and use Key Vault references in app configuration for seamless updates.",
                            bulletPoints: [
                                "Hash passwords with bcrypt, Argon2, or PBKDF2 + salt",
                                "Use PasswordHasher or ASP.NET Identity",
                                "Store secrets in Azure Key Vault, AWS Secrets Manager, or Vault",
                                "Never commit secrets to source control",
                                "Use User Secrets for local development",
                                "Encrypt data at rest (TDE, column encryption)",
                                "Implement secret rotation policies"
                            ],
                            keyConcepts: [
                                {
                                    name: "Password Hashing",
                                    explanation: "Hashing is one-way encryption - you can't reverse it to get the original password. When users log in, hash their input and compare to stored hash. Use algorithms designed for passwords (bcrypt, Argon2) that are slow and resistant to brute force. Add salt (random data) to prevent rainbow table attacks. Never use fast hashes like MD5 or SHA1 for passwords. It's like shredding documents - you can verify a document matches the shreds but can't reconstruct it."
                                },
                                {
                                    name: "Secret Management Services",
                                    explanation: "Secret managers like Azure Key Vault centralize secret storage with access control, auditing, and rotation. Applications retrieve secrets at runtime instead of having them in config files. This separates secrets from code and provides a single source of truth. Secrets are encrypted at rest and in transit. You can rotate secrets without redeploying applications. It's like a secure vault versus hiding keys under the doormat."
                                },
                                {
                                    name: "Defense in Depth",
                                    explanation: "Use multiple layers of security: hash passwords, encrypt databases, use HTTPS, restrict access, audit logs, rotate secrets. If one layer fails, others provide protection. Don't rely on a single security measure. This is especially important for sensitive data like personal information or financial data. It's like having multiple locks on a door plus an alarm system."
                                }
                            ]
                        },
                        {
                            id: 76,
                            question: "What are common web security vulnerabilities (OWASP Top 10) and how do you prevent them?",
                            suggestedAnswer: "OWASP Top 10 includes: SQL Injection - use parameterized queries or ORMs like EF Core. XSS (Cross-Site Scripting) - sanitize input and encode output, use Content Security Policy. Broken Authentication - implement MFA, secure session management, strong password policies. Sensitive Data Exposure - encrypt data at rest and in transit with HTTPS. XML External Entities - disable XXE in XML parsers. Broken Access Control - implement proper authorization checks. Security Misconfiguration - use secure defaults, remove unnecessary features. Insecure Deserialization - validate and sanitize serialized data. Using Components with Known Vulnerabilities - keep dependencies updated. Insufficient Logging - implement comprehensive logging and monitoring. At BIPO, we address these through: parameterized queries in EF Core, React's built-in XSS protection, JWT with refresh tokens, Azure Key Vault for secrets, HTTPS everywhere, policy-based authorization, regular dependency updates with Dependabot, and centralized logging with Application Insights.",
                            bulletPoints: [
                                "SQL Injection: parameterized queries, ORMs",
                                "XSS: sanitize input, encode output, CSP headers",
                                "Broken Authentication: MFA, secure sessions, strong passwords",
                                "Sensitive Data Exposure: encryption, HTTPS",
                                "Broken Access Control: proper authorization checks",
                                "Security Misconfiguration: secure defaults, minimal features",
                                "Vulnerable Dependencies: keep packages updated"
                            ],
                            keyConcepts: [
                                {
                                    name: "SQL Injection",
                                    explanation: "SQL Injection occurs when attackers insert malicious SQL code through user input. For example, entering ' OR '1'='1 in a login form to bypass authentication. Prevent by using parameterized queries or ORMs that automatically parameterize. Never concatenate user input into SQL strings. It's like having a form that accepts only specific formats versus accepting anything and processing it as commands."
                                },
                                {
                                    name: "Cross-Site Scripting (XSS)",
                                    explanation: "XSS injects malicious scripts into web pages viewed by other users. For example, posting <script>alert('hacked')./script> in a comment. Prevent by encoding output (converting < to &lt;), sanitizing input, and using Content Security Policy headers. React automatically escapes values in JSX, providing protection. It's like filtering mail for dangerous items before delivering it."
                                },
                                {
                                    name: "Defense in Depth",
                                    explanation: "Don't rely on a single security measure. Use multiple layers: input validation, output encoding, authentication, authorization, encryption, logging. If one fails, others provide protection. Security is not a checklist but an ongoing process. Regular security audits, penetration testing, and staying updated on vulnerabilities are essential. It's like having multiple security systems protecting your house."
                                }
                            ]
                        },
                        {
                            id: 77,
                            question: "How do you implement HTTPS and SSL/TLS in .NET Core applications?",
                            suggestedAnswer: "HTTPS is enabled by default in .NET Core. For development, use dotnet dev-certs https --trust to create a trusted certificate. In production, obtain SSL certificates from Certificate Authorities like Let's Encrypt or purchase from providers. Configure Kestrel in appsettings.json with certificate path and password, or use Azure App Service which handles certificates automatically. Enforce HTTPS with app.UseHttpsRedirection() middleware. Set HSTS (HTTP Strict Transport Security) headers with app.UseHsts() to force HTTPS for future requests. Configure minimum TLS version to 1.2 or higher. Use certificate renewal automation for Let's Encrypt. At BIPO, we use Azure App Service with managed certificates that auto-renew. We enforce HTTPS redirection and HSTS with one-year max-age. Our API Gateway terminates SSL and communicates with internal services over private network, balancing security and performance.",
                            bulletPoints: [
                                "Development: dotnet dev-certs https --trust",
                                "Production: obtain certificates from CA (Let's Encrypt, etc.)",
                                "Configure Kestrel with certificate path and password",
                                "Azure App Service: managed certificates with auto-renewal",
                                "Enforce HTTPS: app.UseHttpsRedirection()",
                                "HSTS headers: app.UseHsts() for future requests",
                                "Set minimum TLS version to 1.2+"
                            ],
                            keyConcepts: [
                                {
                                    name: "SSL/TLS Encryption",
                                    explanation: "SSL/TLS encrypts data between client and server, preventing eavesdropping and tampering. When you visit an HTTPS site, the browser and server establish an encrypted connection using certificates. The certificate proves the server's identity and provides encryption keys. Modern web uses TLS 1.2 or 1.3 (SSL is deprecated). It's like sending a letter in a locked box only the recipient can open."
                                },
                                {
                                    name: "Certificate Authorities",
                                    explanation: "Certificate Authorities (CAs) are trusted organizations that issue SSL certificates after verifying domain ownership. Browsers trust certificates signed by known CAs. Let's Encrypt provides free automated certificates. Self-signed certificates work but browsers show warnings. In production, always use CA-signed certificates. It's like having an official ID card versus a homemade one."
                                },
                                {
                                    name: "HSTS (HTTP Strict Transport Security)",
                                    explanation: "HSTS tells browsers to always use HTTPS for your site, even if users type http://. This prevents downgrade attacks where attackers intercept the initial HTTP request. Once a browser sees the HSTS header, it remembers to use HTTPS for the specified duration (max-age). This improves security and performance by eliminating the HTTP to HTTPS redirect. It's like training people to always use the secure entrance."
                                }
                            ]
                        },
                        {
                            id: 78,
                            question: "Explain CSRF attacks and how to prevent them in web applications.",
                            suggestedAnswer: "CSRF (Cross-Site Request Forgery) tricks authenticated users into executing unwanted actions. An attacker creates a malicious site with a form that submits to your site, exploiting the user's existing session. For example, a hidden form that transfers money when the page loads. Prevent CSRF with anti-forgery tokens - ASP.NET Core generates unique tokens per session, validates them on POST requests. Use [ValidateAntiForgeryToken] attribute on actions. For APIs, use custom headers that browsers can't set cross-origin, or SameSite cookie attribute. Modern browsers support SameSite=Strict or Lax to prevent cookies from being sent in cross-site requests. SPAs using JWT in Authorization header are naturally CSRF-resistant. At BIPO, our React app uses JWT in headers, not cookies, eliminating CSRF risk. For cookie-based authentication, we use SameSite=Strict and anti-forgery tokens on state-changing operations.",
                            bulletPoints: [
                                "CSRF: tricks users into unwanted actions via existing session",
                                "Use anti-forgery tokens (ASP.NET Core built-in)",
                                "[ValidateAntiForgeryToken] attribute on POST actions",
                                "Custom headers for APIs (can't be set cross-origin)",
                                "SameSite cookie attribute (Strict or Lax)",
                                "JWT in Authorization header is CSRF-resistant",
                                "Only protect state-changing operations (POST, PUT, DELETE)"
                            ],
                            keyConcepts: [
                                {
                                    name: "How CSRF Works",
                                    explanation: "CSRF exploits the browser's automatic cookie sending. When you're logged into a site, your browser stores a session cookie. If you visit a malicious site while logged in, that site can make requests to the legitimate site, and your browser automatically includes the session cookie. The legitimate site thinks it's you making the request. It's like someone using your ID badge when you leave it lying around."
                                },
                                {
                                    name: "Anti-Forgery Tokens",
                                    explanation: "Anti-forgery tokens are random values generated per session and included in forms as hidden fields. When the form submits, the server validates the token matches the session. Attackers can't get the token because of same-origin policy - they can trigger requests but can't read responses. This proves the request came from your site, not a malicious one. It's like a secret handshake only legitimate users know."
                                },
                                {
                                    name: "SameSite Cookies",
                                    explanation: "SameSite cookie attribute controls whether cookies are sent in cross-site requests. Strict prevents all cross-site cookie sending. Lax allows cookies on safe methods (GET) from cross-site but not POST. None allows all but requires Secure flag. Modern browsers default to Lax. This provides CSRF protection at the browser level. It's like having a security guard who checks if requests are coming from within the building."
                                }
                            ]
                        },
                        {
                            id: 79,
                            question: "How do you handle token refresh in React applications?",
                            suggestedAnswer: "Implement token refresh by storing access tokens (short-lived, 15-30 minutes) and refresh tokens (long-lived, days/weeks). When API calls return 401 Unauthorized, intercept with axios or fetch interceptor, call refresh endpoint with refresh token to get new access token, retry original request with new token. Store tokens in memory or httpOnly cookies for security. Implement refresh logic in a central auth service or HTTP client wrapper. Handle refresh failures by logging out. Use a mutex to prevent multiple simultaneous refresh requests. At BIPO, we use axios interceptors for token refresh. Access tokens expire in 15 minutes, refresh tokens in 7 days. When a 401 occurs, we refresh the token and retry. If refresh fails, we redirect to login. We store refresh tokens in httpOnly cookies and access tokens in memory to prevent XSS attacks. This provides good UX without constant logins while maintaining security.",
                            bulletPoints: [
                                "Access tokens: short-lived (15-30 min)",
                                "Refresh tokens: long-lived (days/weeks)",
                                "Intercept 401 responses with axios/fetch interceptor",
                                "Call refresh endpoint to get new access token",
                                "Retry original request with new token",
                                "Store refresh tokens in httpOnly cookies",
                                "Use mutex to prevent concurrent refresh requests"
                            ],
                            keyConcepts: [
                                {
                                    name: "Token Expiration Strategy",
                                    explanation: "Short-lived access tokens limit damage if stolen - they expire quickly. Long-lived refresh tokens provide good UX - users don't log in constantly. If an access token is stolen, it's only valid for minutes. Refresh tokens are more sensitive and should be stored securely (httpOnly cookies). This balances security and usability. It's like having a temporary badge that expires quickly but a permanent ID card to get new badges."
                                },
                                {
                                    name: "Interceptor Pattern",
                                    explanation: "HTTP interceptors wrap all API calls, automatically handling token refresh. When any request gets 401, the interceptor refreshes the token and retries. This centralizes refresh logic - individual components don't need to handle it. Use a mutex or queue to ensure only one refresh happens at a time when multiple requests fail simultaneously. It's like having a security checkpoint that automatically updates your credentials when they expire."
                                },
                                {
                                    name: "Secure Token Storage",
                                    explanation: "Never store tokens in localStorage - it's vulnerable to XSS attacks. Store access tokens in memory (React state/context) and refresh tokens in httpOnly cookies. HttpOnly cookies can't be accessed by JavaScript, preventing XSS theft. However, they're vulnerable to CSRF, so use SameSite attribute. Alternatively, store both in httpOnly cookies and use a CSRF token. It's about choosing the right security trade-offs for your threat model."
                                }
                            ]
                        },
                        {
                            id: 80,
                            question: "What is Azure Active Directory (Azure AD) and how do you integrate it?",
                            suggestedAnswer: "Azure Active Directory (now Microsoft Entra ID) is Microsoft's cloud-based identity and access management service. It provides authentication, single sign-on, multi-factor authentication, and conditional access. Integrate by registering your application in Azure AD portal, configuring redirect URIs and API permissions. In .NET Core, use Microsoft.Identity.Web package and configure AddMicrosoftIdentityWebApp for web apps or AddMicrosoftIdentityWebApi for APIs. In React, use MSAL (Microsoft Authentication Library) to handle login flow and token acquisition. Azure AD supports OAuth 2.0 and OpenID Connect. Benefits include: enterprise SSO, integration with Office 365, MFA, conditional access policies, and centralized user management. At BIPO, we use Azure AD for employee authentication across all applications. Employees log in once with their corporate credentials and access all systems. We use conditional access to require MFA for sensitive operations and restrict access by location and device compliance.",
                            bulletPoints: [
                                "Cloud-based identity and access management service",
                                "Provides: authentication, SSO, MFA, conditional access",
                                "Register app in Azure AD portal",
                                ".NET Core: Microsoft.Identity.Web package",
                                "React: MSAL (Microsoft Authentication Library)",
                                "Supports OAuth 2.0 and OpenID Connect",
                                "Benefits: enterprise SSO, centralized management, MFA"
                            ],
                            keyConcepts: [
                                {
                                    name: "Single Sign-On (SSO)",
                                    explanation: "SSO lets users log in once and access multiple applications without re-authenticating. Azure AD acts as the identity provider - users authenticate with Azure AD, and applications trust Azure AD's authentication. This improves user experience and security (one strong password versus many weak ones). It also centralizes access control - disable an account in Azure AD and they lose access to all apps. It's like a master key that opens multiple doors."
                                },
                                {
                                    name: "Conditional Access",
                                    explanation: "Conditional access policies control access based on conditions: user location, device compliance, risk level, application sensitivity. For example, require MFA when accessing from outside the office, or block access from certain countries. This provides context-aware security beyond just username and password. Policies are configured in Azure AD and enforced automatically. It's like having smart locks that change requirements based on who's entering and from where."
                                },
                                {
                                    name: "App Registration",
                                    explanation: "Registering your app in Azure AD creates an identity for it - a client ID and optionally a client secret. This establishes trust between your app and Azure AD. Configure redirect URIs where Azure AD sends users after authentication, and API permissions your app needs. The registration defines how your app interacts with Azure AD. It's like registering your business to operate in a jurisdiction."
                                }
                            ]
                        }
                    ]
                },
                {
                    id: 8,
                    name: "Azure Cloud & DevOps",
                    questions: [
                        {
                            id: 81,
                            question: "What Azure services would you use for hosting a .NET Core Web API?",
                            suggestedAnswer: "For hosting .NET Core Web APIs in Azure, main options are: Azure App Service for fully managed PaaS with easy deployment, auto-scaling, and built-in CI/CD - best for most scenarios. Azure Container Instances for containerized apps without orchestration complexity. Azure Kubernetes Service (AKS) for microservices needing orchestration, scaling, and complex deployments. Azure Functions for serverless, event-driven APIs with consumption-based pricing. Azure Virtual Machines for full control but more management overhead. Also consider Azure SQL Database or Cosmos DB for data, Azure Key Vault for secrets, Application Insights for monitoring, Azure CDN for static assets, and Azure API Management for API gateway. While I have extensive AWS experience from Liven Group (Lambda, DynamoDB, S3, SNS), I understand Azure's equivalent services. The concepts translate well - App Service is like Elastic Beanstalk, AKS is like EKS, Functions are like Lambda. I'm confident I can quickly adapt my cloud expertise to Azure.",
                            bulletPoints: [
                                "Azure App Service: managed PaaS, easy deployment, auto-scaling",
                                "Azure Container Instances: containerized apps, simple",
                                "Azure Kubernetes Service (AKS): microservices, orchestration",
                                "Azure Functions: serverless, event-driven, consumption pricing",
                                "Azure SQL Database/Cosmos DB: data storage",
                                "Azure Key Vault: secrets management",
                                "Application Insights: monitoring and telemetry"
                            ],
                            keyConcepts: [
                                {
                                    name: "Platform as a Service (PaaS)",
                                    explanation: "Azure App Service is PaaS - you deploy code and Azure handles infrastructure, OS updates, scaling, load balancing. You focus on application logic, not server management. This is ideal for most web APIs unless you need specific infrastructure control. PaaS is faster to deploy and cheaper to operate than managing VMs. It's like renting a furnished apartment versus building a house."
                                },
                                {
                                    name: "Serverless vs Always-On",
                                    explanation: "Azure Functions are serverless - they run on-demand and scale to zero when idle. You pay only for execution time. App Service is always-on - instances run continuously. Use Functions for sporadic workloads or event-driven processing. Use App Service for consistent traffic. Functions have cold start latency but lower cost for low traffic. It's like paying per ride versus owning a car."
                                },
                                {
                                    name: "Service Selection Criteria",
                                    explanation: "Choose based on requirements: App Service for standard web APIs with predictable traffic. Functions for event-driven or sporadic workloads. AKS for microservices needing orchestration. Container Instances for simple containerized apps. Consider factors: traffic patterns, scaling needs, budget, team expertise, and integration requirements. It's like choosing transportation - bicycle, car, or plane depending on distance and budget."
                                }
                            ]
                        },
                        {
                            id: 82,
                            question: "Explain the difference between Azure App Service, Azure Functions, and Azure Container Instances.",
                            suggestedAnswer: "Azure App Service is a fully managed PaaS for hosting web apps and APIs. It provides continuous deployment, auto-scaling, custom domains, and SSL. You deploy code or containers, and Azure handles infrastructure. Best for traditional web applications with consistent traffic. Azure Functions is serverless compute for event-driven code. Functions run in response to triggers (HTTP, timer, queue), scale automatically, and you pay per execution. Best for background jobs, webhooks, or sporadic workloads. Azure Container Instances runs Docker containers without orchestration. It's simpler than Kubernetes, good for isolated containers, batch jobs, or development. Choose App Service for full-featured web apps, Functions for event-driven serverless, Container Instances for simple containerized workloads. At Liven, we used similar AWS services - Elastic Beanstalk (like App Service), Lambda (like Functions), and ECS (like Container Instances). The concepts are identical across cloud providers.",
                            bulletPoints: [
                                "App Service: managed PaaS for web apps/APIs",
                                "Functions: serverless, event-driven compute",
                                "Container Instances: run Docker containers without orchestration",
                                "App Service: always-on, predictable traffic",
                                "Functions: pay per execution, scales to zero",
                                "Container Instances: simple containers, no orchestration",
                                "Choose based on workload pattern and requirements"
                            ],
                            keyConcepts: [
                                {
                                    name: "Compute Models",
                                    explanation: "These services represent different compute models. App Service is traditional hosting - your app runs continuously on dedicated instances. Functions are event-driven - code runs only when triggered. Container Instances are container-based - you bring your own Docker image. Each model suits different scenarios. App Service for web apps, Functions for background processing, Container Instances for containerized workloads. It's like different types of restaurants - full-service, food truck, or catering."
                                },
                                {
                                    name: "Scaling Characteristics",
                                    explanation: "App Service scales by adding instances based on metrics like CPU or requests. Functions scale automatically per function, potentially to thousands of concurrent executions. Container Instances don't auto-scale - you manually create instances. App Service scaling is predictable and controlled. Functions scaling is elastic and automatic. Consider your scaling needs when choosing. It's like having a fixed staff versus calling in help as needed."
                                },
                                {
                                    name: "Pricing Models",
                                    explanation: "App Service charges for instance hours - you pay whether traffic is high or low. Functions charge per execution and memory - you pay only for what you use. Container Instances charge per second of runtime. For consistent traffic, App Service is cost-effective. For sporadic workloads, Functions save money. Analyze your traffic patterns to choose the most economical option. It's like a monthly subscription versus pay-per-use."
                                }
                            ]
                        },
                        {
                            id: 83,
                            question: "How do you implement CI/CD pipelines for .NET Core applications in Azure DevOps?",
                            suggestedAnswer: "Create CI/CD pipelines in Azure DevOps using YAML or classic editor. CI pipeline: trigger on code commit, restore NuGet packages, build solution, run unit tests, publish artifacts. Use dotnet restore, dotnet build, dotnet test, dotnet publish commands. CD pipeline: trigger on CI completion, download artifacts, deploy to Azure App Service or AKS using Azure CLI or deployment tasks. Implement stages for dev, staging, production with approvals. Use variable groups for environment-specific configuration. Store secrets in Azure Key Vault and reference in pipelines. Implement automated testing, code quality checks with SonarQube, and security scanning. At Liven, we used similar patterns with Terraform and Docker. We had multi-stage pipelines: build Docker images, push to ECR, deploy to ECS with blue-green deployments. The principles are identical - automate build, test, and deployment with environment promotion and approvals.",
                            bulletPoints: [
                                "CI: trigger on commit, restore, build, test, publish",
                                "Use dotnet CLI commands in pipeline",
                                "CD: download artifacts, deploy to Azure services",
                                "Multi-stage pipelines: dev, staging, production",
                                "Variable groups for environment config",
                                "Azure Key Vault for secrets",
                                "Automated testing and quality gates"
                            ],
                            keyConcepts: [
                                {
                                    name: "Continuous Integration (CI)",
                                    explanation: "CI automatically builds and tests code on every commit. This catches integration issues early. The CI pipeline compiles code, runs unit tests, and produces artifacts (DLLs, Docker images). If any step fails, the pipeline stops and developers are notified. This ensures the main branch is always in a deployable state. It's like having a quality check at every step of an assembly line."
                                },
                                {
                                    name: "Continuous Deployment (CD)",
                                    explanation: "CD automatically deploys code to environments after CI succeeds. Typically you have multiple stages: dev (automatic), staging (automatic), production (manual approval). Each stage runs deployment tasks, smoke tests, and health checks. CD reduces manual deployment errors and speeds up releases. It's like having an automated delivery system that requires sign-off at critical checkpoints."
                                },
                                {
                                    name: "Pipeline as Code",
                                    explanation: "YAML pipelines define CI/CD as code, versioned with your application. This makes pipelines reproducible, reviewable, and portable. You can test pipeline changes in branches before merging. YAML pipelines are more flexible than classic UI pipelines. They support templates, conditions, and complex workflows. It's like having your deployment instructions in a recipe that anyone can follow."
                                }
                            ]
                        },
                        {
                            id: 84,
                            question: "What is Docker and how do you containerize a .NET Core application?",
                            suggestedAnswer: "Docker is a containerization platform that packages applications with dependencies into isolated containers. Containers are lightweight, portable, and consistent across environments. To containerize .NET Core: create a Dockerfile with base image (mcr.microsoft.com/dotnet/aspnet:8.0), copy source code, restore packages, build application, and define entry point. Use multi-stage builds: build stage with SDK image, runtime stage with smaller runtime image. Build image with docker build, run with docker run. Benefits include: consistent environments, easy deployment, isolation, and scalability. At Liven Group, we extensively used Docker for our .NET services. We created multi-stage Dockerfiles to minimize image size, used Docker Compose for local development, and deployed to AWS ECS. Our images were 100MB runtime images versus 500MB+ with SDK. We also used Terraform to manage ECS infrastructure as code.",
                            bulletPoints: [
                                "Docker: packages apps with dependencies into containers",
                                "Dockerfile: defines image build steps",
                                "Multi-stage builds: SDK for build, runtime for deployment",
                                "Base image: mcr.microsoft.com/dotnet/aspnet:8.0",
                                "Commands: docker build, docker run, docker push",
                                "Benefits: portability, consistency, isolation",
                                "Use .dockerignore to exclude unnecessary files"
                            ],
                            keyConcepts: [
                                {
                                    name: "Containers vs Virtual Machines",
                                    explanation: "Containers share the host OS kernel, making them lightweight and fast to start. VMs include a full OS, making them heavier. Containers use less resources and start in seconds versus minutes for VMs. However, VMs provide stronger isolation. For most applications, containers are sufficient and more efficient. It's like apartments in a building (containers) versus separate houses (VMs)."
                                },
                                {
                                    name: "Multi-Stage Builds",
                                    explanation: "Multi-stage Dockerfiles use one image for building (with SDK, build tools) and another for runtime (with only runtime dependencies). This dramatically reduces final image size. The build stage compiles code, the runtime stage copies only the compiled output. Smaller images deploy faster and have smaller attack surface. It's like cooking in a full kitchen but serving on simple plates."
                                },
                                {
                                    name: "Image Layers",
                                    explanation: "Docker images are built in layers. Each Dockerfile instruction creates a layer. Layers are cached and reused when unchanged. Order instructions from least to most frequently changing (base image, dependencies, code). This speeds up builds by reusing cached layers. For example, copy package files and restore before copying source code. It's like building with LEGO - reuse unchanged sections, only rebuild what changed."
                                }
                            ]
                        },
                        {
                            id: 85,
                            question: "Explain the basics of Kubernetes and when you would use it.",
                            suggestedAnswer: "Kubernetes is a container orchestration platform that automates deployment, scaling, and management of containerized applications. It manages clusters of nodes running containers, handles load balancing, self-healing, rolling updates, and service discovery. Key concepts: Pods (smallest unit, one or more containers), Deployments (manage replicas), Services (expose pods), Ingress (routing), ConfigMaps/Secrets (configuration). Use Kubernetes for microservices architectures, applications needing high availability and auto-scaling, or multi-container applications. It's overkill for simple applications - use App Service or Container Instances instead. Kubernetes has steep learning curve but provides powerful orchestration. At Liven, we considered Kubernetes but chose AWS ECS for simpler orchestration needs. For BIPO's microservices architecture, Kubernetes would be appropriate given the complexity and scale. The decision depends on team expertise, application complexity, and operational requirements.",
                            bulletPoints: [
                                "Container orchestration platform",
                                "Automates deployment, scaling, management",
                                "Key concepts: Pods, Deployments, Services, Ingress",
                                "Features: load balancing, self-healing, rolling updates",
                                "Use for: microservices, high availability, complex apps",
                                "Overkill for simple applications",
                                "Steep learning curve but powerful"
                            ],
                            keyConcepts: [
                                {
                                    name: "Orchestration",
                                    explanation: "Orchestration means coordinating multiple containers to work together. Kubernetes decides which nodes run which containers, restarts failed containers, scales replicas based on load, and routes traffic. You declare desired state (3 replicas of this service), and Kubernetes makes it happen. This is crucial for complex applications with many services. It's like a conductor coordinating an orchestra versus musicians playing independently."
                                },
                                {
                                    name: "Declarative Configuration",
                                    explanation: "Kubernetes uses declarative configuration - you describe what you want (YAML manifests), not how to achieve it. You say 'I want 3 replicas' not 'start container 1, start container 2, start container 3'. Kubernetes figures out the steps. This makes configuration reproducible and version-controllable. It's like telling a chef what dish you want versus explaining every cooking step."
                                },
                                {
                                    name: "When to Use Kubernetes",
                                    explanation: "Use Kubernetes for microservices with many services, applications needing complex scaling and deployment strategies, or when you need portability across cloud providers. Don't use it for simple monoliths, small teams without DevOps expertise, or when simpler solutions suffice. Kubernetes adds operational complexity - only adopt if benefits outweigh costs. It's like using a commercial kitchen for a home cook versus a restaurant."
                                }
                            ]
                        },
                        {
                            id: 86,
                            question: "How do you implement logging and monitoring in Azure (Application Insights, Log Analytics)?",
                            suggestedAnswer: "Application Insights provides APM (Application Performance Monitoring) for .NET applications. Install Microsoft.ApplicationInsights.AspNetCore package, configure in Program.cs with AddApplicationInsightsTelemetry. It automatically tracks requests, dependencies, exceptions, and custom events. Use TelemetryClient to log custom metrics and events. Application Insights provides dashboards, alerts, and analytics. Log Analytics is Azure Monitor's log aggregation service - query logs with KQL (Kusto Query Language). Implement structured logging with ILogger, sending logs to Application Insights or Log Analytics. Use correlation IDs to trace requests across services. Set up alerts for errors, performance degradation, or custom metrics. At BIPO, we use Application Insights for all services. We track custom metrics like payroll processing time, failed authentication attempts, and API latency. We have alerts for error rate spikes and slow queries. Dashboards show real-time system health, and we use KQL queries for troubleshooting production issues.",
                            bulletPoints: [
                                "Application Insights: APM for .NET applications",
                                "Install Microsoft.ApplicationInsights.AspNetCore",
                                "Tracks: requests, dependencies, exceptions, custom events",
                                "Log Analytics: log aggregation with KQL queries",
                                "Use ILogger for structured logging",
                                "Correlation IDs for distributed tracing",
                                "Set up alerts and dashboards"
                            ],
                            keyConcepts: [
                                {
                                    name: "Application Performance Monitoring (APM)",
                                    explanation: "APM tools like Application Insights monitor application health, performance, and usage. They track request rates, response times, failure rates, and dependencies. This visibility helps identify bottlenecks, errors, and usage patterns. APM is essential for production systems - you can't fix what you can't see. It's like having health monitors on a patient versus just asking how they feel."
                                },
                                {
                                    name: "Structured Logging",
                                    explanation: "Structured logging uses key-value pairs instead of plain text messages. For example, 'User {UserId} logged in' with UserId=123 instead of 'User 123 logged in'. This makes logs queryable and analyzable. You can filter, aggregate, and alert on specific fields. Use ILogger with message templates for structured logging. It's like having a database versus a text file - structured data is much more useful."
                                },
                                {
                                    name: "KQL (Kusto Query Language)",
                                    explanation: "KQL is Azure's query language for logs and metrics. It's similar to SQL but optimized for time-series data and log analysis. You can filter, aggregate, join, and visualize data. KQL queries power dashboards and alerts. Learning KQL is essential for Azure monitoring. It's like SQL for logs - a powerful tool for extracting insights from data."
                                }
                            ]
                        },
                        {
                            id: 87,
                            question: "What is Azure Key Vault and how do you use it to manage secrets?",
                            suggestedAnswer: "Azure Key Vault is a cloud service for securely storing and accessing secrets, keys, and certificates. It provides centralized secret management with access control, auditing, and rotation. Create a Key Vault in Azure portal, add secrets like connection strings or API keys. In .NET Core, use Azure.Extensions.AspNetCore.Configuration.Secrets package to load secrets into configuration. Authenticate using Managed Identity (no credentials in code) or Service Principal. Reference secrets with builder.Configuration.AddAzureKeyVault. Key Vault integrates with Azure services - App Service can reference Key Vault secrets directly. Benefits include: centralized management, access control, audit logs, automatic rotation, and compliance. At BIPO, all secrets are in Key Vault - database passwords, API keys, encryption keys. Applications use Managed Identity to access Key Vault, eliminating hardcoded credentials. We rotate secrets quarterly and Key Vault audit logs track all access for compliance.",
                            bulletPoints: [
                                "Cloud service for secrets, keys, certificates",
                                "Centralized management with access control and auditing",
                                "Use Azure.Extensions.AspNetCore.Configuration.Secrets",
                                "Authenticate with Managed Identity (no credentials)",
                                "Reference with AddAzureKeyVault in configuration",
                                "Benefits: security, auditing, rotation, compliance",
                                "Integrates with Azure services"
                            ],
                            keyConcepts: [
                                {
                                    name: "Managed Identity",
                                    explanation: "Managed Identity is Azure's way of giving applications an identity without managing credentials. Azure automatically handles authentication between your app and Key Vault. No passwords or keys in code or config. This is the most secure way to access Azure resources. It's like having a badge that automatically authenticates you versus carrying around passwords."
                                },
                                {
                                    name: "Secret Rotation",
                                    explanation: "Secret rotation means periodically changing secrets to limit exposure if compromised. Key Vault supports versioning - add a new version of a secret without breaking existing apps. Applications can reference 'latest' version or specific versions. Implement rotation policies and automate the process. Regular rotation is a security best practice. It's like changing locks periodically for security."
                                },
                                {
                                    name: "Separation of Concerns",
                                    explanation: "Key Vault separates secret management from application code. Developers don't need access to production secrets - they're managed by operations team. Applications retrieve secrets at runtime. This reduces risk of accidental exposure and enables different secrets per environment. It's like having a safe deposit box versus keeping valuables at home."
                                }
                            ]
                        },
                        {
                            id: 88,
                            question: "How do you implement distributed tracing in microservices?",
                            suggestedAnswer: "Distributed tracing tracks requests across multiple services using correlation IDs. Implement with OpenTelemetry or Application Insights. Each service adds a correlation ID to outgoing requests and logs. This creates a trace showing the request path through services. In .NET Core, use Activity API or OpenTelemetry SDK. Configure HttpClient to propagate trace context. Application Insights automatically correlates requests, dependencies, and logs. Traces show which services were called, in what order, with timings and errors. This is crucial for debugging microservices - you can see the entire request flow. At BIPO, distributed tracing helped us identify a slow database query in the payroll service that was causing timeouts in the employee portal. We traced a request through API Gateway, employee service, payroll service, and found the bottleneck. Without tracing, this would have taken days to debug.",
                            bulletPoints: [
                                "Tracks requests across multiple services",
                                "Use correlation IDs to link related operations",
                                "Implement with OpenTelemetry or Application Insights",
                                "Activity API in .NET Core for trace context",
                                "HttpClient propagates trace headers automatically",
                                "Shows request path, timings, and errors",
                                "Essential for debugging microservices"
                            ],
                            keyConcepts: [
                                {
                                    name: "Correlation ID",
                                    explanation: "A correlation ID is a unique identifier for a request that's passed between services. When Service A calls Service B, it includes the correlation ID. Both services log with this ID. You can then query logs for that ID to see the entire request flow. This connects distributed logs into a coherent story. It's like a tracking number for a package - you can see its journey through multiple facilities."
                                },
                                {
                                    name: "Trace Context Propagation",
                                    explanation: "Trace context includes correlation ID, parent span ID, and trace flags. It's propagated through HTTP headers (traceparent, tracestate). Each service reads incoming context, creates a child span, and propagates context to downstream calls. This builds a tree of operations. OpenTelemetry standardizes this propagation. It's like a relay race where each runner passes the baton with instructions."
                                },
                                {
                                    name: "Observability in Microservices",
                                    explanation: "Microservices are harder to debug than monoliths because requests span multiple services. Distributed tracing provides observability - understanding system behavior from outputs. Combined with logs and metrics, tracing enables debugging production issues. Without tracing, you're blind to inter-service interactions. It's like having a map of a city versus wandering without directions."
                                }
                            ]
                        },
                        {
                            id: 89,
                            question: "Explain Infrastructure as Code (IaC) and tools like Terraform or ARM templates.",
                            suggestedAnswer: "Infrastructure as Code manages infrastructure through code rather than manual processes. Define servers, networks, databases in configuration files, version control them, and apply changes automatically. Benefits include: reproducibility, version control, automation, and consistency across environments. Terraform is cloud-agnostic IaC tool using HCL language. It supports multiple providers (Azure, AWS, GCP). ARM (Azure Resource Manager) templates are Azure-specific JSON templates. Terraform advantages: multi-cloud, better state management, more readable syntax. ARM advantages: native Azure support, no state file management. At Liven Group, we extensively used Terraform to manage AWS infrastructure - ECS clusters, load balancers, databases, S3 buckets. We had separate state files per environment, used modules for reusability, and applied changes through CI/CD pipelines. This made infrastructure reproducible and auditable.",
                            bulletPoints: [
                                "Manage infrastructure through code, not manual processes",
                                "Benefits: reproducibility, version control, automation",
                                "Terraform: cloud-agnostic, HCL language, multi-provider",
                                "ARM templates: Azure-specific, JSON format",
                                "Terraform: better state management, readable syntax",
                                "ARM: native Azure support, no external state",
                                "Version control infrastructure like application code"
                            ],
                            keyConcepts: [
                                {
                                    name: "Declarative vs Imperative",
                                    explanation: "IaC is declarative - you describe desired state, not steps to achieve it. You say 'I want a VM with these specs' not 'create VM, configure network, attach disk'. The tool figures out what changes are needed. This is more maintainable than imperative scripts. You can apply the same configuration repeatedly and get the same result. It's like ordering from a menu versus giving cooking instructions."
                                },
                                {
                                    name: "State Management",
                                    explanation: "IaC tools track infrastructure state - what resources exist and their configuration. Terraform stores state in files (local or remote). When you apply changes, Terraform compares desired state (code) with current state (state file) and makes necessary changes. State enables drift detection and safe updates. It's like having a blueprint and knowing what's actually built."
                                },
                                {
                                    name: "Infrastructure Reproducibility",
                                    explanation: "With IaC, you can recreate entire environments from code. Destroy and rebuild environments for testing. Create identical dev, staging, production environments. Recover from disasters by reapplying code. This is impossible with manual processes. IaC makes infrastructure disposable and repeatable. It's like having a recipe you can follow to make the same dish every time."
                                }
                            ]
                        },
                        {
                            id: 90,
                            question: "How do you implement blue-green or canary deployments in Azure?",
                            suggestedAnswer: "Blue-green deployment runs two identical environments - blue (current) and green (new). Deploy new version to green, test it, then switch traffic from blue to green. If issues arise, switch back to blue. In Azure App Service, use deployment slots - deploy to staging slot, test, then swap with production slot. Swap is instant and reversible. Canary deployment gradually shifts traffic to new version - start with 5%, monitor, increase to 25%, 50%, 100%. Use Azure Traffic Manager or Application Gateway for traffic splitting. Azure App Service supports testing in production with traffic routing rules. At Liven, we implemented blue-green deployments with AWS ECS. We had two task definitions, deployed new version to green, updated load balancer to point to green, kept blue running for rollback. This enabled zero-downtime deployments and easy rollbacks. The pattern is identical in Azure with deployment slots.",
                            bulletPoints: [
                                "Blue-green: two identical environments, instant switch",
                                "Azure App Service: use deployment slots",
                                "Deploy to staging slot, test, swap with production",
                                "Swap is instant and reversible",
                                "Canary: gradually shift traffic (5%, 25%, 50%, 100%)",
                                "Use Traffic Manager or Application Gateway for splitting",
                                "Enables zero-downtime deployments and easy rollbacks"
                            ],
                            keyConcepts: [
                                {
                                    name: "Blue-Green Deployment",
                                    explanation: "Blue-green maintains two production environments. Only one serves traffic at a time. Deploy new version to idle environment, test thoroughly, then switch traffic. If problems occur, switch back immediately. This provides instant rollback and zero downtime. The downside is double infrastructure cost. It's like having a backup generator ready to switch on instantly."
                                },
                                {
                                    name: "Canary Deployment",
                                    explanation: "Canary releases new version to a small subset of users first. Monitor for errors or performance issues. If metrics look good, gradually increase traffic to new version. If issues arise, route all traffic back to old version. This limits blast radius of bugs. Canary is safer than big-bang releases but takes longer. It's like testing water temperature with your toe before jumping in."
                                },
                                {
                                    name: "Deployment Slots",
                                    explanation: "Azure App Service deployment slots are separate instances of your app with their own hostnames. Deploy to a slot, warm it up, test it, then swap with production. Swapping is instant - just switches routing. Slots share the same App Service Plan, so no extra compute cost. This makes blue-green deployments easy in Azure. It's like having a stage behind the curtain where you prepare before the show."
                                }
                            ]
                        }
                    ]
                },
                {
                    id: 9,
                    name: "Integrations & Scheduled Jobs",
                    questions: [
                        {
                            id: 91,
                            question: "How do you implement scheduled background jobs in .NET Core (Hangfire, Quartz.NET)?",
                            suggestedAnswer: "For scheduled background jobs, use Hangfire or Quartz.NET. Hangfire is simpler - install Hangfire.AspNetCore, configure in Program.cs with AddHangfire and UseHangfireServer, create jobs with BackgroundJob.Enqueue or RecurringJob.AddOrUpdate. It provides a dashboard for monitoring. Quartz.NET is more powerful but complex - define jobs implementing IJob, create triggers with cron expressions, schedule with IScheduler. For simple scenarios, use IHostedService with Timer. Hangfire stores job state in database (SQL Server, Redis), ensuring jobs survive restarts. Use Hangfire for fire-and-forget, delayed, and recurring jobs. At BIPO, we use Hangfire for scheduled payroll processing, report generation, and data synchronization. We have recurring jobs that run nightly to process payroll for different countries, generate compliance reports, and sync employee data from HR systems. Hangfire's dashboard helps us monitor job execution and troubleshoot failures.",
                            bulletPoints: [
                                "Hangfire: simple, dashboard, fire-and-forget/delayed/recurring",
                                "Install Hangfire.AspNetCore, configure in Program.cs",
                                "BackgroundJob.Enqueue, RecurringJob.AddOrUpdate",
                                "Quartz.NET: powerful, complex, cron expressions",
                                "IHostedService with Timer for simple scenarios",
                                "Hangfire stores state in database (survives restarts)",
                                "Use for: scheduled tasks, background processing"
                            ],
                            keyConcepts: [
                                {
                                    name: "Fire-and-Forget Jobs",
                                    explanation: "Fire-and-forget jobs run once in the background without blocking the request. For example, sending an email after user registration. The request returns immediately while the job executes asynchronously. This improves response times for operations that don't need immediate completion. Hangfire queues these jobs and processes them reliably. It's like dropping a letter in a mailbox - you don't wait for delivery."
                                },
                                {
                                    name: "Recurring Jobs",
                                    explanation: "Recurring jobs run on a schedule defined by cron expressions. For example, '0 2 * * *' runs daily at 2 AM. Use for periodic tasks like report generation, data cleanup, or batch processing. Hangfire ensures jobs run even if the application restarts. Jobs can be updated or deleted without redeployment. It's like setting a recurring calendar reminder."
                                },
                                {
                                    name: "Job Persistence",
                                    explanation: "Hangfire stores job state in a database, making jobs durable. If the application crashes, jobs resume after restart. This is crucial for important background tasks. In-memory solutions lose jobs on restart. The persistence layer also enables distributed processing - multiple servers can process jobs from the same queue. It's like writing tasks in a notebook versus remembering them."
                                }
                            ]
                        },
                        {
                            id: 92,
                            question: "Explain how to integrate payment gateways (Stripe, PayPal) in .NET Core.",
                            suggestedAnswer: "Integrate payment gateways using their official SDKs. For Stripe, install Stripe.net package, configure API key from appsettings or Key Vault, create PaymentIntent for one-time payments or Subscription for recurring. Handle webhooks for payment events (succeeded, failed). For PayPal, use PayPal SDK, create orders, capture payments, handle IPNs (Instant Payment Notifications). Always validate webhooks using signature verification. Never trust client-side payment status - verify server-side. Store payment details securely, comply with PCI DSS if handling card data. Use payment gateway's hosted checkout pages to avoid PCI compliance burden. At Liven Group, we integrated Stripe for restaurant payment processing. We created PaymentIntents for customer orders, handled webhooks for payment confirmation, and used Stripe's hosted checkout to avoid storing card data. We also implemented refunds and dispute handling through Stripe's API.",
                            bulletPoints: [
                                "Stripe: install Stripe.net, create PaymentIntent/Subscription",
                                "PayPal: use PayPal SDK, create orders, capture payments",
                                "Handle webhooks for payment events",
                                "Validate webhooks with signature verification",
                                "Never trust client-side payment status",
                                "Use hosted checkout pages for PCI compliance",
                                "Store API keys in Key Vault"
                            ],
                            keyConcepts: [
                                {
                                    name: "Payment Intent Flow",
                                    explanation: "Payment Intent represents the customer's intent to pay. Create PaymentIntent on server with amount and currency, send client secret to frontend, frontend collects payment details and confirms payment, webhook notifies server of success/failure. This flow keeps sensitive data on payment gateway's servers. You never handle raw card numbers. It's like having a secure courier handle valuable packages."
                                },
                                {
                                    name: "Webhook Security",
                                    explanation: "Webhooks notify your server of payment events. Attackers could fake webhooks to mark orders as paid. Always verify webhook signatures using the gateway's secret key. Stripe includes a signature header you validate with their SDK. Only trust verified webhooks. Also implement idempotency - handle duplicate webhooks gracefully. It's like verifying a letter's seal before trusting its contents."
                                },
                                {
                                    name: "PCI Compliance",
                                    explanation: "PCI DSS (Payment Card Industry Data Security Standard) governs handling card data. Storing card numbers requires expensive compliance audits. Avoid this by using payment gateway's hosted pages or tokenization. The gateway stores card data, you store tokens. This dramatically reduces compliance burden. It's like having a bank hold valuables instead of keeping them at home."
                                }
                            ]
                        },
                        {
                            id: 93,
                            question: "How do you implement file import/export functionality (CSV, Excel, PDF)?",
                            suggestedAnswer: "For CSV, use CsvHelper library for reading and writing. For Excel, use EPPlus or ClosedXML libraries to read/write XLSX files. For PDF, use iTextSharp or QuestPDF for generation. Import: accept file upload with IFormFile, validate file type and size, stream parse to avoid memory issues, validate data, import to database with transactions. Export: query data, generate file in memory or stream, return File result with appropriate content type. Use background jobs for large exports. Validate imports thoroughly - check data types, required fields, business rules. Handle errors gracefully with detailed error messages. At BIPO, we handle bulk employee data imports via Excel. Users upload XLSX files with employee information, we validate each row (required fields, valid dates, unique IDs), show validation errors, and import valid rows. We also export payroll reports to Excel and PDF. Large exports run as background jobs with email notification when complete.",
                            bulletPoints: [
                                "CSV: CsvHelper library for reading/writing",
                                "Excel: EPPlus or ClosedXML for XLSX files",
                                "PDF: iTextSharp or QuestPDF for generation",
                                "Import: validate file, stream parse, validate data, transaction",
                                "Export: query data, generate file, return File result",
                                "Use background jobs for large files",
                                "Validate thoroughly with detailed error messages"
                            ],
                            keyConcepts: [
                                {
                                    name: "Streaming vs In-Memory",
                                    explanation: "Large files can exhaust memory if loaded entirely. Use streaming to process files incrementally - read one row, process it, read next row. This keeps memory usage constant regardless of file size. For exports, stream data from database to file without loading everything. Streaming is essential for scalability. It's like reading a book page by page versus memorizing the whole book."
                                },
                                {
                                    name: "Data Validation",
                                    explanation: "Never trust imported data. Validate file format, data types, required fields, value ranges, business rules, and referential integrity. Collect all validation errors and show them to users. Don't fail on first error - validate entire file. Use transactions to ensure all-or-nothing imports. Validation prevents corrupt data and security issues. It's like quality control on an assembly line."
                                },
                                {
                                    name: "Content Type Headers",
                                    explanation: "When returning files, set correct Content-Type header: text/csv for CSV, application/vnd.openxmlformats-officedocument.spreadsheetml.sheet for Excel, application/pdf for PDF. Also set Content-Disposition header with filename. This tells browsers how to handle the file - download or display. Correct headers ensure proper file handling. It's like labeling a package so handlers know how to treat it."
                                }
                            ]
                        },
                        {
                            id: 94,
                            question: "What is Azure Service Bus and when would you use it?",
                            suggestedAnswer: "Azure Service Bus is a fully managed enterprise message broker supporting queues and topics/subscriptions. Queues provide point-to-point messaging - one sender, one receiver. Topics enable publish-subscribe - one sender, multiple subscribers. Service Bus guarantees message delivery, supports transactions, dead-letter queues, and message scheduling. Use for: decoupling services, load leveling, reliable communication, event distribution. It's more feature-rich than Azure Queue Storage - supports larger messages, FIFO ordering, sessions, and duplicate detection. At BIPO, we use Service Bus for inter-service communication. When payroll is processed, we publish events to a topic. Multiple services subscribe - notification service sends emails, audit service logs changes, reporting service updates dashboards. This decouples services and enables independent scaling. Service Bus's guaranteed delivery ensures no events are lost even during service outages.",
                            bulletPoints: [
                                "Managed enterprise message broker",
                                "Queues: point-to-point messaging",
                                "Topics/Subscriptions: publish-subscribe pattern",
                                "Features: guaranteed delivery, transactions, dead-letter queues",
                                "Use for: decoupling, load leveling, reliable communication",
                                "More features than Queue Storage (FIFO, sessions, deduplication)",
                                "Integrates with .NET via Azure.Messaging.ServiceBus"
                            ],
                            keyConcepts: [
                                {
                                    name: "Queues vs Topics",
                                    explanation: "Queues are for one-to-one messaging - one producer sends messages, one consumer receives them. Topics are for one-to-many - one producer publishes messages, multiple subscribers receive copies. Use queues for task distribution (load leveling). Use topics for event notification (multiple services react to events). Topics enable loose coupling - add subscribers without changing publishers. It's like direct mail versus broadcasting."
                                },
                                {
                                    name: "Guaranteed Delivery",
                                    explanation: "Service Bus guarantees at-least-once delivery. Messages are persisted and won't be lost even if services crash. Receivers acknowledge (complete) messages after processing. Unacknowledged messages return to queue for retry. Dead-letter queue holds messages that fail repeatedly. This reliability is crucial for critical operations like payments or orders. It's like certified mail with delivery confirmation."
                                },
                                {
                                    name: "Decoupling Services",
                                    explanation: "Message brokers decouple services - they don't need to know about each other or be online simultaneously. Producer sends messages to queue/topic, consumers process when ready. This enables independent deployment, scaling, and failure isolation. If a consumer is down, messages queue up and process when it recovers. It's like leaving voicemail versus requiring both parties on the phone."
                                }
                            ]
                        },
                        {
                            id: 95,
                            question: "How do you implement webhooks in .NET Core applications?",
                            suggestedAnswer: "Webhooks are HTTP callbacks that notify external systems of events. To send webhooks: store subscriber URLs and events they're interested in, when events occur, POST JSON payload to subscriber URLs, implement retries with exponential backoff for failures, sign requests with HMAC for security. To receive webhooks: create API endpoint accepting POST requests, validate signature to ensure authenticity, process payload asynchronously (return 200 immediately, process in background), implement idempotency to handle duplicate deliveries. Use IHttpClientFactory for sending webhooks. Store webhook delivery attempts for debugging. At BIPO, we send webhooks when payroll events occur - processed, approved, paid. External systems subscribe to these events. We sign webhooks with HMAC-SHA256 using shared secrets. We also receive webhooks from payment providers, validating signatures before processing. Webhook processing happens in background jobs to avoid blocking the webhook endpoint.",
                            bulletPoints: [
                                "HTTP callbacks to notify external systems of events",
                                "Sending: POST JSON to subscriber URLs with retries",
                                "Sign requests with HMAC for security",
                                "Receiving: validate signature, return 200, process async",
                                "Implement idempotency for duplicate deliveries",
                                "Use IHttpClientFactory for sending",
                                "Store delivery attempts for debugging"
                            ],
                            keyConcepts: [
                                {
                                    name: "Webhook Security",
                                    explanation: "Webhooks are public endpoints - anyone can call them. Secure webhooks by signing payloads with HMAC using a shared secret. Include signature in request header. Receiver validates signature before processing. This proves the webhook came from you, not an attacker. Also use HTTPS and validate payload structure. It's like sealing an envelope with wax - recipients can verify it wasn't tampered with."
                                },
                                {
                                    name: "Retry Logic",
                                    explanation: "Webhook receivers might be down or slow. Implement retries with exponential backoff - retry after 1 minute, 5 minutes, 30 minutes, etc. Give up after several attempts and mark as failed. Store delivery attempts for debugging. Don't retry indefinitely - respect receiver's downtime. Retries ensure reliable delivery despite temporary failures. It's like ringing a doorbell multiple times before leaving a note."
                                },
                                {
                                    name: "Asynchronous Processing",
                                    explanation: "Webhook endpoints should return 200 OK immediately, then process payload asynchronously. This prevents timeouts if processing is slow. Queue the payload for background processing. If processing fails, the sender will retry. Never do slow operations (database writes, external API calls) in the webhook endpoint. It's like accepting a package at the door and unpacking it later."
                                }
                            ]
                        },
                        {
                            id: 96,
                            question: "Explain message queues and how to implement them (RabbitMQ, Azure Queue Storage).",
                            suggestedAnswer: "Message queues enable asynchronous communication between services. Producers send messages to queues, consumers process them independently. RabbitMQ is a feature-rich message broker supporting multiple protocols, exchanges, routing, and complex topologies. Install RabbitMQ.Client, connect to broker, declare queues and exchanges, publish and consume messages. Azure Queue Storage is simpler, Azure-native queue service. Install Azure.Storage.Queues, create queue client, send messages with SendMessageAsync, receive with ReceiveMessagesAsync. Use queues for: load leveling (smooth traffic spikes), decoupling services, background processing, and reliable communication. At-least-once delivery means handle duplicates idempotently. At Liven Group, we used AWS SQS (similar to Azure Queue Storage) for asynchronous order processing. When orders were placed, we queued them for processing. Workers consumed messages, processed orders, and deleted messages. This decoupled order placement from processing, enabling independent scaling.",
                            bulletPoints: [
                                "Asynchronous communication between services",
                                "RabbitMQ: feature-rich, exchanges, routing, complex topologies",
                                "Azure Queue Storage: simple, Azure-native, cost-effective",
                                "Producers send, consumers process independently",
                                "Use for: load leveling, decoupling, background processing",
                                "At-least-once delivery: handle duplicates idempotently",
                                "Install Azure.Storage.Queues or RabbitMQ.Client"
                            ],
                            keyConcepts: [
                                {
                                    name: "Load Leveling",
                                    explanation: "Queues smooth traffic spikes by buffering requests. When traffic surges, requests queue up and process at a steady rate. This prevents overwhelming downstream services. Without queues, spikes could crash services. Consumers process at their own pace, and queues grow during peaks and shrink during lulls. It's like a buffer tank that absorbs pressure variations."
                                },
                                {
                                    name: "At-Least-Once Delivery",
                                    explanation: "Message queues guarantee at-least-once delivery - messages might be delivered multiple times but never lost. This happens when consumers crash after processing but before acknowledging. Design consumers to be idempotent - processing the same message twice has the same effect as once. Use unique message IDs to detect duplicates. It's like a persistent reminder that keeps nagging until acknowledged."
                                },
                                {
                                    name: "Poison Messages",
                                    explanation: "Poison messages are messages that repeatedly fail processing - bad data, bugs, or external service issues. They can block queues if constantly retried. Implement retry limits and dead-letter queues for failed messages. Investigate and fix poison messages separately. This prevents one bad message from blocking all processing. It's like quarantining a problematic item for special handling."
                                }
                            ]
                        },
                        {
                            id: 97,
                            question: "How do you handle third-party API integrations and rate limiting?",
                            suggestedAnswer: "Integrate third-party APIs using HttpClient with IHttpClientFactory for proper connection management. Configure named or typed clients with base URLs, default headers, and timeouts. Implement retry policies with Polly for transient failures - exponential backoff, circuit breaker. Handle rate limiting by respecting Retry-After headers, implementing token bucket or sliding window algorithms, and queuing requests. Cache responses when appropriate to reduce API calls. Use API keys from Key Vault, never hardcode. Log requests and responses for debugging. Implement timeouts to prevent hanging requests. At BIPO, we integrate with multiple HR systems and government APIs. We use Polly for retries and circuit breakers. For rate-limited APIs, we implement token bucket rate limiting and queue requests. We cache frequently accessed data like country codes and tax rates. This ensures reliable integration despite external API issues.",
                            bulletPoints: [
                                "Use HttpClient with IHttpClientFactory",
                                "Configure named/typed clients with base URLs and headers",
                                "Retry policies with Polly (exponential backoff, circuit breaker)",
                                "Handle rate limiting: Retry-After headers, token bucket",
                                "Cache responses to reduce API calls",
                                "Store API keys in Key Vault",
                                "Implement timeouts and comprehensive logging"
                            ],
                            keyConcepts: [
                                {
                                    name: "Circuit Breaker Pattern",
                                    explanation: "Circuit breaker prevents cascading failures when external services are down. After a threshold of failures, the circuit opens - requests fail immediately without calling the API. After a timeout, the circuit half-opens to test if the service recovered. If successful, it closes and normal operation resumes. This protects your system from wasting resources on failing calls. It's like a home circuit breaker that trips to prevent electrical fires."
                                },
                                {
                                    name: "Rate Limiting Strategies",
                                    explanation: "Rate limiting restricts API calls to prevent exceeding quotas. Token bucket algorithm: tokens regenerate at a fixed rate, each request consumes a token. When tokens are exhausted, requests queue or fail. Sliding window tracks requests in a time window. Respect API's Retry-After header telling you when to retry. Implement client-side rate limiting to avoid hitting limits. It's like having a spending budget that refills periodically."
                                },
                                {
                                    name: "IHttpClientFactory",
                                    explanation: "IHttpClientFactory manages HttpClient instances properly, avoiding socket exhaustion. Creating HttpClient per request exhausts sockets. Reusing one instance doesn't respect DNS changes. IHttpClientFactory pools connections and handles DNS refresh. Register typed or named clients in DI. This is essential for production applications. It's like having a connection pool versus creating new connections constantly."
                                }
                            ]
                        },
                        {
                            id: 98,
                            question: "What is Azure Blob Storage and how do you use it for file storage?",
                            suggestedAnswer: "Azure Blob Storage is object storage for unstructured data like images, videos, documents, and backups. It's highly scalable, durable, and cost-effective. Organize blobs in containers (like folders). Three blob types: Block blobs for files, Append blobs for logs, Page blobs for VHDs. Use Azure.Storage.Blobs package. Create BlobServiceClient, get container reference, upload with UploadAsync, download with DownloadToAsync. Generate SAS tokens for temporary access without exposing keys. Use blob tiers (Hot, Cool, Archive) for cost optimization. Enable versioning and soft delete for data protection. At Liven Group, we used AWS S3 (equivalent to Blob Storage) for storing restaurant images, menus, and receipts. We generated pre-signed URLs for temporary access. We also used lifecycle policies to move old data to cheaper storage tiers. The concepts are identical between S3 and Blob Storage.",
                            bulletPoints: [
                                "Object storage for unstructured data (images, videos, documents)",
                                "Organize blobs in containers",
                                "Three types: Block (files), Append (logs), Page (VHDs)",
                                "Use Azure.Storage.Blobs package",
                                "Generate SAS tokens for temporary access",
                                "Blob tiers: Hot, Cool, Archive for cost optimization",
                                "Enable versioning and soft delete"
                            ],
                            keyConcepts: [
                                {
                                    name: "Object Storage vs File Storage",
                                    explanation: "Object storage (Blob Storage) stores files as objects with metadata, accessed via HTTP. File storage (Azure Files) provides SMB file shares like network drives. Object storage is cheaper, more scalable, and better for cloud-native apps. File storage is for legacy apps needing file system access. Use Blob Storage for web assets, backups, data lakes. It's like a warehouse (object storage) versus a filing cabinet (file storage)."
                                },
                                {
                                    name: "Shared Access Signatures (SAS)",
                                    explanation: "SAS tokens provide temporary, limited access to blobs without exposing storage account keys. Generate SAS with specific permissions (read, write), expiration time, and IP restrictions. This is safer than sharing account keys. Users can upload/download directly to Blob Storage without going through your API. It's like giving someone a temporary key card with specific access levels."
                                },
                                {
                                    name: "Storage Tiers",
                                    explanation: "Blob Storage has three tiers: Hot (frequently accessed, higher storage cost, lower access cost), Cool (infrequently accessed, lower storage cost, higher access cost), Archive (rarely accessed, lowest storage cost, highest access cost with retrieval delay). Use lifecycle policies to automatically move blobs between tiers based on age. This optimizes costs for different data access patterns. It's like storing frequently used items nearby and rarely used items in a remote warehouse."
                                }
                            ]
                        },
                        {
                            id: 99,
                            question: "How do you implement email sending functionality (SendGrid, SMTP)?",
                            suggestedAnswer: "For email sending, use SendGrid or SMTP. SendGrid is a cloud email service with high deliverability, templates, and analytics. Install SendGrid package, configure API key from Key Vault, create SendGridClient, compose email with SendGridMessage, send with SendEmailAsync. For SMTP, use MailKit library - create SmtpClient, connect to SMTP server with credentials, compose MimeMessage, send with SendAsync. SendGrid advantages: better deliverability, no SMTP configuration, templates, tracking. SMTP advantages: works with any email provider, no third-party dependency. Use background jobs for sending to avoid blocking requests. Implement retry logic for failures. At BIPO, we use SendGrid for transactional emails - payroll notifications, password resets, compliance alerts. We use templates for consistent branding, track open rates, and handle bounces. SendGrid's deliverability ensures emails reach inboxes, not spam folders. We send emails via Hangfire background jobs to avoid blocking API requests.",
                            bulletPoints: [
                                "SendGrid: cloud email service, high deliverability, templates",
                                "Install SendGrid package, configure API key",
                                "SMTP: MailKit library, works with any email provider",
                                "SendGrid: better deliverability, templates, tracking",
                                "SMTP: no third-party dependency, universal",
                                "Use background jobs for sending",
                                "Implement retry logic and error handling"
                            ],
                            keyConcepts: [
                                {
                                    name: "Email Deliverability",
                                    explanation: "Deliverability is the ability to reach inboxes, not spam folders. Factors include: sender reputation, SPF/DKIM/DMARC records, content quality, engagement rates. SendGrid manages reputation and authentication, improving deliverability. Self-hosted SMTP requires careful configuration. Poor deliverability means your emails are useless. It's like having a phone number that goes straight to voicemail versus one that rings."
                                },
                                {
                                    name: "Email Templates",
                                    explanation: "Templates separate email content from code. Design templates with placeholders for dynamic data. SendGrid provides template editor with versioning. This lets non-developers update email content without code changes. Templates ensure consistent branding and reduce code duplication. You can A/B test templates to improve engagement. It's like mail merge - one template, many personalized emails."
                                },
                                {
                                    name: "Transactional vs Marketing Emails",
                                    explanation: "Transactional emails are triggered by user actions - order confirmations, password resets, notifications. Marketing emails are bulk promotional emails. They have different regulations (CAN-SPAM, GDPR) and deliverability considerations. SendGrid handles both but transactional emails have higher priority. Never send marketing emails through transactional email services - it hurts deliverability. It's like the difference between a receipt and a sales flyer."
                                }
                            ]
                        },
                        {
                            id: 100,
                            question: "Explain event-driven architecture and how to implement it in .NET Core.",
                            suggestedAnswer: "Event-driven architecture uses events to communicate between loosely coupled services. When something happens (order placed, user registered), publish an event. Other services subscribe to events and react. Implement with message brokers like Azure Service Bus, RabbitMQ, or in-process with MediatR. Define event classes, create publishers that send events to broker, create subscribers that handle events. Benefits include: loose coupling, scalability, flexibility, and resilience. Services can be added or removed without affecting others. Use events for: cross-service communication, audit logging, notifications, and data synchronization. At BIPO, we use event-driven architecture extensively. When payroll is processed, we publish PayrollProcessedEvent. Multiple services react: notification service sends emails, audit service logs the event, reporting service updates dashboards, integration service syncs to external systems. This decouples services and enables independent development and deployment. We use Azure Service Bus topics for event distribution.",
                            bulletPoints: [
                                "Services communicate via events, not direct calls",
                                "Publish events when something happens",
                                "Subscribers react to events independently",
                                "Implement with: Service Bus, RabbitMQ, MediatR",
                                "Benefits: loose coupling, scalability, flexibility",
                                "Use for: cross-service communication, audit, notifications",
                                "Events enable independent service development"
                            ],
                            keyConcepts: [
                                {
                                    name: "Loose Coupling",
                                    explanation: "Event-driven architecture decouples services - publishers don't know about subscribers. Add new subscribers without changing publishers. Services can be deployed, scaled, and updated independently. If a subscriber fails, it doesn't affect the publisher or other subscribers. This flexibility is crucial for microservices. It's like a radio broadcast - the broadcaster doesn't know who's listening, and listeners can tune in or out freely."
                                },
                                {
                                    name: "Event Sourcing",
                                    explanation: "Event sourcing stores all state changes as events, not just current state. The event log is the source of truth. Rebuild state by replaying events. This provides complete audit trail, enables time travel debugging, and supports complex business logic. However, it's complex and not always necessary. Use for domains requiring audit trails or complex state management. It's like keeping a detailed diary versus just remembering current situation."
                                },
                                {
                                    name: "Eventual Consistency",
                                    explanation: "Event-driven systems are eventually consistent - changes propagate asynchronously. When an event is published, subscribers process it eventually, not immediately. This improves performance and scalability but complicates reasoning about state. Design UIs to handle eventual consistency - show 'processing' states, use optimistic updates. It's like sending a letter - it arrives eventually, not instantly like a phone call."
                                }
                            ]
                        }
                    ]
                }
            ]
        };

        // State Management
        let currentCategoryId = null;
        let currentQuestionId = null;
        let selectedCategoryFilter = 'all';
        let allQuestions = [];
        let sidebarVisible = true;

        // Initialize the application
        function init() {
            try {
                console.log('Initializing application...');
                console.log('Categories:', interviewData.categories.length);
                buildAllQuestionsList();
                console.log('All questions:', allQuestions.length);
                renderSidebar();
                loadFirstQuestion();
                console.log('Initialization complete');
            } catch (error) {
                console.error('Initialization error:', error);
                alert('Error loading questions: ' + error.message);
            }
        }

        // Toggle sidebar visibility
        function toggleSidebar() {
            const sidebar = document.getElementById('sidebar');
            const toggleBtn = document.getElementById('sidebarToggle');
            
            sidebarVisible = !sidebarVisible;
            
            if (sidebarVisible) {
                sidebar.classList.remove('hidden');
                toggleBtn.classList.remove('visible');
            } else {
                sidebar.classList.add('hidden');
                toggleBtn.classList.add('visible');
            }
        }

        // Build a flat list of all questions with metadata
        function buildAllQuestionsList() {
            allQuestions = [];
            let globalIndex = 1;
            
            interviewData.categories.forEach(category => {
                category.questions.forEach(question => {
                    allQuestions.push({
                        ...question,
                        categoryId: category.id,
                        categoryName: category.name,
                        globalIndex: globalIndex++
                    });
                });
            });
        }

        // Get filtered questions based on category filter
        function getFilteredQuestions() {
            if (selectedCategoryFilter === 'all') {
                return allQuestions;
            }
            return allQuestions.filter(q => q.categoryId === selectedCategoryFilter);
        }

        // Switch between views
        function switchView(view) {
            const questionsView = document.getElementById('questionsView');
            const overviewView = document.getElementById('overviewView');
            const navButtons = document.querySelectorAll('.header-nav-btn');

            if (view === 'questions') {
                questionsView.style.display = 'flex';
                overviewView.classList.remove('active');
                navButtons[0].classList.add('active');
                navButtons[1].classList.remove('active');
            } else {
                questionsView.style.display = 'none';
                overviewView.classList.add('active');
                navButtons[0].classList.remove('active');
                navButtons[1].classList.add('active');
                renderPracticeOverview();
            }
        }

        // Render practice overview
        function renderPracticeOverview() {
            renderOverviewStats();
            renderOverviewTable();
        }

        // Render overview statistics
        function renderOverviewStats() {
            const statsContainer = document.getElementById('overviewStats');
            
            let totalQuestions = 0;
            let totalPractice = 0;
            let questionsWithPractice = 0;
            let questionsNeedingPractice = 0;

            interviewData.categories.forEach(category => {
                category.questions.forEach(question => {
                    totalQuestions++;
                    const suggestedCount = getCounter(question.id, 'suggested');
                    const bulletCount = getCounter(question.id, 'bullet');
                    const totalCount = suggestedCount + bulletCount;
                    
                    totalPractice += totalCount;
                    
                    if (totalCount > 0) {
                        questionsWithPractice++;
                    } else {
                        questionsNeedingPractice++;
                    }
                });
            });

            statsContainer.innerHTML = `
                <div class="stat-card">
                    <h4>Total Questions</h4>
                    <div class="stat-value">${totalQuestions}</div>
                </div>
                <div class="stat-card">
                    <h4>Total Practice Sessions</h4>
                    <div class="stat-value">${totalPractice}</div>
                </div>
                <div class="stat-card">
                    <h4>Questions Practiced</h4>
                    <div class="stat-value">${questionsWithPractice}</div>
                </div>
                <div class="stat-card">
                    <h4>Need Practice</h4>
                    <div class="stat-value">${questionsNeedingPractice}</div>
                </div>
            `;
        }

        // Render overview table
        function renderOverviewTable() {
            const overviewContent = document.getElementById('overviewContent');
            let html = '';

            interviewData.categories.forEach(category => {
                html += `
                    <div class="overview-category">
                        <div class="overview-table">
                            <div class="overview-category-header">${category.name}</div>
                `;

                category.questions.forEach(question => {
                    const suggestedCount = getCounter(question.id, 'suggested');
                    const bulletCount = getCounter(question.id, 'bullet');
                    const totalCount = suggestedCount + bulletCount;
                    
                    let badgeClass = '';
                    if (totalCount === 0) badgeClass = 'low';
                    else if (totalCount < 5) badgeClass = 'medium';
                    else badgeClass = 'high';

                    html += `
                        <div class="overview-question" onclick="navigateToQuestion(${category.id}, ${question.id})">
                            <div class="overview-question-text">${question.question}</div>
                            <div class="overview-practice-count">
                                <span>Suggested: <span class="practice-badge ${badgeClass}">${suggestedCount}x</span></span>
                                <span>Bullet: <span class="practice-badge ${badgeClass}">${bulletCount}x</span></span>
                            </div>
                        </div>
                    `;
                });

                html += `
                        </div>
                    </div>
                `;
            });

            overviewContent.innerHTML = html;
        }

        // Navigate to a specific question from overview
        function navigateToQuestion(categoryId, questionId) {
            switchView('questions');
            loadQuestion(categoryId, questionId);
        }

        // Render sidebar with category filters and all questions
        function renderSidebar() {
            const sidebar = document.getElementById('sidebar');
            
            // Build category filter chips
            let categoryChipsHTML = `
                <div class="category-filter">
                    <div class="category-filter-title">Filter by Category:</div>
                    <div class="category-chips">
                        <div class="category-chip ${selectedCategoryFilter === 'all' ? 'active' : ''}" 
                             onclick="filterByCategory('all')">
                            All
                        </div>
            `;
            
            interviewData.categories.forEach(category => {
                categoryChipsHTML += `
                    <div class="category-chip ${selectedCategoryFilter === category.id ? 'active' : ''}" 
                         onclick="filterByCategory(${category.id})">
                        ${category.name}
                    </div>
                `;
            });
            
            categoryChipsHTML += `
                    </div>
                </div>
            `;

            // Build question list
            const filteredQuestions = getFilteredQuestions();
            let questionsHTML = `
                <div class="question-counter">
                    Showing <strong>${filteredQuestions.length}</strong> of <strong>${allQuestions.length}</strong> questions
                </div>
                <div class="question-list">
            `;

            filteredQuestions.forEach(question => {
                const isActive = question.categoryId === currentCategoryId && question.id === currentQuestionId;
                const suggestedCount = getCounter(question.id, 'suggested');
                const bulletCount = getCounter(question.id, 'bullet');
                
                questionsHTML += `
                    <div class="question-item ${isActive ? 'active' : ''}" 
                         onclick="loadQuestionById(${question.id})"
                         data-question-id="${question.id}">
                        <div class="question-number">${question.globalIndex}</div>
                        <div class="question-text">
                            ${question.question}
                            <div class="question-category-tag">${question.categoryName}</div>
                            <div class="question-practice-info">
                                <span class="practice-mini-badge ${suggestedCount === 0 ? 'zero' : ''}">ðŸ“ ${suggestedCount}</span>
                                <span class="practice-mini-badge ${bulletCount === 0 ? 'zero' : ''}">ðŸŽ¯ ${bulletCount}</span>
                            </div>
                        </div>
                    </div>
                `;
            });

            questionsHTML += `</div>`;

            sidebar.innerHTML = categoryChipsHTML + questionsHTML;
        }

        // Filter questions by category
        function filterByCategory(categoryId) {
            selectedCategoryFilter = categoryId;
            renderSidebar();
        }

        // Load question by ID (finds the category automatically)
        function loadQuestionById(questionId) {
            const questionData = allQuestions.find(q => q.id === questionId);
            if (questionData) {
                loadQuestion(questionData.categoryId, questionId);
                // Auto-hide sidebar after selecting a question
                if (sidebarVisible) {
                    toggleSidebar();
                }
            }
        }

        // Load first question on init
        function loadFirstQuestion() {
            if (allQuestions.length > 0) {
                const firstQuestion = allQuestions[0];
                loadQuestion(firstQuestion.categoryId, firstQuestion.id);
                // Auto-hide sidebar on initial load
                if (sidebarVisible) {
                    toggleSidebar();
                }
            }
        }

        // Load a specific question
        function loadQuestion(categoryId, questionId) {
            currentCategoryId = categoryId;
            currentQuestionId = questionId;

            const category = interviewData.categories.find(c => c.id === categoryId);
            const question = category.questions.find(q => q.id === questionId);

            // Update active states in sidebar
            renderSidebar();

            // Update question header
            document.getElementById('questionTitle').textContent = question.question;
            document.getElementById('categoryBadge').textContent = category.name;

            // Render question content
            renderQuestionContent(question);
        }

        // Store current question for concept access
        let currentQuestionData = null;

        // Render question content
        function renderQuestionContent(question) {
            const contentArea = document.getElementById('questionContent');
            
            // Store question data for concept access
            currentQuestionData = question;
            
            const suggestedCounter = getCounter(question.id, 'suggested');
            const bulletCounter = getCounter(question.id, 'bullet');

            contentArea.innerHTML = `
                <!-- Suggested Answer Section -->
                <div class="answer-section">
                    <div class="answer-header" onclick="toggleAnswer('suggested-${question.id}')">
                        <h3>ðŸ“ Suggested Answer</h3>
                        <div class="answer-controls">
                            <div class="counter">
                                <span>Practice:</span>
                                <span class="counter-value" id="suggested-counter-${question.id}">${suggestedCounter}</span>
                                <button class="counter-btn" onclick="incrementCounter(${question.id}, 'suggested', event)">+</button>
                            </div>
                            <button class="toggle-btn">Show</button>
                        </div>
                    </div>
                    <div class="answer-content" id="suggested-${question.id}">
                        <p>${question.suggestedAnswer || ''}</p>
                    </div>
                </div>

                <!-- Bullet Points Section -->
                <div class="answer-section">
                    <div class="answer-header" onclick="toggleAnswer('bullet-${question.id}')">
                        <h3>ðŸŽ¯ Bullet Point Answer</h3>
                        <div class="answer-controls">
                            <div class="counter">
                                <span>Practice:</span>
                                <span class="counter-value" id="bullet-counter-${question.id}">${bulletCounter}</span>
                                <button class="counter-btn" onclick="incrementCounter(${question.id}, 'bullet', event)">+</button>
                            </div>
                            <button class="toggle-btn">Show</button>
                        </div>
                    </div>
                    <div class="answer-content" id="bullet-${question.id}">
                        <ul>
                            ${question.bulletPoints.map(point => `<li>${point}</li>`).join('')}
                        </ul>
                    </div>
                </div>

                <!-- Key Concepts Section -->
                <div class="key-concepts">
                    <h3>ðŸ’¡ Key Concepts</h3>
                    <div class="concept-buttons">
                        ${question.keyConcepts.map((concept, index) => 
                            `<button class="concept-btn" onclick="showConceptByIndex(${index})">${concept.name}</button>`
                        ).join('')}
                    </div>
                </div>

                <!-- Navigation -->
                <div class="navigation">
                    <button class="nav-btn" onclick="navigateQuestion(-1)" id="prevBtn">
                        â† Previous
                    </button>
                    <button class="nav-btn" onclick="navigateQuestion(1)" id="nextBtn">
                        Next â†’
                    </button>
                </div>
            `;

            updateNavigationButtons();
        }

        // Toggle answer visibility
        function toggleAnswer(answerId) {
            const answerContent = document.getElementById(answerId);
            const toggleBtn = event.currentTarget.querySelector('.toggle-btn');
            
            answerContent.classList.toggle('visible');
            toggleBtn.textContent = answerContent.classList.contains('visible') ? 'Hide' : 'Show';
        }

        // Get counter from localStorage
        function getCounter(questionId, type) {
            const key = `counter-${questionId}-${type}`;
            return parseInt(localStorage.getItem(key) || '0');
        }

        // Increment counter
        function incrementCounter(questionId, type, event) {
            event.stopPropagation();
            const key = `counter-${questionId}-${type}`;
            const currentValue = getCounter(questionId, type);
            const newValue = currentValue + 1;
            localStorage.setItem(key, newValue.toString());
            
            document.getElementById(`${type}-counter-${questionId}`).textContent = newValue;
            
            // Update sidebar if visible
            if (!sidebarVisible) {
                renderSidebar();
            }
        }

        // Show concept by index
        function showConceptByIndex(index) {
            if (currentQuestionData && currentQuestionData.keyConcepts[index]) {
                const concept = currentQuestionData.keyConcepts[index];
                showConcept(concept);
            }
        }

        // Show concept modal
        function showConcept(concept) {
            const modal = document.getElementById('conceptModal');
            document.getElementById('conceptTitle').textContent = concept.name;
            document.getElementById('conceptBody').innerHTML = `<p>${concept.explanation || ''}</p>`;
            modal.classList.add('active');
        }

        // Close modal
        function closeModal() {
            document.getElementById('conceptModal').classList.remove('active');
        }

        // Close modal when clicking outside
        window.onclick = function(event) {
            const modal = document.getElementById('conceptModal');
            if (event.target === modal) {
                closeModal();
            }
        }

        // Navigate between questions (across all questions, not per category)
        function navigateQuestion(direction) {
            const currentIndex = allQuestions.findIndex(q => q.id === currentQuestionId);
            const newIndex = currentIndex + direction;

            if (newIndex >= 0 && newIndex < allQuestions.length) {
                const newQuestion = allQuestions[newIndex];
                loadQuestion(newQuestion.categoryId, newQuestion.id);
            }

            updateNavigationButtons();
        }

        // Update navigation buttons
        function updateNavigationButtons() {
            const currentIndex = allQuestions.findIndex(q => q.id === currentQuestionId);

            const prevBtn = document.getElementById('prevBtn');
            const nextBtn = document.getElementById('nextBtn');

            // Disable previous button if at first question
            prevBtn.disabled = currentIndex === 0;

            // Disable next button if at last question
            nextBtn.disabled = currentIndex === allQuestions.length - 1;
        }

        // Initialize on page load
        window.onload = init;
    </script>
</body>
</html>
